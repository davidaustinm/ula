<?xml version="1.0" encoding="UTF-8"?>

<exercises>

  <exercise>
    <statement>
      <p>
	Suppose that
	<me>
	  A = \begin{bmatrix}
	  2.1 \amp -1.9 \amp 0.1 \amp 3.7 \\
	  -1.5 \amp 2.7 \amp 0.9 \amp -0.6 \\
	  -0.4 \amp 2.8 \amp -1.5 \amp 4.2 \\
	  -0.4 \amp 2.4 \amp 1.9 \amp -1.8
	  \end{bmatrix}.
	</me>
	<sage>
	  <input>
	  </input>
	</sage>
	<ol marker="a.">
	  <li>
	    <p>
	      Find the singular values of <m>A</m>.  What is
	      <m>\rank(A)</m>? 
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the sequence of matrices
	      <m>A_1</m>, <m>A_2</m>, <m>A_3</m>, and <m>A_4</m>
	      where <m>A_k</m> is the rank <m>k</m> approximation of
	      <m>A</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The singular values are
	      <m>6.4</m>, <m>5.2</m>, <m>2.1</m>, and <m>0.5</m> so we
	      have <m>\rank(A) = 4</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>A_1=\begin{bmatrix}
	      1.3 \amp -1.2 \amp -1.2 \amp 3.8 \\
	      -0.58 \amp 0.56 \amp 0.56 \amp -1.7 \\
	      0.92 \amp -0.90 \amp -0.89 \amp 2.7 \\
	      -0.84 \amp 0.83 \amp 0.82 \amp -2.5
	      \end{bmatrix}</m>,
	      <m>A_2=\begin{bmatrix}
	      1.5 \amp -2.0 \amp -1.2 \amp 3.4 \\
	      -1.3 \amp 2.8 \amp 0.43 \amp -0.77 \\
	      -0.32 \amp 2.8 \amp -1.1 \amp 4.3 \\
	      -1.3 \amp 2.1 \amp 0.75 \amp -2.0
	      \end{bmatrix}</m>,
	    </p>
	    <p>
	      <m>A_3=\begin{bmatrix}
	      2.2 \amp -1.9 \amp 0.048 \amp 3.7 \\
	      -1.2 \amp 2.9 \amp 0.72 \amp -0.72 \\
	      -0.51 \amp 2.8 \amp -1.4 \amp 4.2 \\
	      -0.60 \amp 2.3 \amp 2.0 \amp -1.7
	      \end{bmatrix}</m>, <m>A_4=A</m>
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\rank(A) = 4</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>A_1=\begin{bmatrix}
	      1.3 \amp -1.2 \amp -1.2 \amp 3.8 \\
	      -0.58 \amp 0.56 \amp 0.56 \amp -1.7 \\
	      0.92 \amp -0.90 \amp -0.89 \amp 2.7 \\
	      -0.84 \amp 0.83 \amp 0.82 \amp -2.5
	      \end{bmatrix}</m>,
	      <m>A_2=\begin{bmatrix}
	      1.5 \amp -2.0 \amp -1.2 \amp 3.4 \\
	      -1.3 \amp 2.8 \amp 0.43 \amp -0.77 \\
	      -0.32 \amp 2.8 \amp -1.1 \amp 4.3 \\
	      -1.3 \amp 2.1 \amp 0.75 \amp -2.0
	      \end{bmatrix}</m>,\
	    </p>
	    <p>
	      <m>A_3=\begin{bmatrix}
	      2.2 \amp -1.9 \amp 0.048 \amp 3.7 \\
	      -1.2 \amp 2.9 \amp 0.72 \amp -0.72 \\
	      -0.51 \amp 2.8 \amp -1.4 \amp 4.2 \\
	      -0.60 \amp 2.3 \amp 2.0 \amp -1.7
	      \end{bmatrix}</m>, <m>A_4=A</m>
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
  </exercise>

  <exercise>
    <statement>
      <p>
	Suppose we would like to find the best quadratic function
	<me>
	  \beta_0 + \beta_1x + \beta_2x^2=y
	</me>
	fitting the points
	<me>
	  (0,1), (1,0), (2,1.5), (3,4), (4,8).
	</me>
	<sage>
	  <input>
	    
	  </input>
	</sage>
	<ol marker="a.">
	  <li>
	    <p>
	      Set up a linear system <m>A\xvec = \bvec</m> describing
	      the coefficients <m>\xvec =
	      \threevec{\beta_0}{\beta_1}{\beta_2}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Find the singular value decomposition of <m>A</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Use the singular value decomposition to find the
	      least-squares approximate solution <m>\xhat</m>.
	    </p>
	  </li>
	</ol>

      </p>
    </statement>
    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The linear system is
	      <me>
		\begin{bmatrix}
		1 \amp 0 \amp 0 \\
		1 \amp 1 \amp 1 \\
		1 \amp 2 \amp 4 \\
		1 \amp 3 \amp 9 \\
		1 \amp 4 \amp 16 \\
		\end{bmatrix}
		\xvec
		=
		\fivevec10{1.5}48
	      </me>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>U</m> is a <m>5\times5</m> matrix, <m>\Sigma</m> is
	      <m>5\times3</m>, and <m>V</m> is <m>3\times3</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      The rank of <m>A</m> is 3 so we'll
	      form the reduced singular value decomposition
	      <m>A=U_3\Sigma_3V_3^T</m>.  Then <m>\xhat =
	      V_3\Sigma_3^{-1}U_3^T\bvec =
	      \threevec{0.87}{-1.34}{0.79}</m>
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The linear system is
	      <me>
		\begin{bmatrix}
		1 \amp 0 \amp 0 \\
		1 \amp 1 \amp 1 \\
		1 \amp 2 \amp 4 \\
		1 \amp 3 \amp 9 \\
		1 \amp 4 \amp 16 \\
		\end{bmatrix}
		\xvec
		=
		\fivevec10{1.5}48
	      </me>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>U</m> is a <m>5\times5</m> matrix, <m>\Sigma</m> is
	      <m>5\times3</m>, and <m>V</m> is <m>3\times3</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>\xhat=\threevec{0.87}{-1.34}{0.79}</m>
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
		
  </exercise>

  <exercise>
    <statement>
      <p>
	Remember that the outer product of two vectors <m>\uvec</m> and
	<m>\vvec</m> is the matrix <m>\uvec~\vvec^T</m>.
	<ol marker="a.">
	  <li>
	    <p>
	      Suppose that <m>\uvec = \twovec{2}{-3}</m> and
	      <m>\vvec=\threevec201</m>.  Evaluate the outer product
	      <m>\uvec~\vvec^T</m>.  To get a clearer sense of how this
	      works, perform this operation without using technology.
	    </p>
	    <p>
	      How is each of the columns of <m>\uvec~\vvec^T</m>
	      related to <m>\uvec</m>?
	    </p>
	  </li>
	  <li>
	    <p>
	      Suppose <m>\uvec</m> and <m>\vvec</m> are general
	      vectors.  What is <m>\rank(\uvec~\vvec^T)</m> and what is
	      a basis for its column space <m>\col(\uvec~\vvec^T)</m>?
	    </p>
	  </li>
	  <li>
	    <p>
	      Suppose that <m>\uvec</m> is a unit vector.  What is the
	      effect of multiplying a vector by the matrix
	      <m>\uvec~\uvec^T</m>?
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\uvec~\vvec^T = \begin{bmatrix}
	      2\uvec \amp 0\uvec \amp \uvec
	      \end{bmatrix} = \begin{bmatrix}
	      4 \amp 0 \amp 2 \\
	      -6 \amp 0 \amp -3
	      \end{bmatrix}</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      Since each column of <m>\uvec~\vvec^T</m> is a scalar
	      multiple of <m>\uvec</m>, the rank of the matrix is
	      <m>1</m> and <m>\uvec</m> forms a basis.
	    </p>
	  </li>
	  <li>
	    <p>
	      This matrix projects vectors orthogonally onto the line
	      defined by <m>\uvec</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	  
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>\uvec~\vvec^T =
	      \begin{bmatrix}
	      4 \amp 0 \amp 2 \\
	      -6 \amp 0 \amp -3
	      \end{bmatrix}</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      The rank is <m>1</m> and <m>\uvec</m> forms a basis.
	    </p>
	  </li>
	  <li>
	    <p>
	      This matrix projects vectors orthogonally onto the line
	      defined by <m>\uvec</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
		
  </exercise>
	  
	  

  <exercise>
    <statement>
      <p>
	Evaluating the following cell loads in a dataset recording
	some features of 1057 houses.  Notice how the lot size varies
	over a relatively small range compared to the other features.
	For this reason, in addition to demeaning the data, we'll
	scale each feature by dividing by its standard deviation so
	that the range of values is similar for each feature.  The
	matrix <m>A</m> holds the result.
	<sage>
	  <input>
import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/housing.csv', index_col=0)
df = df.fillna(df.mean())
std = (df-df.mean())/df.std()
A = matrix(std.values).T
df.T

	  </input>
	</sage>
	<ol marker="a.">
	  <li>
	    <p>
	      Find the singular values of <m>A</m> and use them to
	      determine the variance in the direction of the principal
	      components. 
	      <sage>
		<input>

		</input>
	      </sage>
	    </p>
	  </li>
	  <li>
	    <p>
	      For what fraction of the variance do the first two
	      principal components account?
	    </p>
	  </li>
	  <li>
	    <p>
	      Find a singular value decomposition of <m>A</m> and
	      construct the <m>2\times1057</m> matrix <m>B</m>
	      whose entries are the coordinates of the demeaned data
	      points projected on to the two-dimensional subspace
	      spanned by the first two principal components.  You can
	      plot the projected data points using
	      <c>list_plot(B.columns())</c>.
	      <sage>
		<input>
		</input>
	      </sage>
	    </p>
	  </li>

	  <li>
	    <p>
	      Study the entries in the first two principal components
	      <m>\uvec_1</m> and <m>\uvec_2</m>.  Would a more
	      expensive house lie on the left, right, top, or bottom
	      of the plot you constructed?
	    </p>
	  </li>

	  <li>
	    <p>
	      In what ways does a house that lies on the far left of
	      the plot you constructed differ from an average house?
	      In what ways does a house that lies near the top of the
	      plot you constructed differ from an average house?
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The singular values <m>\sigma_i</m> are <m>45.5</m>, <m>32.6</m>,
	      <m>28.9</m>, and <m>15.9</m>.  Therefore, the variances
	      in the direction of the principal components are
	      given by <m>\frac{1}{1057}\sigma_i^2</m> and are
	      <m>1.96</m>, <m>1.01</m>, <m>0.79</m>, and <m>0.24</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      The first two principal components retain <m>74\%</m> of
	      the total variance.
	    </p>
	  </li>
	  <li>
	    <p>
	      The matrix <m>B</m> is formed from the first two rows of
	      the product <m>\Sigma V^T</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      The fourth components, the ones that correspond to house
	      prices, of <m>\uvec_1</m> and <m>\uvec_2</m> are
	      <m>-0.65</m> and <m>-0.03</m> respectively.  This shows
	      us that <m>\uvec_2</m> doesn't contribute much to the
	      house price.  Moving in the negative <m>\uvec_1</m>
	      direction causes the house price to go up so the more
	      expensive homes are on the left.
	    </p>
	  </li>
	  <li>
	    <p>
	      The first principal component is <m>\uvec_1 =
	      \fourvec{-0.64}{-0.20}{0.36}{-0.65}</m>.  If we move to
	      the left, this vector is multiplied by a negative number
	      so the living area, lot size, and price go up while the
	      age of the house decreases.
	    </p>
	    <p>
	      The second principal component is
	      <m>\uvec_2=\fourvec{0.05}{0.86}{0.51}{-0.03}</m>.  If we
	      move up, this is multiplied by a positive number so the
	      lot size and the age of the house increase.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The variances are
	      <m>1.96</m>, <m>1.01</m>, <m>0.79</m>, and <m>0.24</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>74\%</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      The matrix <m>B</m> is formed from the first two rows of
	      the product <m>\Sigma V^T</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Left
	    </p>
	  </li>
	  <li>
	    <p>
	      Houses on the left have larger living area, lot sizes,
	      and prices and are newer.  Houses near the top have
	      larger lot sizes and ages.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	Let's revisit the voting records of justices on the second
	Rehnquist court.  Evaluating the following cell will load the
	voting records of the justices in the 188 cases decided by a
	5-4 vote and store them in the matrix <m>A</m>.
	<sage>
	  <input>
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = matrix(RDF, fivefour.values)
v = vector(188*[1])
fivefour

	  </input>
	</sage>
	<ol marker="a.">
	  <li>
	    <p>
	      The cell above also defined the 188-dimensional vector 
	      <m>\vvec</m> 
	      whose entries are all 1.  What does the product
	      <m>A\vvec</m> represent?  Use the following cell to
	      evaluate this product.
	      <sage>
		<input>

		</input>
	      </sage>
	    </p>
	  </li>
	  <li>
	    <p>
	      How does the product <m>A\vvec</m> tell us which justice
	      voted in the majority most frequently?  What does this
	      say about the presence of a swing vote on the court?
	    </p>
	  </li>
	  <li>
	    <p>
	      How does this product tell us whether we should
	      characterize this court as leaning conservative or
	      progressive?
	    </p>
	  </li>
	  <li>
	    <p>
	      How does this product tell us about the presence of a
	      second swing vote on the court?
	    </p>
	  </li>
	  <li>
	    <p>
	      Study the left singular vector <m>\uvec_3</m> and
	      describe how it reinforces the fact that there was a second
	      swing vote.  Who was this second swing vote?
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      <m>A\vvec</m> is a 9-dimensional vector.  For each
	      justice, it tells us the number of times that justice
	      voted with the majority minus the number of times they
	      voted in the minority.
	    </p>
	  </li>
	  <li>
	    <p>
	      Out of 188 cases, the fifth justice Sandra Day O'Connor
	      voted with the majority 94 more times than with the
	      minority.  Since a swing vote will often vote in the
	      majority, this gives additional evidence that Sandra Day
	      O'Connor is a swing vote.
	    </p>
	  </li>
	  <li>
	    <p>
	      The first five justices more often vote with the
	      majority and the last four more often with the
	      minority.  This means the court leans toward the first
	      five justices, who are the more conservative ones.
	    </p>
	  </li>
	  <li>
	    <p>
	      The fourth justice Anthony Kennedy voted with the
	      majority 84 times more than with the minority.  He is a
	      potential swing vote.
	    </p>
	  </li>
	  <li>
	    <p>
	      The third singular vector
	      <m>\uvec_3</m> effectively records his vote and not the
	      votes of the other justices.  This lends support to the
	      claim that Anthony Kennedy is a second, somewhat less
	      influential, swing vote.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      For each justice, it tells us the number of times that
	      justice voted with the majority minus the number of
	      times they voted in the minority.
	    </p>
	  </li>
	  <li>
	    <p>
	      The justice that most often votes with the majority
	      could be a swing vote.
	    </p>
	  </li>
	  <li>
	    <p>
	      The first five justices usually vote in the majority.
	    </p>
	  </li>
	  <li>
	    <p>
	      A second justice voted with the
	      majority 84 times more than with the minority.
	    </p>
	  </li>
	  <li>
	    <p>
	      Anthony Kennedy
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
  </exercise>

  <exercise>
    <statement>
      <p>
	The following cell loads a dataset that describes the
	percentages with which justices on the second Rehnquist court
	agreed with one another.  For instance, the entry in the first
	row and second column is 72.78, which means that Justices
	Rehnquist and Scalia agreed with each other in 72.78% of the cases. 
	<sage>
	  <input>
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = 1/100*matrix(RDF, agreement.values)
agreement

	  </input>
	</sage>
	<ol marker="a.">
	  <li>
	    <p>
	      Examine the matrix <m>A</m>.  What special structure
	      does this matrix have and why should we expect it to
	      have this structure?
	    </p>
	  </li>
	  <li>
	    <p>
	      Plot the singular values of <m>A</m> below.  For what
	      value of <m>k</m> would the approximation <m>A_k</m>
	      be a reasonable approximation of <m>A</m>?
	      <sage>
		<input>
plot_sv(A)

		</input>
	      </sage>
	    </p>
	  </li>
	  <li>
	    <p>
	      Find a singular value decomposition <m>A=U\Sigma V^T</m>
	      and examine the matrices <m>U</m> and <m>V</m> using,
	      for instance, <c>n(U, 3)</c>.  What do you notice about
	      the relationship between <m>U</m> and <m>V</m> and why
	      should we expect this relationship to hold?
	      <sage>
		<input>

		</input>
	      </sage>
	    </p>
	  </li>
	  <li>
	    <p>
	      The command <c>approximate(A, k)</c> will form the
	      approximating matrix <m>A_k</m>.  Study the matrix
	      <m>A_1</m> using the <c>display_matrix</c> command.
	      Which justice or justices seem to be most agreeable,
	      that is, most likely to agree with other justices?
	      Which justice is least agreeable?
	      <sage>
		<input>

		</input>
	      </sage>
	    </p>
	  </li>
	  <li>
	    <p>
	      Examine the difference <m>A_2-A_1</m> and describe how
	      this tells us about the presence of voting blocs and
	      swing votes on the court.
	      <sage>
		<input>

		</input>
	      </sage>
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix is symmetric because the number of times
	      Justice A votes with Justice B is the same as the number
	      of times Justice B votes with Justice A.
	    </p>
	  </li>
	  <li>
	    <p>
	      The first two singular values appear to be most
	      important so <m>A_2</m> should be a good approximation
	      to <m>A</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>U=V</m> since <m>A</m> is a positive definite,
	      symmetric matrix.
	    </p>
	  </li>
	  <li>
	    <p>
	      The sixth justice, John Paul Stevens, is the least
	      agreeable.
	    </p>
	  </li>
	  <li>
	    <p>
	      The fourth and fifth justices, Anthony Kennedy and
	      Sandra Day O'Connor are less inclined to agree with the
	      others so they form swing votes.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
	      
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The matrix is symmetric.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>A_2</m> should be a good approximation
	      to <m>A</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>U=V</m>
	    </p>
	  </li>
	  <li>
	    <p>
	     John Paul Stevens, is the least
	      agreeable.
	    </p>
	  </li>
	  <li>
	    <p>
	      The fourth and fifth justices, Anthony Kennedy and
	      Sandra Day O'Connor are less inclined to agree with the
	      others.
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	      
  </exercise>

  <exercise>
    <statement>
      <p>
	Suppose that <m>A=U_r\Sigma_rV_r^T</m> is a reduced singular
	value decomposition of the <m>m\times n</m> matrix <m>A</m>.
	The matrix <m>A^+ = V_r\Sigma_r^{-1}U_r^T</m> is called the
	<em>Moore-Penrose inverse</em> of <m>A</m>.
	<ol marker="a.">
	  <li>
	    <p>
	      Explain why <m>A^+</m> is an <m>n\times m</m> matrix.
	    </p>
	  </li>
	  <li>
	    <p>
	      If <m>A</m> is an invertible, square matrix, explain why
	      <m>A^+=A^{-1}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Explain why <m>AA^+\bvec=\bhat</m>, the orthogonal
	      projection of <m>\bvec</m> onto <m>\col(A)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Explain why <m>A^+A\xvec=\xhat</m>, the orthogonal
	      projection of <m>\xvec</m> onto <m>\col(A^T)</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The number of columns of <m>A^+</m> is the number of
	      columns of <m>U_r^T</m>, which is <m>m</m>, and the
	      number of rows is the number of rows of <m>V_r</m>,
	      which is <m>n</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      If <m>A</m> is invertible, then <m>A</m> is an
	      <m>m\times m</m> matrix whose rank is <m>m</m>.  The
	      reduced singular value decomposition is therefore
	      <m>A=U\Sigma V^T</m> so that <m>A^+=V\Sigma^{-1}U^T =
	      (U\Sigma V^T)^{-1}=A^{-1}</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>AA^{+} = (U_r\Sigma_rV_r^T)(V_r\Sigma^{-1}U_r^T) =
	      U_rU_r^T</m>, which projects vectors orthogonal onto
	      <m>\col(A)</m> since the columns of <m>U_r</m> are an
	      orthonormal basis for <m>\col(A)</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>AA^{+} = (V_r\Sigma^{-1}U_r^T)(U_r\Sigma_rV_r^T)
	      V_rV_r^T</m>, which projects vectors orthogonal onto
	      <m>\col(A^T)</m> since the columns of <m>V_r</m> are an
	      orthonormal basis for <m>\col(A^T)</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>
    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      Find the number of columns of <m>U_r^T</m> and the
	      number of rows of <m>V_r</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      If <m>A</m> is invertible, then <m>A</m> is an
	      <m>m\times m</m> matrix whose rank is <m>m</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>AA^{+} = (U_r\Sigma_rV_r^T)(V_r\Sigma^{-1}U_r^T) =
	      U_rU_r^T</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>AA^{+} = (V_r\Sigma^{-1}U_r^T)(U_r\Sigma_rV_r^T)
	      V_rV_r^T</m>
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
  </exercise>

  <exercise>
    <statement>
      <p>
	In <xref ref="subsec-partial-pivot"/>, we saw how some
	linear algebraic computations are sensitive to round off error
	made by a computer.  A singular value decomposition can help
	us understand when this situation can occur.
      </p>

      <p>
	For instance, consider the matrices
	<me>
	  A = \begin{bmatrix}
	  1.0001 \amp 1 \\
	  1 \amp 1 \\
	  \end{bmatrix},\hspace{24pt}
	  B = \begin{bmatrix}
	  1 \amp 1 \\
	  1 \amp 1 \\
	  \end{bmatrix}.
	</me>
	The entries in these matrices are quite close to one another,
	but <m>A</m> is invertible while <m>B</m> is not.  It seems
	like <m>A</m> is <em>almost</em> singular.
	In fact, we can measure how close
	a matrix is to being singular by forming the <em>condition
	number</em>, <m>\sigma_1/\sigma_n</m>, the ratio of the
	largest to smallest singular value.  If <m>A</m> were
	singular, the condition number would be undefined because the
	singular value <m>\sigma_n=0</m>.  Therefore,
	we will think of matrices with large condition numbers as
	being close to singular. 
	<ol marker="a.">
	  <li>
	    <p>
	      Define the matrix <m>A</m> and find a singular value
	      decomposition.  What is the condition number of
	      <m>A</m>?
	      <sage>
		<input>

		</input>
	      </sage>
	    </p>
	  </li>
	  <li>
	    <p>
	      Define the left singular vectors <m>\uvec_1</m> and
	      <m>\uvec_2</m>.  Compare the results <m>A^{-1}\bvec</m>
	      when
	      <ol marker="1.">
		<li>
		  <p>
		    <m>\bvec=\uvec_1+\uvec_2</m>.
		  </p>
		</li>
		<li>
		  <p>
		    <m>\bvec=2\uvec_1+\uvec_2</m>.
		  </p>
		</li>
	      </ol>
	      Notice how a small change in the vector <m>\bvec</m>
	      leads to a small change in
	      <m>A^{-1}\bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Now compare the results <m>A^{-1}\bvec</m> when
	      <ol marker="1.">
		<li>
		  <p>
		    <m>\bvec=\uvec_1+\uvec_2</m>.
		  </p>
		</li>
		<li>
		  <p>
		    <m>\bvec=\uvec_1+2\uvec_2</m>.
		  </p>
		</li>
	      </ol>
	      Notice now how a small change in <m>\bvec</m> leads to a
	      large change in <m>A^{-1}\bvec</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Previously, we saw that, if we write <m>\xvec</m> in
	      terms of left singular vectors
	      <m>\xvec=c_1\vvec_1+c_2\vvec_2</m>, then we have
	      <me>
		\bvec=A\xvec = c_1\sigma_1\uvec_1 +
		c_2\sigma_2\uvec_2.
	      </me>
	      If we write <m>\bvec=d_1\uvec_1+d_2\uvec_2</m>, explain
	      why <m>A^{-1}\bvec</m> is sensitive to small changes in
	      <m>d_2</m>. 
	    </p>
	  </li>
	</ol>
	Generally speaking, a square matrix <m>A</m> with a large
	condition number will demonstrate this type of behavior so
	that the computation of <m>A^{-1}</m> is likely to be affected
	by round off error.  We call such a matrix
	<em>ill-conditioned</em>. 
      </p>
    </statement>

    <solution>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The singular values of <m>A</m> are <m>2</m> and
	      <m>5\times10^{-5}</m>, which means the condition number
	      is <m>40,000</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      <ol marker="1.">
		<li>
		  <p>
		    <m>A^{-1}\bvec = \twovec{-14142.5}{14142.5}</m>
		  </p>
		</li>
		<li>
		  <p>
		    <m>A^{-1}\bvec = \twovec{-14142.8}{14142.8}</m>
		  </p>
		</li>
	      </ol>
	    </p>
	  </li>
	  <li>
	    <p>
	      <ol marker="1.">
		<li>
		  <p>
		    <m>A^{-1}\bvec = \twovec{-14142.5}{14142.5}</m>
		  </p>
		</li>
		<li>
		  <p>
		    <m>A^{-1}\bvec = \twovec{-28284.6}{28285.3}</m>
		  </p>
		</li>
	      </ol>
	    </p>
	  </li>
	  <li>
	    <p>
	      We have <m>d_i = \sigma_ic_i</m> so
	      <m>c_i=\frac{d_i}{\sigma_i}</m>.  Since <m>\sigma_2</m>
	      is so much smaller than <m>\sigma_1</m>,
	      even small changes in <m>d_2</m> can lead to relatively
	      large changes in <m>c_2</m>.
	    </p>
	  </li>
	</ol>
      </p>
    </solution>

    <answer>
      <p>
	<ol marker="a.">
	  <li>
	    <p>
	      The condition number
	      is <m>40,000</m>
	    </p>
	  </li>
	  <li>
	    <p>
	      <ol marker="1.">
		<li>
		  <p>
		    <m>A^{-1}\bvec = \twovec{-14142.5}{14142.5}</m>
		  </p>
		</li>
		<li>
		  <p>
		    <m>A^{-1}\bvec = \twovec{-14142.8}{14142.8}</m>
		  </p>
		</li>
	      </ol>
	    </p>
	  </li>
	  <li>
	    <p>
	      <ol marker="1.">
		<li>
		  <p>
		    <m>A^{-1}\bvec = \twovec{-14142.5}{14142.5}</m>
		  </p>
		</li>
		<li>
		  <p>
		    <m>A^{-1}\bvec = \twovec{-28284.6}{28285.3}</m>
		  </p>
		</li>
	      </ol>
	    </p>
	  </li>
	  <li>
	    <p>
	      <m>c_2=\frac{d_2}{\sigma_2}</m>
	    </p>
	  </li>
	</ol>
      </p>
    </answer>
	  
  </exercise>

</exercises>

