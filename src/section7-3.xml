<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-pca"
	 xmlns:xi="http://www.w3.org/2001/XInclude">
  <title> Principal Component Analysis </title>

  <introduction>

    <p>
      We are sometimes presented with a dataset having many data
      points that live in a high dimensional space.  For instance, we
      looked at a dataset describing body fat index (BFI) in
      <xref ref="activity-BFI" /> where each data point is
      six-dimensional.  Developing an intuitive understanding of the
      data is hampered by the fact that it cannot be visualized.
    </p>

    <p>
      This section explores a technique called <em> principal
      component analysis</em>, which enables us to reduce the
      dimension of a dataset so that it may be visualized or studied
      in a way so that interesting features more readily stand out.
      Our previous work with variance and the orthogonal
      diagonalization of symmetric matrices provides the key ideas.
    </p>

    <exploration label="ula-preview-7-3">
      <introduction>
	<p>
	  We will begin by recalling our earlier discussion of
	  variance.  Suppose we have a dataset that leads to the
	  covariance matrix
	  <me>
	    C = \begin{bmatrix}
	    7 \amp -4 \\
	    -4 \amp 13
	    \end{bmatrix}
	  </me>.
        </p>
      </introduction>

      <task label="ula-preview-7-3-a">
        <statement>
	  <p>
	    Suppose that <m>\uvec</m> is a unit eigenvector of
	    <m>C</m> with eigenvalue <m>\lambda</m>.  What is the
	    variance <m>V_{\uvec}</m> in the <m>\uvec</m>
	    direction?
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    <m>V_{\uvec} = \uvec\cdot(C\uvec) =
	    \lambda\uvec\cdot\uvec = \lambda</m>.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-7-3-b">
        <statement>
	  <p>
	    Find an orthogonal diagonalization of <m>C</m>.
	    <sage>
	      <input>
	      </input>
	    </sage>
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    We can write <m>C=QDQ^T</m> where
	    <me>
	      D=\begin{bmatrix}
	      15 \amp 0 \\
	      0 \amp 5 \\
	      \end{bmatrix},~~~
	      Q = \begin{bmatrix}
	      \frac1{\sqrt{5}} \amp \frac2{\sqrt{5}} \\
	      -\frac2{\sqrt{5}} \amp \frac1{\sqrt{5}} \\
	      \end{bmatrix}
	    </me>.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-7-3-c">
        <statement>
	  <p>
	    What is the total variance?
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    The total variance is the sum of the eigenvalues,
	    <m>V=\lambda_1 + \lambda_2 = 15 + 5 = 20</m>.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-7-3-d">
        <statement>
	  <p>
	    In
	    which direction is the variance greatest and what is
	    the variance in this direction?  If we project the
	    data onto this line, how much variance is lost?
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    The variance is greatest in the direction of the
	    eigenvector associated to the largest eigenvalue.
	    This direction is defined by
	    <m>\twovec{\frac{1}{\sqrt{5}}}{-\frac2{\sqrt{5}}}</m>,
	    and the variance is 15 in this direction.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-7-3-e">
        <statement>
	  <p>
	    In which direction is the variance smallest and how is
	    this direction related to the direction of maximum
	    variance?
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    The variance is smallest in the direction defined by
	    <m>\twovec{\frac2{\sqrt{5}}}{\frac1{\sqrt{5}}}</m>.
	  </p>
        </solution>
      </task>

      <task component="rs-preview">
        <query label="ula-preview-7-3-poll" visibility="instructor">
          <statement>
            <p>I feel confident with the material in this activity.</p>
          </statement>
          <choices>
            <choice><p>Strongly Agree</p></choice>
            <choice><p>Agree</p></choice>
            <choice><p>Neutral</p></choice>
            <choice><p>Disagree</p></choice>
            <choice><p>Strongly Disagree</p></choice>
          </choices>
        </query>
      </task>

      <task component="rs-preview"
            label="ula-preview-7-3-what-else">
        <statement>
          <p>What would you need to know to feel
          more confident about this material?</p>
        </statement>
        <response/>
      </task>

    </exploration>

    <p>
      Here are some ideas we've seen previously that will be
      particularly useful for us in this section.  Remember that 
      the covariance matrix of a dataset is <m>C=\frac 1N AA^T</m>
      where <m>A</m> is the matrix of <m>N</m> demeaned
      data points.
      <ul>
	<li>
	  <p>
	    When <m>\uvec</m> is a unit vector, the variance of the
	    demeaned data after projecting onto the line defined by
	    <m>\uvec</m> is given by the quadratic form <m>V_{\uvec} =
	    \uvec\cdot(C\uvec)</m>.
	  </p>
	</li>
	<li>
	  <p>
	    In particular, if <m>\uvec</m> is a unit eigenvector of
	    <m>C</m> with associated eigenvalue <m>\lambda</m>, then
	    <m>V_{\uvec} = \lambda</m>.
	  </p>
	</li>
	<li>
	  <p>
	    Moreover, variance is additive, as we recorded in <xref
	    ref="prop-variance-additivity" />: if <m>W</m> is a subspace
	    having an orthonormal basis
	    <m>\uvec_1,\uvec_2,\ldots,\uvec_n</m>, then the variance
	    <me>
	      V_W = V_{\uvec_1} + V_{\uvec_2} + \ldots +
	      V_{\uvec_n}\text{.} 
	    </me>
	  </p>
	</li>
      </ul>
    </p>

  </introduction>

  <subsection>
    <title> Principal Component Analysis </title>

    <p>
      Let's begin by looking at an example that illustrates the
      central theme of this technique.
    </p>

    <activity>
      <statement>
	<p>
	  Suppose that we work with a dataset having 100
	  five-dimensional data points.  The demeaned data matrix
	  <m>A</m> is therefore <m>5\times100</m> and leads to the
	  covariance matrix <m>C=\frac1{100}~AA^T</m>, which is a
	  <m>5\times5</m> matrix.  Because <m>C</m> is symmetric, the
	  Spectral Theorem tells us it is orthogonally diagonalizable
	  so suppose that <m>C = QDQ^T</m> where
	  <me>
	    Q = \begin{bmatrix}
	    \uvec_1 \amp \uvec_2 \amp \uvec_3 \amp \uvec_4 \amp \uvec_5
	    \end{bmatrix},\hspace{24pt}
	    D = \begin{bmatrix}
	    13 \amp 0 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 10 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 2 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	    0 \amp 0 \amp 0 \amp 0 \amp 0
	    \end{bmatrix}.
	  </me>
	  <ol marker="a.">
	    <li>
	      <p>
		What is <m>V_{\uvec_2}</m>, the variance in the
		<m>\uvec_2</m> direction?
	      </p>
	    </li>
	    
	    <li>
	      <p>
		Find the variance of the data projected onto the line
		defined by <m>\uvec_4</m>.  What does this say about the
		data?
	      </p>
	    </li>

	    <li>
	      <p>
		What is the total variance of the data?
	      </p>
	    </li>
	    
	    <li>
	      <p>
		Consider the 2-dimensional subspace spanned by
		<m>\uvec_1</m> and <m>\uvec_2</m>.  If we project the
		data onto this subspace, what fraction 
		of the total variance is represented by the variance of the
		projected data?
	      </p>
	    </li>
	    
	    <li>
	      <p>	    
		How does this question change if we project onto the
		3-dimensional subspace 
		spanned by <m>\uvec_1</m>, <m>\uvec_2</m>, and
		<m>\uvec_3</m>?
	      </p>
	    </li>
	    
	    <li>
	      <p>
		What does this tell us about the data?
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>V_{\uvec_2} = \lambda_2 = 10</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>V_{\uvec_4} = \lambda_4 = 0</m>, which tells us
		there is no variance in the <m>\uvec_4</m> direction.
		Therefore, when we project onto the line defined by
		<m>\uvec_4</m>, every data point projects to
		<m>\zerovec</m> so every data point is in the
		orthogonal complement of <m>\uvec_4</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>V = V_{\uvec_1} + V_{\uvec_2} + V_{\uvec_3} +
		V_{\uvec_4} + V_{\uvec_5}  = 13+10+2+0+0 = 25</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The variance of the data projected onto this subspace
		is <m>13+10=23</m>, which represents <m>23/25=92\%</m>
		of the variance.
	      </p>
	    </li>
	    <li>
	      <p>
		Projecting onto this 3-dimensional subspace retains
		all of the variance.
	      </p>
	    </li>
	    <li>
	      <p>
		All of the data lies in the <m>3</m>-dimensional
		subspace spanned by
		<m>\uvec_1</m>, <m>\uvec_1</m>, and <m>\uvec_1</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
      
      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>10</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>0</m>, which tells us
		every data point is in the
		orthogonal complement of <m>\uvec_4</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>25</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>92\%</m> of the variance
	      </p>
	    </li>
	    <li>
	      <p>
		<m>100\%</m> of the variance.
	      </p>
	    </li>
	    <li>
	      <p>
		All of the data lies in the <m>3</m>-dimensional
		subspace spanned by
		<m>\uvec_1</m>, <m>\uvec_1</m>, and <m>\uvec_1</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
	  
    </activity>

    <p>
      This activity demonstrates how the eigenvalues of the covariance
      matrix can tell us when data are clustered around, or even wholly
      contained within, a smaller dimensional subspace.  In
      particular, the original data is 5-dimensional, but we see
      that it actually lies in a 3-dimensional subspace of
      <m>\real^5</m>.  Later in this section, we'll see how to use
      this observation to work with the data as if it were
      three-dimensional, an idea known as <em>dimensional
      reduction</em>.
    </p>

    <p>
      <idx> principal components</idx> The eigenvectors <m>\uvec_j</m>
      of the covariance matrix are called <em>principal
      components</em>, and we will order them so that their associated
      eigenvalues decrease.  Generally speaking, we hope that the
      first few principal components retain most of the variance, as
      the example in the activity demonstrates.  In that example, we
      have the sequence of subspaces
      <ul>
	<li>
	  <p>
	    <m>W_1</m>, the 1-dimensional subspace spanned by
	    <m>\uvec_1</m>, which retains <m>13/25 = 52\%</m> of the
	    total variance,
	  </p>
	</li>
	<li>
	  <p>
	    <m>W_2</m>, the 2-dimensional subspace spanned by
	    <m>\uvec_1</m> and <m>\uvec_2</m>, which retains <m>23/25
	    = 92\%</m> of the variance, and
	  </p>
	</li>
	<li>
	  <p>
	    <m>W_3</m>, the 3-dimensional subspace spanned by
	    <m>\uvec_1</m>, <m>\uvec_2</m>, and <m>\uvec_3</m>, which
	    retains all of the variance.
	  </p>
	</li>
      </ul>
    </p>

    <p>
      Notice how we retain more of the total variance as we increase
      the dimension of the subspace onto which the data are projected.
      Eventually, projecting the data onto <m>W_3</m> retains all the
      variance, which tells us the data must lie in <m>W_3</m>, a
      smaller dimensional subspace of <m>\real^5</m>.
    </p>

    <p>
      In fact, these subspaces are the best possible.  We know that
      the first principal component <m>\uvec_1</m> is the eigenvector
      of <m>C</m> associated to the largest eigenvalue.  This means
      that the variance is as large as possible in the <m>\uvec_1</m>
      direction.  In other words, projecting onto any other line
      will retain a smaller amount of variance.  Similarly, projecting
      onto any other 2-dimensional subspace besides <m>W_2</m> will
      retain less variance than projecting onto <m>W_2</m>.  The
      principal components have the wonderful ability to pick out the
      best possible subspaces to retain as much variance as
      possible.
    </p>

    <p>
      Of course, this is a contrived example.  Typically, the presence
      of noise in a dataset means that we do not expect all the points
      to be wholly contained in a smaller dimensional subspace.  In
      fact, the 2-dimensional subspace <m>W_2</m> retains
      <m>92\%</m> of the variance.  Depending on the situation, we may
      want to write off the remaining <m>8\%</m> of the variance as
      noise in exchange for the convenience of working with a smaller
      dimensional subspace.  As we'll see later, we will seek a
      balance using a number of principal components large enough to
      retain most of the variance but small enough to be easy to work
      with. 
    </p>

    <activity>
      <statement>
	<p>
	  We will work here with a dataset having 100 3-dimensional
	  demeaned data points.  Evaluating the following cell will
	  plot those data points and define the demeaned data matrix
	  <c>A</c> whose shape is <m>3\times100</m>.
	  <sage>
	    <input>
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/pca_demo.py', globals())
	    </input>
	  </sage>
	  Notice that the data appears to cluster around a plane
	  though it does not seem to be wholly contained within that
	  plane.

	  <ol marker="a.">
	    <li>
	      <p>
		Use the matrix <c>A</c> to construct the covariance
		matrix <m>C</m>.  Then determine the variance in the
		direction of 
		<m>\uvec=\threevec{1/3}{2/3}{2/3}</m>?
		<sage>
		  <input>

		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Find the eigenvalues of <m>C</m> and determine the
		total variance.
		<sage>
		  <input>
		  </input>
		</sage>
		Notice that Sage does not necessarily sort the
		eigenvalues in decreasing order.
	      </p>
	    </li>

	    <li>
	      <p>
		Use the <c>right_eigenmatrix()</c> command to find the
		eigenvectors of <m>C</m>.  Remembering that the Sage
		command <c>B.column(1)</c> retrieves the vector
		represented by the second column of <c>B</c>,
		define vectors <c>u1</c>, <c>u2</c>, and <c>u3</c>
		representing the three principal components in order of
		decreasing eigenvalues.  How can you check if these
		vectors are an orthonormal basis for <m>\real^3</m>?
	      </p>  
	    </li>

	    <li>
	      <p>
		What fraction of the total variance is retained by
		projecting the data onto <m>W_1</m>, the subspace
		spanned by <m>\uvec_1</m>?  What fraction of the total
		variance is retained by projecting onto <m>W_2</m>,
		the subspace spanned by <m>\uvec_1</m> and
		<m>\uvec_2</m>?  What fraction of the total variance
		do we lose by projecting onto <m>W_2</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		If we project a data point <m>\xvec</m> onto
		<m>W_2</m>, the Projection Formula tells us we obtain
		<me>
		  \xhat = (\uvec_1\cdot\xvec) \uvec_1 +
		  (\uvec_2\cdot\xvec) \uvec_2.
		</me>
		Rather than viewing the projected data in
		<m>\real^3</m>, we will record the coordinates of
		<m>\xhat</m> in the basis defined by <m>\uvec_1</m>
		and <m>\uvec_2</m>;  that is, we will record the
		coordinates
		<me>
		  \twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}.  
		</me>
		Construct the matrix <m>Q</m> so that <m>Q^T\xvec =
		\twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}</m>.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Since each column of <m>A</m> represents a data point,
		the matrix <m>Q^TA</m> represents the coordinates of
		the projected data points.  Evaluating the following
		cell will plot those projected data points.
		<sage>
		  <input>
		    pca_plot(Q.T*A)
		  </input>
		</sage>
		Notice how this plot enables us to view the data as if
		it were two-dimensional.
		Why is this plot wider than it is tall?
	      </p>
	    </li>
	  </ol>

	</p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		After constructing the covariance matrix <m>C =
		\frac{1}{100}AA^T</m>, we find that <m>V_{\uvec} =
		\uvec\cdot(C\uvec) = 7885</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The total variance <m>V</m> is the sum of the
		eigenvalues of <m>C</m> so we obtain
		<m>V=12195</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		If we obtain <m>P</m>, the matrix of eigenvectors,
		from Sage, computing <m>P^TP</m> evaluates the dot
		products between the columns.  Since <m>P^TP=I</m>,
		the basis provided by Sage is orthonormal.
	      </p>
	    </li>
	    <li>
	      <p>
		Projecting onto <m>W_1</m>, we see that
		<m>\lambda_1/V = 0.83</m> so <m>W_1</m> retains about
		<m>83\%</m> of the total variance.  The subspace
		<m>W_2</m> retains <m>(\lambda_1+\lambda_2)/V=0.98</m>
		or <m>98\%</m> of the total variance.  If we project
		onto <m>W_2</m> we lose less than <m>2\%</m> of the
		variance. 
	      </p>
	    </li>
	    <li>
	      <p>
		<m>Q=\begin{bmatrix}\uvec_1\amp\uvec_2\end{bmatrix}</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The plot is wider because the variance in the
		<m>\uvec_1</m> direction, which corresponds to the
		horizontal coordinate, is greater than the variance in
		the <m>\uvec_2</m> direction.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>V_{\uvec} = 7885</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>V=12195</m>
	      </p>
	    </li>
	    <li>
	      <p>
		If <m>P</m> is the matrix of eigenvectors, evaluate
		<m>P^TP</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>W_1</m> retains 
		<m>83\%</m> of the total variance, and 
		<m>W_2</m> retains <m>98\%</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>Q=\begin{bmatrix}\uvec_1\amp\uvec_2\end{bmatrix}</m>
	      </p>
	    </li>
	    <li>
	      <p>
		Because the variance in the <m>\uvec_1</m> direction
		is greater than the variance in the <m>\uvec_2</m>
		direction.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
    </activity>

    <p>
      This example is a more realistic illustration of principal
      component analysis.  The plot of the 3-dimensional data
      appears to show that the data lies close to a plane, and the
      principal components will identify this plane.  Starting with
      the <m>3\times100</m> matrix of demeaned data <m>A</m>, we
      construct the covariance matrix <m>C=\frac{1}{100} ~AA^T</m> and
      study its eigenvalues.  Notice that the first two principal
      components account for more than 98% of the variance, which
      means we can expect the points to lie close to <m>W_2</m>, the
      two-dimensional subspace spanned by <m>\uvec_1</m> and
      <m>\uvec_2</m>.
    </p>

    <p>
      Since <m>W_2</m> is a subspace of <m>\real^3</m>, 
      projecting the data points onto <m>W_2</m> gives a list of 100
      points in <m>\real^3</m>.  In order to visualize them more
      easily, we instead consider the coordinates of the projections
      in the basis defined by <m>\uvec_1</m> and <m>\uvec_2</m>.  For
      instance, we know that the projection of a data point
      <m>\xvec</m> is
      <me>
	\xhat = (\uvec_1\cdot\xvec)\uvec_1 +
	(\uvec_2\cdot\xvec)\uvec_2,
      </me>
      which is a three-dimensional vector.  Instead, we can record the
      coordinates <m>\twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}</m>
      and plot them in the two-dimensional coordinate plane, as
      illustrated in <xref ref="fig-pca-coords" />.
    </p>

    <figure xml:id="fig-pca-coords">
      <sidebyside widths="50% 40%">
	<image source = "images/pca-proj">
          <shortdescription>
            A three dimensional vector orthogonally projected onto a
            plane defined by an orthonormal basis.
          </shortdescription>
          <description>
            <p>A three dimensional diagram with two orthonormal
            vectors <m>\uvec_1</m> and <m>\uvec_2</m> that define a
            plane.  In the plane is a grid that describes the linear
            combinations formed by <m>\uvec_1</m> and <m>\uvec_2</m>.
            A third vector <m>\xvec</m> that is not on the
            plane is shown along with its orthogonal projection
            <m>\xhat</m>.</p>
          </description>
        </image>
	<image source = "images/pca-coords">
          <shortdescription>
            A two dimensional view of the plane defined by the
            orthonormal basis.
          </shortdescription>
          <description>
            <p>This two dimensional diagram shows the plane defined by
            the orthonormal basis <m>\uvec_1</m> and <m>\uvec_2</m>.
            The vector <m>\uvec_1</m> points horizontally to the right
            while <m>\uvec_2</m> points vertically upward.  There is a
            standard <m>1\times1</m> coordinate grid, which
            illustrates the linear combinations of <m>\uvec_1</m> and
            <m>\uvec_2</m>.  The vector
            <m>\xhat=\twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}</m>
            is also shown.</p>
          </description>
        </image>
      </sidebyside>
      <caption>
	The projection <m>\xhat</m> of a data point <m>\xvec</m> onto
	<m>W_2</m> is a three-dimensional vector, which may be
	represented by the two coordinates describing this vector as a
	linear combination of <m>\uvec_1</m> and <m>\uvec_2</m>.
      </caption>
    </figure>

    <p>
      If we form the matrix <m>Q=\begin{bmatrix}\uvec_1 \amp \uvec_2
      \end{bmatrix}</m>, then we have
      <me>
	Q^T\xvec = \twovec{\uvec_1\cdot\xvec}{\uvec_2\cdot\xvec}.
      </me>
      This means that the columns of <m>Q^TA</m> represent the
      coordinates of the projected points, which may now be plotted in
      the plane.
    </p>

    <p>
      In this plot, the first coordinate, represented by the
      horizontal coordinate, represents the projection of a data point
      onto the line defined by <m>\uvec_1</m> while the second
      coordinate represents the projection onto the line defined by
      <m>\uvec_2</m>.  Since <m>\uvec_1</m> is the first principal
      component, the variance in the <m>\uvec_1</m> direction is
      greater than the variance in the <m>\uvec_2</m> direction.  For
      this reason, the plot will be more spread out in the horizontal
      direction than in the vertical.
    </p>

  </subsection>

  <subsection>
    <title> Using Principal Component Analysis </title>

    <p>
      Now that we've explored the ideas behind principal component
      analysis, we will look at a few examples that illustrate its use.
    </p>

    <activity>
      <statement>
	<p>
	  The next cell will load a dataset describing the average
	  consumption of various food groups for citizens in each of
	  the four nations of the United Kingdom.  The units for each
	  entry are grams per person per week.
	  <sage>
	    <input>
import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/uk-diet.csv', index_col=0)
data_mean = vector(df.T.mean())
A = matrix([vector(row) for row in (df.T-df.T.mean()).values]).T
df

	    </input>
	  </sage>
	  We will view this as a dataset consisting of four points in
	  <m>\real^{17}</m>.  As such, it is impossible to visualize
	  and studying the numbers themselves doesn't lead to much
	  insight.
	</p>

	<p>
	  In addition to loading the data, evaluating the cell above
	  created a vector <c>data_mean</c>, which is the mean of the
	  four data points, and <c>A</c>, the <m>17\times4</m> matrix
	  of demeaned data.
	  <ol marker="a.">
	    <li>
	      <p>
		What is the average consumption of Beverages across
		the four nations?
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Find the covariance matrix <m>C</m> and its
		eigenvalues.  Because there are four points in
		<m>\real^{17}</m> whose mean is zero, there are only
		three nonzero eigenvalues.  
	      </p>
	    </li>

	    <li>
	      <p>
		For what percentage of the total variance does the
		first principal component account?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the first principal component <m>\uvec_1</m> and
		project the four demeaned data points onto the line
		defined by <m>\uvec_1</m>.  Plot those points on <xref
		ref="fig-pca-1d" />
	      </p>

	      <figure xml:id="fig-pca-1d">
		<sidebyside width="90%">
		  <image source = "images/pca-plot-1">
                    <shortdescription>
                      A number line suitable for
                      plotting the projected data.
                    </shortdescription>
                    <description>
                      <p>A horizontal number line running from
                      <m>-500</m> on the left to <m>500</m> on the
                      right.</p>
                    </description>
                  </image>
		</sidebyside>
		<caption>
		  A plot of the demeaned data projected onto the
		  first principal component.
		</caption>
	      </figure>
	    </li>

	    <li>
	      <p>
		For what percentage of the total variance do the first
		two principal components account?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the coordinates of the demeaned data points
		projected onto <m>W_2</m>, the two-dimensional
		subspace of <m>\real^{17}</m> spanned by
		the first two principal components.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>

	      <p>
		Plot these coordinates in <xref ref="fig-pca-2d" />.
	      </p>

	      <figure xml:id="fig-pca-2d">
		<sidebyside width="90%">
		  <image source="images/pca-plot-2">
                    <shortdescription>
                      A coordinate grid and set of axes suitable for
                      plotting the projected data.
                    </shortdescription>
                    <description>
                      <p>A coordinate grid and set of axes suitable
                      for plotting the projected data.  The horizontal
                      range runs from <m>-500</m> to <m>500</m>, and
                      the vertical range runs from <m>-300</m> to
                      <m>300</m>.</p>
                    </description>
                  </image>
		</sidebyside>
		<caption>
		  The coordinates of the demeaned data points
		  projected onto the first two principal components.
		</caption>
	      </figure>
	    </li>

	    <li>
	      <p>
		What information do these plots reveal that is not
		clear from consideration of the original data points?
	      </p>
	    </li>

	    <li>
	      <p>
		Study the first principal component <m>\uvec_1</m>
		and find the first component of <m>\uvec_1</m>, which
		corresponds to the dietary category Alcoholic Drinks.
		(To do this, you may wish to use
		<c>N(u1, digits=2)</c> for a result that's easier to read.)
		If a data point lies on the far right side of the plot
		in <xref ref="fig-pca-2d" />, what does it mean about
		that nation's consumption of Alcoholic Drinks?
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		Beverages is the second category so this would be the
		second component of the <c>data_mean</c> vector,
		which is <m>57.5</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The three nonzero eigenvalues are <m>78805</m>,
		<m>33946</m>, and <m>4093</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The total variance <m>V=116844</m> is the sum of the
		eigenvalues so the first principal component accounts
		for <m>\lambda_1/V = 67\%</m> of the total variance.
	      </p>
	    </li>
	    <li>
	      <p>
		The coordinates are
		<table>
		  <tabular halign="right">
		    <row bottom="minor">
		      <cell> Nation </cell>
		      <cell> Coordinate </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>-145</m> </cell>
		    </row>
		    <row>
		      <cell> Northern Ireland </cell>
		      <cell> <m>477</m> </cell>
		    </row>
		    <row>
		      <cell> Scotland </cell>
		      <cell> <m>-92</m> </cell>
		    </row>
		    <row>
		      <cell> Wales </cell>
		      <cell> <m>-241</m> </cell>
		    </row>
		  </tabular>
		</table>
	      </p>
	    </li>

	    <li>
	      <p>
		The first two principal components account for
		<m>96\%</m> of the total variance.
	      </p>
	    </li>
	    <li>
	      <p>
		The coordinates are
		<table>
		  <tabular halign="right">
		    <row bottom="minor">
		      <cell> Nation </cell>
		      <cell> Coordinates </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>(-145, 3)</m> </cell>
		    </row>
		    <row>
		      <cell> Northern Ireland </cell>
		      <cell> <m>(477, 59)</m> </cell>
		    </row>
		    <row>
		      <cell> Scotland </cell>
		      <cell> <m>(-92, -286)</m> </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>(-241, 225)</m> </cell>
		    </row>
		  </tabular>
		</table>
	      </p>
	    </li>

	    <li>
	      <p>
		Northern Ireland appears to be significantly different
		from the other three nations.  There are several possible
		reasons for this, both historical and geographical,
		that we might explore.
	      </p>
	    </li>
	    <li>
	      <p>
		The first component of <m>\uvec_1</m> is negative.
		Therefore, if a nation is on the right side of this
		plot, the average consumption of Alcoholic Drinks will
		be less than the mean.  This can be confirmed by
		looking at the original data.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>57.5</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>78805</m>,
		<m>33946</m>, and <m>4093</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>67\%</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The coordinates are
		<table>
		  <tabular halign="right">
		    <row bottom="minor">
		      <cell> Nation </cell>
		      <cell> Coordinate </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>-145</m> </cell>
		    </row>
		    <row>
		      <cell> Northern Ireland </cell>
		      <cell> <m>477</m> </cell>
		    </row>
		    <row>
		      <cell> Scotland </cell>
		      <cell> <m>-92</m> </cell>
		    </row>
		    <row>
		      <cell> Wales </cell>
		      <cell> <m>-241</m> </cell>
		    </row>
		  </tabular>
		</table>
	      </p>
	    </li>

	    <li>
	      <p>
		<m>96\%</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The coordinates are
		<table>
		  <tabular halign="right">
		    <row bottom="minor">
		      <cell> Nation </cell>
		      <cell> Coordinates </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>(-145, 3)</m> </cell>
		    </row>
		    <row>
		      <cell> Northern Ireland </cell>
		      <cell> <m>(477, 59)</m> </cell>
		    </row>
		    <row>
		      <cell> Scotland </cell>
		      <cell> <m>(-92, -286)</m> </cell>
		    </row>
		    <row>
		      <cell> England </cell>
		      <cell> <m>(-241, 225)</m> </cell>
		    </row>
		  </tabular>
		</table>
	      </p>
	    </li>

	    <li>
	      <p>
		Northern Ireland appears to be significantly different
		from the other three nations.
	      </p>
	    </li>
	    <li>
	      <p>
		The average consumption of Alcoholic Drinks will
		be less than the mean.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
    </activity>

    <p>
      This activity demonstrates how principal component analysis
      enables us to extract information from a dataset that may not be
      easily obtained otherwise.  As in our previous example, we see
      that the data points lie quite close to a
      two-dimensional subspace of <m>\real^{17}</m>.  In fact,
      <m>W_2</m>, the subspace spanned by the first two principal
      components, accounts for more than 96% of the variance.
      More importantly, when we project the data onto <m>W_2</m>, it
      becomes apparent that Northern Ireland is fundamentally
      different from the other three nations.
    </p>

    <p>
      With some additional thought, we can determine more specific
      ways in which Northern Ireland is different.  On the
      <m>2</m>-dimensional plot, Northern Ireland lies far to the right
      compared to the other three nations.  Since the data has been
      demeaned, the origin <m>(0,0)</m> in this plot corresponds to
      the average of the four nations.  The coordinates of the point
      representing Northern Ireland are about <m>(477, 59)</m>,
      meaning that the projected data point differs from the mean by
      about <m>477\uvec_1+59\uvec_2</m>.
    </p>

    <p>
      Let's just focus on the contribution from <m>\uvec_1</m>.  We
      see that the ninth component of <m>\uvec_1</m>, the one that
      describes Fresh Fruit, is about <m>-0.63</m>.  This means that
      the ninth component of <m>477\uvec_1</m> differs from the mean
      by about <m>477(-0.63) = -300</m> grams per person per week.  So
      roughly speaking, people in Northern Ireland are eating about
      300 fewer grams of Fresh Fruit than the average across the four
      nations.  This is borne out by looking at the original data,
      which show that the consumption of Fresh Fruit in Northern
      Ireland is significantly less than the other nations.  Examing
      the other components of <m>\uvec_1</m> shows other ways in
      which Northern Ireland differs from the other three nations.
    </p>

    <activity xml:id="activity-pca-iris">
      <statement>
	<p>
	  In this activity, we'll look at a
	  <url href="https://archive.ics.uci.edu/ml/datasets/Iris"
	       visual="archive.ics.uci.edu">
	  well-known dataset 
	  </url>
	  that describes 150 irises representing three species of
	  iris: Iris setosa, Iris versicolor, and Iris virginica.
	  For each flower, the length and width of
	  its sepal and the length and width of its petal, all in
	  centimeters, are recorded. 
	</p>

	<figure xml:id="fig-iris">
	  <sidebyside width="70%">
	    <image source="images/Iris_versicolor.jpg">
              <shortdescription>
                A photograph of an iris versicolor.
              </shortdescription>
              <description>
                <p>A photograph of an iris versicolor showing three
                shorter petals and three longer sepals.</p>
              </description>
            </image>
	  </sidebyside>
	  <caption>
	    One of the three species, iris versicolor, represented
	    in the dataset showing three shorter petals and three
	    longer sepals.  
	    (Source:
	    <url
		href="https://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpg"
		visual="gvsu.edu/s/21D">
	      Wikipedia
	      </url>,
	      License:
	      <url
		  href="https://commons.wikimedia.org/wiki/Commons:GNU_Free_Documentation_License,_version_1.2"
		  visual="gvsu.edu/s/21E">
		GNU Free Documetation License</url>)	      
	  </caption>
	</figure>

	<p>
	  Evaluating the following cell will load the dataset, which
	  consists of 150 points in <m>\real^4</m>.  In addition,
	  we have a vector <c>data_mean</c>, a four-dimensional
	  vector holding the mean of the data points, and <c>A</c>,
	  the <m>4\times150</m> demeaned data matrix.
	  <sage>
	    <input>
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/pca_iris.py', globals())
df.T


	    </input>
	  </sage>

	  Since the data is four-dimensional, we are not able to
	  visualize it.  Of course, we could forget about two of the
	  measurements and plot the 150 points represented by their,
	  say,  
	  sepal length and sepal width.

	  <sage>
	    <input>
sepal_plot()	      
	    </input>
	  </sage>
	  <ol marker="a.">
	    <li>
	      <p>
		What is the mean sepal width?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the covariance matrix <m>C</m> and its
		eigenvalues.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Find the fraction of variance for which the first two
		principal components account.
	      </p>
	    </li>

	    <li>
	      <p>
		Construct the first two principal components
		<m>\uvec_1</m> and <m>\uvec_2</m> along with the
		matrix <m>Q</m> whose columns are <m>\uvec_1</m> and
		<m>\uvec_2</m>.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		As we have seen, the columns of the matrix <m>Q^TA</m>
		hold the coordinates of the demeaned data points after
		projecting onto <m>W_2</m>, the subspace spanned by
		the first two principal components.  Evaluating the
		following cell shows a plot of these coordinates.
		<sage>
		  <input>
pca_plot(Q.T*A)		    
		  </input>
		</sage>

		Suppose we have a flower whose coordinates in this
		plane are <m>(-2.5, -0.75)</m>.  To what species does
		this iris most likely belong?  Find an estimate of the
		sepal length, sepal width, petal length, and petal
		width for this flower.
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose you have an iris, but you only know that its
		sepal length is 5.65 cm and its sepal width is 2.75
		cm.  Knowing only these two measurements, determine
		the coordinates <m>(c_1, c_2)</m> in the plane where
		this iris lies.  To what species does this iris most
		likely belong?  Now estimate the petal length and petal
		width of this iris.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose you find another iris whose sepal width is 3.2
		cm and whose petal width is 2.2 cm.  Find the
		coordinates <m>(c_1, c_2)</m> of this iris and
		determine the species to which it most likely
		belongs.  Also, estimate the sepal length and the
		petal length.
		<sage>
		  <input>
		  </input>
		</sage>
	      </p>
	    </li>

	  </ol>

      
	</p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		The second component of <c>data_mean</c>, which is the
		one corresponding to sepal width, is <m>3.05</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The eigenvalues are <m>4.20</m>, <m>0.24</m>,
		<m>0.08</m>, and <m>0.02</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The first two principal components account for
		<m>97.8\%</m> of the variance.
	      </p>
	    </li>
	    <li>
	      <p>
		If <m>P</m> is the matrix whose columns are an
		orthonormal basis of eigenvectors, then <m>Q</m> is
		formed from the first two columns of <m>P</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		This would most likely belong to Iris setosa.  To find
		its measurements, we evaluate <m>-2.5\uvec_1 -
		0.75\uvec_2 + \mvec</m> where <m>\mvec</m> is the
		vector of means.  This is the same as
		<m>Q\twovec{-2.5}{-0.75} + \mvec</m>, which
		gives the vector of
		measurements <m>\fourvec{5.43}{3.81}{1.49}{0.25}</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Subtracting the mean sepal length and sepal width, we
		have <m>(-0.19, -0.30)</m>.  Then the first two
		components of <m>c_1\uvec_1+c_2\uvec_2 =
		Q\twovec{c_1}{c_2} = \twovec{-0.19}{-0.30}</m>.  This
		gives <m>(c_1, c_2) = (0.18, 0.40)</m>.  This looks
		like an Iris versicolor.  As in the
		previous part, we can now find the petal length to be
		<m>3.99</m> and the petal width to be <m>1.29</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Using the same approach as the last part, we find
		<m>(c_1,c_2)=(2.90, -0.53)</m>, which gives a sepal
		length of <m>7.23</m> and a petal length of
		<m>6.15</m>.  Most likely, this flower belongs to Iris
		virginica. 
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>3.05</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>4.20</m>, <m>0.24</m>,
		<m>0.08</m>, <m>0.02</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>97.8\%</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The columns of <m>Q</m> are for the first two
		principal components.
	      </p>
	    </li>
	    <li>
	      <p>
		Iris setosa and the vector of measurements is
		<m>\fourvec{5.43}{3.81}{1.49}{0.25}</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The petal length is
		<m>3.99</m> and the petal width is <m>1.29</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The sepal length is <m>7.23</m> and the petal length
		is <m>6.15</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
		
    </activity>
      

  </subsection>

  <subsection>
    <title> Summary </title>

    <p>
      This section has explored principal component analysis as a
      technique to reduce the dimension of a dataset.  From the
      demeaned data matrix <m>A</m>, we form the covariance matrix
      <m>C= \frac1N ~AA^T</m>, where <m>N</m> is the number of data
      points. 
      <ul>
	<li>
	  <p>
	    The eigenvectors <m>\uvec_1, \uvec_2, \ldots \uvec_m</m>,
	    of <m>C</m> are called the principal components.  We arrange
	    them so that their corresponding eigenvalues are in
	    decreasing order.
	  </p>
	</li>

	<li>
	  <p>
	    If <m>W_n</m> is the subspace spanned by the first
	    <m>n</m> principal components, then the variance of the
	    demeaned data projected onto <m>W_n</m> is the sum of the
	    first <m>n</m> eigenvalues of <m>C</m>.  No other
	    <m>n</m>-dimensional subspace retains more variance when
	    the data is projected onto it.
	  </p>
	</li>

	<li>
	  <p>
	    If <m>Q</m> is the matrix whose columns are the first
	    <m>n</m> principal components, then the columns of
	    <m>Q^TA</m> hold the coordinates, expressed in the basis
	    <m>\uvec_1,\ldots,\uvec_n</m>, of the data once
	    projected onto <m>W_n</m>.
	  </p>
	</li>

	<li>
	  <p>
	    Our goal is to use a number of principal components that
	    is large enough to retain most of the variance in the
	    dataset but small enough to be manageable.
	  </p>
	</li>
      </ul>
    </p>

  </subsection>

  <xi:include href="exercises/exercises7-3.xml" />
  
</section>
