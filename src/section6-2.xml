<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-transpose"
	 xmlns:xi="http://www.w3.org/2001/XInclude">
  <title> Orthogonal complements and the matrix transpose </title>

  <introduction>

    <p>
      We've now seen how the dot product enables us to determine the
      angle between two vectors and, more specifically, when two
      vectors are orthogonal.  Moving forward, we will explore how the
      orthogonality condition simplifies many common tasks, such as
      expressing a vector as a linear combination of a given set of
      vectors.
    </p>

    <p>
      This section introduces the notion of an orthogonal complement,
      the set of vectors each of which is orthogonal to a prescribed
      subspace.  We'll also find a way to describe dot products using
      matrix products, which allows us to study orthogonality using
      many of the tools for understanding linear systems that we
      developed earlier.
    </p>

    <exploration label="ula-preview-6-2">
      <task label="ula-preview-6-2-a"
            attachment="yes">
        <statement>
	  <p> Sketch the vector <m>\vvec=\twovec{-1}2</m> on
	  <xref ref="fig-pa-6-2" /> and one vector that is orthogonal to it.
          </p>
          <p component="rs-preview">You can upload a scan of your
          sketch below.</p>
	  <figure xml:id="fig-pa-6-2">
	    <sidebyside width="50%">
	      <image source="images/empty-4" />
	    </sidebyside>
	    <caption>
	      Sketch the vector <m>\vvec</m> and one vector
	      orthogonal to it.
	    </caption>
	  </figure>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    The vector <m>\wvec=\twovec21</m> is an example of a
	    vector orthogonal to <m>\vvec</m>.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-6-2-b">
        <statement>
	  <p>
	    If a vector <m>\xvec</m> is orthogonal to
	    <m>\vvec</m>, what do we know about the dot product
	    <m>\vvec\cdot\xvec</m>?
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    The dot product must be zero.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-6-2-c">
        <statement>
	  <p>
	    If we write <m>\xvec=\twovec xy</m>, use the dot product to
	    write an equation for the vectors orthogonal to <m>\vvec</m>
	    in terms of <m>x</m> and <m>y</m>.
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    <m>\vvec\cdot\xvec = -x+2y = 0</m>.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-6-2-d">
        <statement>
          <p>
	    Use this equation to sketch the set of all vectors orthogonal
	    to <m>\vvec</m> in <xref ref="fig-pa-6-2" />.
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    This is the line <m>y=\frac12 x</m>.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-6-2-e">
        <statement>
	  <p>
	    <xref ref="sec-subspaces" /> introduced the
	    column space <m>\col(A)</m> and null space
	    <m>\nul(A)</m> of a matrix <m>A</m>.
	    If <m>A</m> is a matrix, what is the meaning of the null
	    space <m>\nul(A)</m>?
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    It is the set of vectors <m>\xvec</m> for which
	    <m>A\xvec = \zerovec</m>.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-6-2-f">
        <statement>
	  <p>
	    What is the meaning of the column space <m>\col(A)</m>?
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    It is the set of vector <m>\bvec</m> for which the
	    equation <m>A\xvec=\bvec</m> is consistent.
	  </p>
        </solution>
      </task>

      <task component="rs-preview">
        <query label="ula-preview-6-2-poll" visibility="instructor">
          <statement>
            <p>I feel confident with the material in this activity.</p>
          </statement>
          <choices>
            <choice><p>Strongly Agree</p></choice>
            <choice><p>Agree</p></choice>
            <choice><p>Neutral</p></choice>
            <choice><p>Disagree</p></choice>
            <choice><p>Strongly Disagree</p></choice>
          </choices>
        </query>
      </task>

      <task component="rs-preview"
            label="ula-preview-6-2-what-else">
        <statement>
          <p>What would you need to know to feel
          more confident about this material?</p>
        </statement>
        <response/>
      </task>
    </exploration>
  </introduction>
  
  <subsection>
    <title> Orthogonal complements </title>

    <p>
      The preview activity presented us with a vector <m>\vvec</m> and
      led us through the process of describing all the vectors
      orthogonal to <m>\vvec</m>.  Notice that the set of scalar
      multiples of <m>\vvec</m>
      describes a line <m>L</m>, a
      1-dimensional subspace of <m>\real^2</m>.
      We then described a second line consisting of all the vectors
      orthogonal to <m>\vvec</m>.  Notice that every vector on this
      line is orthogonal to every vector on the line <m>L</m>.  We
      call this new line the <em>orthogonal complement</em> of
      <m>L</m> and denote it by <m>L^\perp</m>.  The lines <m>L</m>
      and <m>L^\perp</m> are illustrated on the left of <xref
      ref="fig-orthog-comps" />.
    </p>

    <figure xml:id="fig-orthog-comps">
      <sidebyside widths="45% 45%">
	<image source="images/orthog-lines" />
	<image source="images/orthog-comp" />
      </sidebyside>
      <caption>
	On the left is a line <m>L</m> and its orthogonal complement
	<m>L^\perp</m>.  On the right is a plane <m>W</m> and its
	orthogonal complement <m>W^\perp</m> in <m>\real^3</m>.
      </caption>
    </figure>

    <p>
      The next definition places this example into a more general context.
    </p>

    <definition>
      <idx>orthogonal complement</idx>
      <statement>
	<p>
	  Given a subspace <m>W</m> of <m>\real^m</m>, the
	  <term>orthogonal complement</term> of <m>W</m> is the set of
	  vectors in <m>\real^m</m> each of which is orthogonal to
	  every vector in <m>W</m>.  We denote the orthogonal
	  complement by <m>W^\perp</m>.
	</p>
      </statement>
    </definition>

    <p>
      A typical example appears on the right of <xref
      ref="fig-orthog-comps" />.  Here we see a plane <m>W</m>, a
      two-dimensional subspace of <m>\real^3</m>, and its orthogonal
      complement <m>W^\perp</m>, which is a line in <m>\real^3</m>.
    </p>

    <p>
      As the next activity demonstrates, the orthogonal complement of
      a subspace <m>W</m> is itself a subspace of <m>\real^m</m>.
    </p>

    <activity>
      <statement>
	<p>
	  Suppose that <m>\wvec_1=\threevec10{-2}</m> and
	  <m>\wvec_2=\threevec11{-1}</m> form a basis for
	  <m>W</m>, a two-dimensional subspace of
	  <m>\real^3</m>.  We will find a description of the
	  orthogonal complement <m>W^\perp</m>. 
	  <ol marker="a.">
	    <li>
	      <p>
		Suppose that the vector <m>\xvec</m> is orthogonal to
		<m>\wvec_1</m>.  If we write
		<m>\xvec=\threevec{x_1}{x_2}{x_3}</m>,
		use the fact that <m>\wvec_1\cdot\xvec =
		0</m> to write a linear
		equation for <m>x_1</m>, <m>x_2</m>, and <m>x_3</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Suppose that <m>\xvec</m> is also orthogonal to
		<m>\wvec_2</m>.  In the same way, write a linear
		equation for <m>x_1</m>, <m>x_2</m>, and <m>x_3</m>
		that arises from the fact that <m>\wvec_2\cdot\xvec =
		0</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		If <m>\xvec</m> is orthogonal to both <m>\wvec_1</m>
		and <m>\wvec_2</m>, these two equations give us a
		linear system <m>B\xvec=\zerovec</m> for some matrix
		<m>B</m>.  Identify the matrix <m>B</m> and write a
		parametric description of the solution space to the
		equation <m>B\xvec = \zerovec</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Since <m>\wvec_1</m> and <m>\wvec_2</m> form a basis
		for the two-dimensional subspace <m>W</m>, any vector
		<m>\wvec</m> in <m>W</m> can be written as a linear
		combination
		<me>
		  \wvec = c_1\wvec_1 + c_2\wvec_2\text{.}
		</me>
		If <m>\xvec</m> is orthogonal to both <m>\wvec_1</m>
		and <m>\wvec_2</m>, use the distributive property of
		dot products to explain why <m>\xvec</m> is orthogonal
		to <m>\wvec</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Give a basis for the orthogonal complement
		<m>W^\perp</m> and state the dimension <m>\dim W^\perp</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Describe <m>(W^\perp)^\perp</m>, the orthogonal
		complement of <m>W^\perp</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		We have the equation <m>\wvec_1\cdot\xvec = x_1 - 2x_3
		= 0</m>. 
	      </p>
	    </li>
	    <li>
	      <p>
		We have the equation <m>\wvec_2\cdot\xvec = x_1+x_2 - x_3
		= 0</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		These two equations give <m>B\xvec=\zerovec</m> where
		<me>
		  B = \begin{bmatrix}
		  1 \amp 0 \amp -2 \\
		  1 \amp 1 \amp -1 \\
		  \end{bmatrix}
		  \sim
		  \begin{bmatrix}
		  1 \amp 0 \amp -2 \\
		  0 \amp 1 \amp 1 \\
		  \end{bmatrix}\text{,}
		</me>
		whose solutions have the parametric form
		<m>\xvec=x_3\threevec2{-1}1</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		By distributivity,
		<m>(c_1\wvec_1+c_2\wvec_2)\cdot\xvec=
		c_1\wvec_1\cdot\xvec + c_2\wvec_2\cdot\xvec = 0</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>W^\perp</m> is the solution space to the equation
		<m>B\xvec=\zerovec</m>.  Therefore, a basis consists
		of the single vector
		<m>\threevec2{-1}1</m>, and <m>W^\perp</m> is
		one-dimensional.
	      </p>
	    </li>
	    <li>
	      <p>
		Since every vector in <m>W^\perp</m> is orthogonal to
		every vector in <m>W</m>, the orthogonal complement of
		<m>W^\perp</m> is <m>W</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>\wvec_1\cdot\xvec = x_1 - 2x_3
		= 0</m>. 
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\wvec_2\cdot\xvec = x_1+x_2 - x_3
		= 0</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>
		  B = \begin{bmatrix}
		  1 \amp 0 \amp -2 \\
		  1 \amp 1 \amp -1 \\
		  \end{bmatrix}
		</m>
		so
		<m>\xvec=x_3\threevec2{-1}1</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		By distributivity,
		<m>(c_1\wvec_1+c_2\wvec_2)\cdot\xvec=
		c_1\wvec_1\cdot\xvec + c_2\wvec_2\cdot\xvec = 0</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		A basis consists of
		<m>\threevec1{-1}1</m>, and <m>W^\perp</m> is
		one-dimensional.
	      </p>
	    </li>
	    <li>
	      <p>
		The orthogonal complement of
		<m>W^\perp</m> is <m>W</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
    </activity>

    <example xml:id="example-orthog-comp-line">
      <statement>
	<p>
	  If <m>L</m> is the line 
	  defined by <m>\vvec=\threevec1{-2}3</m> in <m>\real^3</m>,
	  we will describe the orthogonal complement <m>L^\perp</m>,
	  the set of vectors orthogonal to <m>L</m>.
	</p>

	<p>
	  If <m>\xvec</m> is orthogonal to <m>L</m>, it must be
	  orthogonal to <m>\vvec</m> so we have
	  <me>
	    \vvec\cdot\xvec = x_1-2x_2+3x_3 = 0\text{.}
	  </me>
	</p>

	<p>
	  We can describe the solutions to this equation
	  parametrically as
	<me>
	  \xvec=\threevec{x_1}{x_2}{x_3} =
	  \cthreevec{2x_2-3x_3}{x_2}{x_3} = 
	  x_2\threevec210+x_3\threevec{-3}01\text{.}
	</me>
	Therefore, the orthogonal complement <m>L^\perp</m> is a
	plane, a two-dimensional subspace of <m>\real^3</m>,
	spanned by the vectors <m>\threevec210</m> and
	<m>\threevec{-3}01</m>. 
	</p>
      </statement>
    </example>

    <example xml:id="example-orthog-comp-gen">
      <statement>
	<p>
	  Suppose that <m>W</m> is the <m>2</m>-dimensional subspace
	  of <m>\real^5</m> with basis
	  <me>
	    \wvec_1=\fivevec{-1}{-2}23{-4},\hspace{24pt}
	    \wvec_2=\fivevec24202\text{.}
	  </me>
	  We will give a description of the orthogonal complement
	  <m>W^\perp</m>.
	</p>

	<p>
	  If <m>\xvec</m> is in <m>W^\perp</m>, we know that
	  <m>\xvec</m> is orthogonal to both <m>\wvec_1</m> and
	  <m>\wvec_2</m>.  Therefore,
	  <md>
	    <mrow>
	      \wvec_1\cdot\xvec \amp {}={}-x_1-2x_2+2x_3+3x_4-4x_5
	      \amp {}={} 0
	    </mrow>
	    <mrow>
	      \wvec_2\cdot\xvec \amp {}={} 2x_1+4x_2+2x_3+0x_4+2x_5
	      \amp {}={} 0
	    </mrow>
	  </md>
	  In other words, <m>B\xvec=\zerovec</m> where
	<me>
	  B = 
	  \begin{bmatrix}
	  -1 \amp -2 \amp 2 \amp 3 \amp -4 \\
	  2 \amp 4 \amp 2 \amp 0 \amp 2
	  \end{bmatrix}
	  \sim
	  \begin{bmatrix}
	  1 \amp 2 \amp 0 \amp -1 \amp 2 \\
	  0 \amp 0 \amp 1 \amp 1 \amp -1
	  \end{bmatrix}\text{.}
	</me>
	The solutions may be described parametrically as
	<me>
	  \xvec=\fivevec{x_1}{x_2}{x_3}{x_4}{x_5}
	  =x_2\fivevec{-2}1000 + x_4\fivevec10{-1}10 +
	  x_5\fivevec{-2}0101\text{.}
	</me>
	The distributive property of dot products implies that any
	vector that is orthogonal to both <m>\wvec_1</m> and
	<m>\wvec_2</m> is also orthogonal to any linear combination of
	<m>\wvec_1</m> and <m>\wvec_2</m> since
	<me>
	  (c_1\wvec_1 + c_2\wvec_2)\cdot\xvec = c_1\wvec_1\cdot\xvec +
	  c_2\wvec_2\cdot\xvec = 0\text{.}
	</me>
	Therefore, <m>W^\perp</m> is a <m>3</m>-dimensional subspace
	of <m>\real^5</m> with basis
	<me>
	  \vvec_1=\fivevec{-2}1000, \hspace{24pt}
	  \vvec_2=\fivevec10{-1}10, \hspace{24pt}
	  \vvec_3=\fivevec{-2}0101\text{.}
	</me>
	One may check that the vectors <m>\vvec_1</m>, <m>\vvec_2</m>,
	and <m>\vvec_3</m> are each orthogonal to both <m>\wvec_1</m>
	and <m>\wvec_2</m>.
	</p>
      </statement>
    </example>

  </subsection>
  
  <subsection>
    <title> The matrix transpose </title>

    <p>
      The previous activity and examples show how we can describe the
      orthogonal complement of a subspace as the solution set of a
      particular linear system.  We will make this connection more
      explicit by defining a new matrix operation called the
      <em>transpose</em>.
    </p>

    <definition>
      <idx>transpose</idx>
      <statement>
	<p> The <term>transpose</term> of the <m>m\times n</m> matrix
	<m>A</m> is the <m>n\times m</m> matrix <m>A^T</m> whose rows
	are the columns of <m>A</m>.
	</p>
      </statement>
    </definition>

    <example>
      <statement>
	<p> If
	<m>A=\begin{bmatrix}
	4 \amp -3 \amp 0 \amp 5 \\
	-1 \amp 2 \amp 1 \amp 3 \\
	\end{bmatrix}
	</m>,
	then
	<m>A^T=\begin{bmatrix}
	4 \amp -1 \\
	-3 \amp 2 \\
	0 \amp 1 \\
	5 \amp 3 \\
	\end{bmatrix}
	</m></p>
      </statement>
    </example>

    <activity>
      <statement>
	<p> This activity illustrates how multiplying a vector by
	<m>A^T</m> is related to computing dot products with the columns
	of <m>A</m>.  You'll develop a better understanding of this
	relationship if you compute the dot products and matrix products
	in this activity without using technology.
	<ol marker="a.">
	  <li><p> If
	  <m>B =
	  \begin{bmatrix}
	  3 \amp 4 \\
	  -1 \amp 2 \\
	  0 \amp -2 \\
	  \end{bmatrix}
	  </m>, write the matrix <m>B^T</m>.
	  </p></li>

	  <li><p> Suppose that
	  <me>
	    \vvec_1=\threevec20{-2},\hspace{24pt}
	    \vvec_2=\threevec112,\hspace{24pt}
	    \wvec=\threevec{-2}23\text{.}
	  </me>
	  Find the dot products <m>\vvec_1\cdot\wvec</m> and
	  <m>\vvec_2\cdot\wvec</m>.
	  </p></li>

	  <li><p> Now write the matrix
	  <m>A = \begin{bmatrix} \vvec_1 \amp \vvec_2 \end{bmatrix}</m> and
	  its transpose <m>A^T</m>.  Find the product <m>A^T\wvec</m> and
	  describe how this product computes both dot products
	  <m>\vvec_1\cdot\wvec</m> and <m>\vvec_2\cdot\wvec</m>.
	  </p></li>

	  <li><p> Suppose that <m>\xvec</m> is a vector that is
	  orthogonal to both <m>\vvec_1</m> and <m>\vvec_2</m>.  What
	  does this say about the dot products <m>\vvec_1\cdot\xvec</m>
	  and <m>\vvec_2\cdot\xvec</m>?  What does this say about the
	  product <m>A^T\xvec</m>?
	  </p></li>

	  <li><p> Use the matrix <m>A^T</m> to give a parametric
	  description of all the vectors <m>\xvec</m> that are
	  orthogonal to <m>\vvec_1</m>
	  and <m>\vvec_2</m>.  
	  </p></li>

	  <li>
	    <p>
	      Remember that <m>\nul(A^T)</m>, the null space of
	      <m>A^T</m>, is the solution set of the equation
	      <m>A^T\xvec=\zerovec</m>.  If <m>\xvec</m> is a vector in
	      <m>\nul(A^T)</m>, explain why <m>\xvec</m> must be
	      orthogonal to both <m>\vvec_1</m> and <m>\vvec_2</m>.
	    </p>
	  </li>
	  <li>
	    <p>
	      Remember that <m>\col(A)</m>, the column space of <m>A</m>,
	      is the set of linear combinations of the columns of
	      <m>A</m>.  Therefore, any vector in <m>\col(A)</m> can be
	      written as <m>c_1\vvec_1+c_2\vvec_2</m>.
	      If <m>\xvec</m> is a vector in <m>\nul(A^T)</m>, explain
	      why <m>\xvec</m> is orthogonal to every vector in
	      <m>\col(A)</m>.
	    </p>
	  </li>
	</ol>
	</p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>A^T = \begin{bmatrix}
		3 \amp -1 \amp 0 \\
		4 \amp 2 \amp -2
		\end{bmatrix}
		</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<md>
		  <mrow>
		    \vvec_1\cdot\wvec \amp{}={} 2(-2) + 0(2) + (-2)3
		    \amp{}={} -10
		  </mrow>
		  <mrow>
		    \vvec_2\cdot\wvec \amp{}={} 1(-2) + 1(2) + 2(3)
		    \amp{}={} 6
		  </mrow>
		</md>
	      </p>
	    </li>
	    <li>
	      <p>
		<me>A^T\wvec =
		\twovec{2(-2)+0(2)+(-2)3}{1(-2)+1(2)+2(3)} =
		\twovec{-10}{6}
		=\twovec{\vvec_1\cdot\wvec}{\vvec_2\cdot\wvec}
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		Both dot products are 0 so we have <m>A^T\xvec =
		\zerovec</m>.
	      </p>
	    </li>
	    <li>
	      <p> We need to solve the equation <m>A^T\xvec =
	      \zerovec</m> so we find the reduced row echelon form 
	      <me>A^T \sim \begin{bmatrix}
		1 \amp 0 \amp -1 \\
		0 \amp 1 \amp 3\\
		\end{bmatrix}\text{.}
	      </me>
	      The vectors orthogonal to both <m>\vvec_1</m> and
	      <m>\vvec_2</m> have the form
	      <m>\xvec=x_3\threevec1{-3}1</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A^T\xvec =
		\twovec{\vvec_1\cdot\xvec}{\vvec_2\cdot\xvec} =
		\twovec00</m> tells us that <m>\vvec_1\cdot\xvec=0</m>
		and <m>\vvec_2\cdot\xvec=0</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Since <m>\xvec</m> is orthogonal to both
		<m>\vvec_1</m> and <m>\vvec_2</m>, we have
		<me>
		  (c_1\vvec_1+c_2\vvec_2)\cdot\xvec =
		  c_1\vvec_1\cdot\xvec + c_2\vvec_2\cdot\xvec = 0
		</me>
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>A^T = \begin{bmatrix}
		3 \amp -1 \amp 0 \\
		4 \amp 2 \amp -2
		\end{bmatrix}
		</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\vvec_1\cdot\wvec =-10</m>,
		<m>\vvec_2\cdot\wvec =6</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<me>A^T\wvec 
		=\twovec{\vvec_1\cdot\wvec}{\vvec_2\cdot\wvec}
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A^T\xvec =
		\zerovec</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\xvec=x_3\threevec1{-3}1</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A^T\xvec =
		\twovec{\vvec_1\cdot\xvec}{\vvec_2\cdot\xvec} =
		\twovec00</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Apply the distributive property of dot products.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
		    
    </activity>

    <p>
      The previous activity demonstrates an important connection
      between the matrix transpose and dot products.  More
      specifically, the components of the product <m>A^T\xvec</m> are
      simply the dot products of the columns of <m>A</m> with
      <m>\xvec</m>.  We will make frequent use of this observation
      so let's record it as a proposition.
    </p>

    <proposition xml:id="prop-transpose-multiplication">
      <statement>
	<p>
	  If <m>A</m> is the matrix whose columns are
	  <m>\vvec_1,\vvec_2,\ldots,\vvec_n</m>, then <me>A^T\xvec =
	  \begin{bmatrix}
	  \vvec_1\cdot\xvec \\
	  \vvec_2\cdot\xvec \\
	  \vdots \\
	  \vvec_n\cdot\xvec \\
	  \end{bmatrix}
	</me>
	</p>
      </statement>
    </proposition>
	  
    <example>
      <statement>
	<p>
	  Suppose that <m>W</m> is a subspace of <m>\real^4</m> having
	  basis 
	  <me>
	    \wvec_1=\fourvec1021,\hspace{24pt}
	    \wvec_2=\fourvec2134\text{,}
	  </me>
	  and that we wish to describe the orthogonal complement
	  <m>W^\perp</m>.
	</p>

	<p>
	  If <m>A</m> is 
	  the matrix <m>A = \begin{bmatrix}\wvec_1 \amp
	  \wvec_2\end{bmatrix}</m> and <m>\xvec</m> is in
	  <m>W^\perp</m>, we have
	  <me>
	    A^T\xvec = \twovec{\wvec_1\cdot\xvec}{\wvec_2\cdot\xvec} =
	    \twovec00\text{.}
	  </me>
	  Describing vectors <m>\xvec</m> that are orthogonal to both
	  <m>\wvec_1</m> and <m>\wvec_2</m> is therefore equivalent to
	  the more familiar task of describing the solution set
	  <m>A^T\xvec = \zerovec</m>.  To do so, we find the reduced
	  row echelon form of <m>A^T</m> and write the solution set
	  parametrically as
	  <me>
	    \xvec = x_3\fourvec{-2}{1}10 +
	    x_4\fourvec{-1}{-2}01\text{.}
	  </me>
	  Once again, the distributive property of dot products tells
	  us that such a vector is also orthogonal to any linear
	  combination of <m>\wvec_1</m> and <m>\wvec_2</m> so this
	  solution set is, in fact, the orthogonal complement
	  <m>W^\perp</m>.  Indeed, we see that the vectors
	  <me>
	    \vvec_1=\fourvec{-2}110,\hspace{24pt}
	    \vvec_2=\fourvec{-1}{-2}01
	  </me>
	  form a basis for <m>W^\perp</m>, which is a two-dimensional
	  subspace of <m>\real^4</m>.
	</p>
      </statement>
    </example>

    <p>
      To place this example in a slightly more general context, note that
      <m>\wvec_1</m> and <m>\wvec_2</m>, the columns of
      <m>A</m>, form a basis of <m>W</m>.  Since <m>\col(A)</m>, the
      column space of <m>A</m> is the subspace of linear combinations
      of the columns of <m>A</m>, we have <m>W=\col(A)</m>.
    </p>
    
    <p>
      This example also shows that the orthogonal complement
      <m>W^\perp = \col(A)^\perp</m> is described by the solution set
      of <m>A^T\xvec = \zerovec</m>.  This solution set is what we
      have called <m>\nul(A^T)</m>, the null space of <m>A^T</m>.  In
      this way, we see the following proposition, which is visually
      represented in <xref ref="fig-orthog-comp" />.
    </p>

    <proposition xml:id="prop-col-orthog">
      <statement>
	<p> For any matrix <m>A</m>, the orthogonal complement of
	<m>\col(A)</m> is 
	<m>\nul(A^T)</m>; that is,
	<me>
	  \col(A)^\perp = \nul(A^T)\text{.}
	</me>
	</p>
      </statement>
    </proposition>
 	
    <figure xml:id="fig-orthog-comp">
      <sidebyside width="45%">
	<image source = "images/nul-at" />
      </sidebyside>
      <caption>
	The orthogonal complement of the column space of <m>A</m> is
	the null space of <m>A^T</m>.
      </caption>
    </figure>
	

  </subsection>

  <subsection>
    <title> Properties of the matrix transpose </title>
 
    <p> The transpose is a simple algebraic operation performed on a
    matrix.  The next activity explores some of its properties.
    </p>

    <activity>
      <statement>
	<p> In Sage, the transpose of a matrix <c>A</c> is given by
	<c>A.T</c>.  Define the matrices
	<me>
	  A =
	  \begin{bmatrix}
	  1 \amp 0 \amp -3 \\
	  2 \amp -2 \amp 1 \\
	  \end{bmatrix},
	  \hspace{6pt}
	  B =
	  \begin{bmatrix}
	  3 \amp -4 \amp 1 \\
	  0 \amp 1 \amp 2 \\
	  \end{bmatrix},
	  \hspace{6pt}
	  C=
	  \begin{bmatrix}
	  1 \amp 0 \amp -3 \\
	  2 \amp -2 \amp 1 \\
	  3 \amp 2 \amp 0 \\
	  \end{bmatrix}\text{.}
	</me>
	<sage>
	  <input>
	  </input>
	</sage>
	<ol marker="a.">
	  <li><p> Evaluate <m>(A+B)^T</m> and <m>A^T+B^T</m>.  What do
	  you notice about the relationship between these two matrices?
	  </p></li>

	  <li><p> What happens if you transpose a matrix twice;  that
	  is, what is <m>(A^T)^T</m>?
	  </p></li>

	  <li><p> Find <m>\det(C)</m> and <m>\det(C^T)</m>.  What do you
	  notice about the relationship between these determinants?
	  </p></li>

	  <li><p>
	    <ol marker="1."> 
	      <li><p> Find the product <m>AC</m> and its transpose
	      <m>(AC)^T</m>.
	      </p></li>
	      <li><p> Is it possible to compute the product <m>A^TC^T</m>?
	      Explain why or why not.
	      </p></li>
	      <li><p> Find the product <m>C^TA^T</m> and compare it to
	      <m>(AC)^T</m>.  What do you notice about the relationship
	      between these two matrices?
	      </p></li>
	  </ol> </p></li>

	  <li><p> What is the transpose of the identity matrix
	  <m>I</m>?
	  </p></li>

	  <li><p> If a square matrix <m>D</m> is invertible, explain why
	  you can guarantee that <m>D^T</m> is invertible and why
	  <m>(D^T)^{-1} = (D^{-1})^T</m>.
	  </p></li>

	</ol></p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>(A+B)^T = A^T + B^T</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>(A^T)^T = A</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\det(C^T) = \det(C)</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<ol marker="1.">
		  <li>
		    <p>
		      <m> AC = \begin{bmatrix}
		      -8 \amp -6 \amp -3 \\
		      1 \amp 6 \amp -8
		      \end{bmatrix}
		      </m> and
		      <m>(AC)^T = \begin{bmatrix}
		      -8 \amp 1 \\
		      -6 \amp 6 \\
		      -3 \amp -8
		      \end{bmatrix}
		      </m>
		    </p>
		  </li>
		  <li>
		    <p>
		      The product <m>A^TC^T</m> is not defined because
		      <m>A^T</m> has two columns and <m>C^T</m> has
		      three rows.
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>(AC)^T=C^TA^T</m>
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>I^T=I</m>
	      </p>
	    </li>
	    <li>
	      <p>
		We have <m>DD^{-1} = I</m> so <m>(DD^{-1})^T =
		(D^{-1})^TD^T = 
		I^T=I</m>.  This means that <m>(D^T)^{-1} =
		(D^{-1})^T</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
	    
      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>(A+B)^T = A^T + B^T</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>(A^T)^T = A</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>\det(C^T) = \det(C)</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>(AC)^T=C^TA^T</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>I^T=I</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>(D^T)^{-1} = (D^{-1})^T</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
		      
    </activity>
	
    <p> In spite of the fact that we are looking at some specific
    examples, this activity demonstrates the following general
    properties of the transpose, which may be verified with a little
    effort. 
    </p>

    <assemblage>
      <title> Properties of the transpose </title>

      <p> Here are some properties of the matrix
      transpose, expressed in terms of general matrices <m>A</m>,
      <m>B</m>, and <m>C</m>.  We assume that <m>C</m> is a square
      matrix. 

      <ul>
	<li><p> If <m>A+B</m> is defined, then <m>(A+B)^T =
	A^T+B^T</m>.
	</p></li>

	<li><p> <m>(sA)^T = sA^T</m>.</p></li>

	<li><p> <m>(A^T)^T = A</m>.
	</p></li>

	<li><p> <m>\det(C) = \det(C^T)</m>.
	</p></li>

	<li><p> If <m>AB</m> is defined, then <m>(AB)^T = B^TA^T</m>.
	Notice that the order of the multiplication is reversed.
	</p></li>

	<li><p> If <m>C</m> is invertible, then <m>(C^T)^{-1} =
	(C^{-1})^T</m>.
	</p></li>

      </ul></p>
    </assemblage>

    <p>
      There is one final property we wish to record though we will
      wait until <xref ref="sec-svd-intro" /> to explain why it is
      true.
    </p>

    <proposition xml:id="prop-col-row-rank">
      <statement>
	For any matrix <m>A</m>, we have
	<me>\rank(A) = \rank(A^T)\text{.}</me>
      </statement>
    </proposition>

    <p>
      This proposition is important because it implies a relationship
      between the dimensions of a subspace and its orthogonal
      complement.  For instance, if <m>A</m> is an <m>m\times n</m>
      matrix, we saw in <xref ref="sec-subspaces" /> that
      <m>\dim\col(A) = \rank(A)</m> and <m>\dim\nul(A) =
      n-\rank(A)</m>.
    </p>

    <p>
      Now suppose that <m>W</m> is an <m>n</m>-dimensional subspace of
      <m>\real^m</m> with basis
      <m>\wvec_1,\wvec_2,\ldots,\wvec_n</m>.  If we form the
      <m>m\times n</m> matrix <m>A=\begin{bmatrix}\wvec_1 \amp \wvec_2
      \amp \ldots \amp \wvec_n\end{bmatrix}</m>, then <m>\col(A) = W</m> so that
      <me>
	\rank(A) = \dim\col(A) = \dim W = n\text{.}
      </me>
    </p>

    <p>
      The transpose <m>A^T</m> is an <m>n\times m</m> matrix having
      <m>\rank(A^T) = \rank(A)= n</m>.  Since <m>W^\perp =
      \nul(A^T)</m>, we have
      <me>
	\dim W^\perp = \dim\nul(A^T) = m - \rank(A^T) = m-n =
	m-\dim W\text{.}
      </me>
      This explains the following proposition.
    </p>

    <proposition xml:id="prop-orthog-dim">
      <statement>
	<p> If <m>W</m> is a subspace of <m>\real^m</m>, then
	<me>
	  \dim W + \dim W^\perp = m\text{.}
	</me>
	</p>
      </statement>
    </proposition>

    <example>
      <statement>
	<p> In <xref ref="example-orthog-comp-line"/>, we
	constructed the orthogonal complement of a line in
	<m>\real^3</m>.  The dimension of the orthogonal complement
	should be <m>3 - 1 = 2</m>, which explains why we found the
	orthogonal complement to be a plane.
	</p>
      </statement>
    </example>

    <example>
      <statement>
	<p> In <xref ref="example-orthog-comp-gen" />, we looked at
	<m>W</m>, a <m>2</m>-dimensional subspace of <m>\real^5</m>
	and found its orthogonal complement <m>W^\perp</m> to be a
	<m>5-2=3</m>-dimensional subspace of <m>\real^5</m>.
	</p>
      </statement>
    </example>

    <activity>
      <statement>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		Suppose that <m>W</m> is a <m>5</m>-dimensional
		subspace of <m>\real^9</m> and that <m>A</m> is a matrix whose
		columns form a basis for <m>W</m>;  that is, <m>\col(A) =
		W</m>.
		<ol marker="1.">
		  <li><p> What is the shape of <m>A</m>? </p></li>
		  
		  <li><p> What is the rank of <m>A</m>? </p></li>
		  
		  <li><p> What is the shape of <m>A^T</m>? </p></li>
		  
		  <li><p> What is the rank of <m>A^T</m>? </p></li>
		  
		  <li><p> What is <m>\dim\nul(A^T)</m>?  </p></li>
		  
		  <li><p> What is <m>\dim W^\perp</m>? </p></li>
		  
		  <li><p> How are the dimensions of <m>W</m> and
		  <m>W^\perp</m> related? </p></li>
		</ol>
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose that <m>W</m> is a subspace of <m>\real^4</m>
		having basis
		<me>
		  \wvec_1 = \fourvec102{-1},\hspace{24pt}
		  \wvec_2 = \fourvec{-1}2{-6}3.
		</me>
		<ol marker="1.">
		  <li>
		    <p>
		      Find the dimensions <m>\dim W</m> and <m>\dim
		      W^\perp</m>.
		    </p>
		  </li>

		  <li>
		    <p>
		      Find a basis for <m>W^\perp</m>.  It may be
		      helpful to know that the Sage command
		      <c>A.right_kernel()</c> produces a basis for
		      <m>\nul(A)</m>.
		      <sage>
			<input>
			</input>
		      </sage>
		    </p>
		  </li>

		  <li>
		    <p>
		      Verify that each of the basis vectors you found
		      for <m>W^\perp</m> are orthogonal to the basis
		      vectors for <m>W</m>.
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>
      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<ol marker="1.">
		  <li>
		    <p>
		      <m>A</m> is <m>9\times 5</m>.
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>\rank(A) = 5</m>.
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>A^T</m> is <m>5\times 9</m>.
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>\rank(A^T) = 5</m>
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>\dim \nul(A^T) = 9 - 5 = 4</m>
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>\dim W^\perp = \dim\nul(A^T) = 4</m>
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>\dim W + \dim W^\perp = 9 </m> since the
		      subspaces live in <m>\real^9</m>.
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>
	    <li>
	      <p>
		<ol marker="1.">
		  <li>
		    <p>
		      <m>\dim W = 2</m> so <m>\dim W^\perp =
		      4-2=2</m>.
		    </p>
		  </li>
		  <li>
		    <p>A basis is
		    <m>\vvec_1=\fourvec1{-1}01</m> and
		    <m>\vvec_2=\fourvec0012</m> .
		    </p>
		  </li>
		  <li>
		    <p>You can verify by computing the four dot
		    products.
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<ol marker="1.">
		  <li>
		    <p>
		      <m>9\times 5</m>
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>5</m>
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>5\times 9</m>
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>5</m>
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>4</m>
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>4</m>
		    </p>
		  </li>
		  <li>
		    <p>
		      <m>\dim W + \dim W^\perp = 9 </m>
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>
	    <li>
	      <p>
		<ol marker="1.">
		  <li>
		    <p>
		      <m>2</m> 
		    </p>
		  </li>
		  <li>
		    <p>
		    <m>\fourvec1{-1}01</m> and
		    <m>\fourvec0012</m> .
		    </p>
		  </li>
		  <li>
		    <p>Verify by computing the four dot
		    products.
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
    </activity>

  </subsection>

  <subsection>
    <title> Summary </title>

    <p>
      This section introduced the matrix transpose, its connection
      to dot products, and its use in describing the orthogonal
      complement of a subspace.
      <ul>
	<li>
	  <p>
	    The columns of the matrix <m>A</m> are the rows of the
	    matrix transpose <m>A^T</m>.
	  </p>
	</li>
	<li>
	  <p>
	    The components of the product <m>A^T\xvec</m> are the dot
	    products of <m>\xvec</m> with the columns of <m>A</m>.
	  </p>
	</li>

	<li>
	  <p>
	    The orthogonal complement of the column space of
	    <m>A</m> equals the null space of <m>A^T</m>;  that is,
	    <m>\col(A)^\perp = \nul(A^T)</m>.
	  </p>
	</li>

	<li>
	  <p> If <m>W</m> is a subspace of <m>\real^p</m>, then
	  <me>
	    \dim W + \dim W^\perp = p.
	  </me>
	  </p>
	</li>
      </ul>
    </p>

  </subsection>

  <xi:include href="exercises/exercises6-2.xml" />

</section>
