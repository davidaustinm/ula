<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-quadratic-forms"
	 xmlns:xi="http://www.w3.org/2001/XInclude">
  <title> Quadratic forms </title>

  <introduction>

    <p>
      With our understanding of symmetric matrices and variance in
      hand, we'll now explore how to determine the directions in which
      the variance of a dataset is as large as possible and where it
      is as small as possible.  This is part of a much larger story
      involving a type of function, called a <em>quadratic form</em>,
      that we'll introduce here.
    </p>

    <exploration xml:id="preview-quadforms"
                 label="ula-preview-7-2">
      <introduction>
	<p>
	  Let's begin by looking at an example.  Suppose we have three
	  data points that form the demeaned data matrix
	  <me>
	    A = \begin{bmatrix}
	    2 \amp 1 \amp -3 \\
	    1 \amp 2 \amp -3 \\
	    \end{bmatrix}
	  </me>.
        </p>
      </introduction>

      <task label="ula-preview-7-2-a"
            attachment="yes">
        <statement>
	  <p>
	    Plot the demeaned data points in <xref
	    ref="fig-quad-preview" />.  In which direction does
	    the variance appear to be largest and in which does it
	    appear to be smallest?
	  </p>
          <p component="rs-preview">You can scan and upload your sketch below.</p>
	  <figure xml:id="fig-quad-preview">
	    <sidebyside width="50%">
	      <image source="images/empty-4" />
	    </sidebyside>
	    <caption>
	      Use this coordinate grid to plot the demeaned data
	      points.
	    </caption>
	  </figure>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    The variance appears to be greatest in the direction
	    of <m>\twovec11</m> and smallest in the direction of
	    <m>\twovec{-1}1</m>.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-7-2-b">
        <statement>
          <p>
	    Construct the covariance matrix <m>C</m> and determine
	    the variance in the direction of <m>\twovec11</m> and
	    the variance in the direction of <m>\twovec{-1}1</m>.
	    <sage>
	      <input>
	      </input>
	    </sage>
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    In the direction of <m>\twovec11</m>, the variance is
	    <m>9</m>, while in the direction of
	    <m>\twovec{-1}1</m>, the variance is <m>1/3</m>.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-7-2-c">
        <statement>
	  <p>
	    What is the total variance of this dataset?
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    The total variance is <m>9+1/3 = 28/3</m>.
	  </p>
        </solution>
      </task>

      <task label="ula-preview-7-2-d">
        <statement>
	  <p>
	    Generally speaking, if <m>C</m> is the covariance matrix
	    of a dataset and <m>\uvec</m> is an eigenvector of
	    <m>C</m> having unit length and with associated
	    eigenvalue <m>\lambda</m>, what is <m>V_{\uvec}</m>?
	  </p>
        </statement>
        <response component="rs-preview"/>
        <solution>
	  <p>
	    <m>V_{\uvec} = \uvec\cdot(C\uvec) =
	    \lambda\uvec\cdot\uvec = \lambda</m>
	  </p>
        </solution>
      </task>

      <task component="rs-preview">
        <query label="ula-preview-7-2-poll" visibility="instructor">
          <statement>
            <p>I feel confident with the material in this activity.</p>
          </statement>
          <choices>
            <choice><p>Strongly Agree</p></choice>
            <choice><p>Agree</p></choice>
            <choice><p>Neutral</p></choice>
            <choice><p>Disagree</p></choice>
            <choice><p>Strongly Disagree</p></choice>
          </choices>
        </query>
      </task>

      <task component="rs-preview"
            label="ula-preview-7-2-what-else">
        <statement>
          <p>What would you need to know to feel
          more confident about this material?</p>
        </statement>
        <response/>
      </task>

    </exploration>

  </introduction>

  <subsection>
    <title> Quadratic forms </title>

    <p>
      Given a matrix <m>A</m> of <m>N</m> demeaned data points,
      the symmetric covariance matrix <m>C=\frac1N AA^T</m> determines
      the variance in a particular direction
      <me>
	V_{\uvec} = \uvec\cdot(C\uvec),
      </me>
      where <m>\uvec</m> is a unit vector defining the direction.
    </p>

    <p>
      More generally, a symmetric <m>m\times m</m> matrix
      <m>A</m> defines a function <m>q:\real^m \to \real</m>
      by
      <me>
	q(\xvec) = \xvec\cdot(A\xvec).
      </me>
      Notice that this expression is similar to the one we use to find
      the variance <m>V_{\uvec}</m> in terms of the covariance matrix
      <m>C</m>.  The only difference is that we allow <m>\xvec</m> to
      be any vector rather than requiring it to be a unit vector.
    </p>

    <example>
      <statement>
	<p>
	  Suppose that
	  <m>A=\begin{bmatrix}
	  1 \amp 2\\
	  2 \amp 1
	  \end{bmatrix}
	  </m>.  If we write <m>\xvec=\twovec{x_1}{x_2}</m>, then we have
	  <md>
	    <mrow>
	      q\left(\twovec {x_1}{x_2}\right) \amp = \twovec
	      {x_1}{x_2} \cdot
	      \left(
	      \begin{bmatrix}
	      1 \amp 2 \\
	      2 \amp 1
	      \end{bmatrix}
	      \twovec {x_1}{x_2}
	      \right)
	    </mrow>
	    <mrow>
	      \amp = \twovec {x_1}{x_2} \cdot
	      \twovec{x_1 + 2x_2}{2x_1 + x_2}
	    </mrow>
	    <mrow>
	      \amp = x_1^2 + 2x_1x_2 + 2x_1x_2 + x_2^2
	    </mrow>
	    <mrow>
	      \amp = x_1^2 + 4x_1x_2 + x_2^2.
	    </mrow>
	  </md>
	</p>

	<p>
	  We may evaluate the quadratic form using some input vectors:
	  <me>
	    q\left(\twovec 10\right) = 1, \hspace{24pt}
	    q\left(\twovec 11\right) = 6, \hspace{24pt}
	    q\left(\twovec 24\right) = 52.
	  </me>
	  Notice that the value of the quadratic form is a scalar.
	</p>
      </statement>
    </example>

    <definition>
      <idx> quadratic form </idx>
      <statement>
	<p>
	  If <m>A</m> is a symmetric <m>m\times m</m> matrix, the
	  <em>quadratic form</em> defined by <m>A</m> is the function
	  <m>q_A(\xvec) = \xvec\cdot(A\xvec)</m>.  
	</p>
      </statement>
    </definition>

    <activity>
      <statement>
	<p>
	  Let's look at some more examples of quadratic forms.
	  <ol marker="a.">
	    <li>
	      <p>
		Consider the symmetric matrix
		<m>D = \begin{bmatrix}
		3 \amp 0 \\
		0 \amp -1 \\
		\end{bmatrix}
		</m>.  Write the quadratic form <m>q_D(\xvec)</m> defined by
		<m>D</m> in terms of the components of
		<m>\xvec=\twovec{x_1}{x_2}</m>.  What is the value of 
		<m>q_D\left(\twovec2{-4}\right)</m>? 
	      </p>
	    </li>

	    <li>
	      <p>
		Given the symmetric matrix
		<m>A=\begin{bmatrix}
		2 \amp 5 \\
		5 \amp -3
		\end{bmatrix}
		</m>, write the quadratic form <m>q_A(\xvec)</m> defined
		by <m>A</m> and evaluate
		<m>q_A\left(\twovec{2}{-1}\right)</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose that <m>q\left(\twovec{x_1}{x_2}\right) = 3x_1^2
		- 4x_1x_2 + 4x_2^2</m>.  Find a symmetric matrix
		<m>A</m> 
		such that <m>q</m> is the quadratic form defined by
		<m>A</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose that <m>q</m> is a quadratic form and that
		<m>q(\xvec) = 3</m>.  What is <m>q(2\xvec)</m>? 
		<m>q(-\xvec)</m>? <m>q(10\xvec)</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose that <m>A</m> is a symmetric matrix and
		<m>q_A(\xvec)</m> is the quadratic form defined by
		<m>A</m>.  Suppose that <m>\xvec</m> is an eigenvector
		of <m>A</m> with associated eigenvalue -4 and with
		length 7.  What is <m>q_A(\xvec)</m>?
	      </p>
	    </li>

	  </ol>
	</p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>q_D(\xvec) = 3x_1^2 - x_2^2</m> and
		<m>q_D\left(\twovec2{-4}\right) = -4</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>q_A(\xvec) = 2x_1^2 + 10x_1x_2-3x_2^2</m> and 
		<m>q_A\left(\twovec2{-1}\right) = -15</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A=\begin{bmatrix}
		3 \amp -2 \\
		-2 \amp 4 \\
		\end{bmatrix}</m>
	      </p>
	    </li>
	    <li>
	      <p>
		Notice that <m>q(2\xvec) = (2\xvec)\cdot(A(2\xvec)) =
		4\xvec\cdot(A\xvec) = 4q(\xvec)</m>.  In the same way,
		we have <m>q(-\xvec) = q(\xvec)</m> and <m>q(10\xvec)
		= 100q(\xvec)</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>q_A(\xvec) = \xvec\cdot(A\xvec) = -4\xvec\cdot\xvec
		= -4(49) = -196</m>
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
	      
      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>q_D(\xvec) = 3x_1^2 - x_2^2</m> and
		<m>q_D\left(\twovec2{-4}\right) = -4</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>q_A(\xvec) = 2x_1^2 + 10x_1x_2-3x_2^2</m> and 
		<m>q_A\left(\twovec2{-1}\right) = -15</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A=\begin{bmatrix}
		3 \amp -2 \\
		-2 \amp 4 \\
		\end{bmatrix}</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>q(2\xvec) = 12</m>, 
		<m>q(-\xvec) = 3</m> and <m>q(10\xvec)=300</m>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>q_A(\xvec) = -196</m>
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
	      
    </activity>

    <p>
      Linear algebra is principally about things that are linear.
      However, quadratic forms, as the name implies,  
      have a distinctly non-linear character.  First, if
      <m>A=\begin{bmatrix} a \amp b \\ b \amp c \end{bmatrix}</m>, is
      a symmetric matrix, then the associated quadratic form is
      <me>
	q_A\left(\twovec{x_1}{x_2}\right) = ax_1^2 + 2bx_1x_2 + cx_2^2.
      </me>
      Notice how the variables <m>x_1</m> and <m>x_2</m> are multiplied
      together, which tells us this isn't a linear function.
    </p>

    <p>
      This expression assumes an especially simple form when
      <m>D</m> is a diagonal matrix.  In particular, if
      <m>D = \begin{bmatrix}
      a \amp 0 \\
      0 \amp c \\
      \end{bmatrix}
      </m>, then <m>q_D\left(\twovec{x_1}{x_2}\right) = ax_1^2 +
      cx_2^2</m>.  This is special because there is no cross-term
      involving <m>x_1x_2</m>.
    </p>

    <p>
      Remember that matrix transformations have the property that
      <m>T(s\xvec) = sT(\xvec)</m>.  Quadratic forms behave
      differently:
      <me>
	q_A(s\xvec) = (s\xvec)\cdot(A(s\xvec)) = s^2\xvec\cdot(A\xvec)=
	s^2q_A(\xvec).
      </me>
      For instance, when we multiply <m>\xvec</m> by the scalar 2,
      then <m>q_A(2\xvec) = 4q_A(\xvec)</m>.  Also, notice that
      <m>q_A(-\xvec) = q_A(\xvec)</m> since the scalar is squared.
    </p>

    <p>
      Finally, evaluating a quadratic form on an eigenvector has a
      particularly simple form.  Suppose that <m>\xvec</m> is an
      eigenvector of <m>A</m> with associated eigenvalue
      <m>\lambda</m>.  We then have
      <me>
	q_A(\xvec) = \xvec\cdot(A\xvec) = \lambda\xvec\cdot\xvec =
	\lambda \len{\xvec}^2.
      </me>
    </p>

    <p>
      Let's now return to our motivating question:  in which direction
      <m>\uvec</m> is the variance <m>V_{\uvec}=\uvec\cdot(C\uvec)</m>
      of a dataset as 
      large as possible 
      and in which is it as small as possible.  Remembering that the
      vector <m>\uvec</m> is a unit vector, we can now state a more
      general form of this question:
      <em>
	If <m>q_A(\xvec)</m> is a quadratic form, for which unit
	vectors	<m>\uvec</m> is <m>q_A(\uvec)=\uvec\cdot(A\uvec)</m>
	as large as 
	possible and for which is it as small as possible?
      </em>
      Since a unit vector specifies a direction, we will often ask for
      the directions in which the quadratic form <m>q(\xvec)</m> is at
      its maximum or minimum value.
    </p>

    <activity>
      <statement>
	<p>
	  We can gain some intuition about this problem by graphing the
	  quadratic form and paying particular attention to the unit
	  vectors.
	  
	  <ol marker="a.">
	    <li>
	      <p>
		Evaluating the following cell defines the matrix
		<m>D = \begin{bmatrix}
		3 \amp 0 \\
		0 \amp -1
		\end{bmatrix}
		</m> and displays the graph of the associated quadratic form
		<m>q_D(\xvec)</m>.  In addition, the points corresponding
		to vectors <m>\uvec</m> with unit length are displayed
		as a curve. 
		<sage>
		  <input>
		    <!-->
x, y, t = var('x', 'y', 't')

## We define our matrix here
A = matrix(2, 2, [3,0,0,-1])    

def q(x,y):
   return vector([x,y])*(A*vector([x,y]))
graph = plot3d(q(x,y), (x,-1.2,1.2), (y,-1.2,1.2), color='orange', opacity=0.9, aspect_ratio=(1,1,1/max(matrix(RDF, A).singular_values())))
curve = parametric_plot3d([cos(t), sin(t), q(cos(t), sin(t))], (t,0,2*pi), thickness=3)   
graph + curve
		    -->
sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/quad_plot.py', globals())

## We define our matrix here
A = matrix(2, 2, [3, 0, 0, -1])

quad_plot(A)

		  </input>
		</sage>
		Notice that the matrix <m>D</m> is diagonal.  In which
		directions does the quadratic form have its maximum and
		minimum values? 
	      </p>
	    </li>

	    <li>
	      <p>
		Write the quadratic form <m>q_D</m> associated to
		<m>D</m>.  What is the value of
		<m>q_D\left(\twovec10\right)</m>?  What is the value of
		<m>q_D\left(\twovec01\right)</m>? 
	      </p>
	    </li>

	    <li>
	      <p>
		Consider a unit vector
		<m>\uvec=\twovec{u_1}{u_2}</m> so that <m>u_1^2+u_2^2 =
		1</m>, an expression we can rewrite as <m>u_1^2 =
		1-u_2^2</m>.  Write the quadratic form <m>q_D(\uvec)</m>
		and replace <m>u_1^2</m> by <m>1-u_2^2</m>.  Now explain
		why the maximum of <m>q_D(\uvec)</m> is 3.  In which
		direction does the maximum occur?  Does this agree with
		what you observed from the graph above?
	      </p>
	    </li>

	    <li>
	      <p>
		Write the quadratic form <m>q_D(\uvec)</m>
		and replace <m>u_2^2</m> by <m>1-u_1^2</m>.  What is the
		minimum value of <m>q_D(\uvec)</m> and in which
		direction does the minimum occur?
	      </p>
	    </li>

	    <li>
	      <p>
		Use the previous Sage cell to change the matrix to
		<m>A=\begin{bmatrix}
		1 \amp 2 \\
		2 \amp 1
		\end{bmatrix}
		</m> and display the graph of the quadratic form
		<m>q_A(\xvec) = \xvec\cdot(A\xvec)</m>.
		Determine the directions in which the maximum 
		and minimum occur.
	      </p>
	    </li>

	    <li>
	      <p>
		Remember that
		<m>A=\begin{bmatrix}
		1 \amp 2 \\
		2 \amp 1
		\end{bmatrix}
		</m> is symmetric so that <m>A=QDQ^T</m> where <m>D</m>
		is the diagonal matrix above and <m>Q</m> is the
		orthogonal matrix that rotates vectors by
		<m>45^\circ</m>.  Notice that
		<me>
		  q_A(\uvec) = \uvec\cdot(A\uvec) =
		  \uvec\cdot(QDQ^T\uvec) = (Q^T\uvec)\cdot(DQ^T\uvec)
		  = q_D(\vvec)
		</me>
		where <m>\vvec=Q^T\uvec</m>.  That is, we have
		<m>q_A(\uvec) = q_D(\vvec)</m>.
	      </p>
	      <p>
		Explain why <m>\vvec = Q^T\uvec</m> is also a unit
		vector;  that
		is, explain why
		<me>|\vvec|^2 = |Q^T\uvec|^2 =
		(Q^T\uvec)\cdot(Q^T\uvec) = 1.
		</me>
	      </p>
	    </li>

	    <li>
	      <p>
		Using the fact that <m>q_A(\uvec) = q_D(\vvec)</m>,
		explain how we now know the maximum value of
		<m>q_A(\uvec)</m> is 3 and determine the direction in
		which it occurs.  Also, determine the minimum value of
		<m>q_A(\uvec)</m> and determine the direction in which
		it occurs.
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		The maximum appears to occur in the direction of
		<m>\twovec10</m> and the minimum appears to occur in
		the direction of <m>\twovec01</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>q_D(\xvec) = 3x_1^2 - x_2^2</m> so that
		<m>q_D\left(\twovec10\right) = 3</m> and 
		<m>q_D\left(\twovec01\right) = -1</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		We have <m>q_D(\uvec) = 3u_1^2 - u_2^2 = 3 -
		4u_2^2</m> so that the quadratic form only depends on
		<m>u_2</m>.  The graph of this function of <m>u_2</m>
		is a parabola that has a maximum of <m>3</m> when
		<m>u_2=0</m>.  Since <m>u_1^2 = 1-u_2^2</m>, this
		means that the maximum occurs when <m>u_1=\pm 1</m>.
		We therefore see that the maximum value of
		<m>q_D(\uvec)</m> is <m>3</m> in the direction
		<m>\twovec10</m> as we saw from the graph.
	      </p>
	    </li>
	    <li>
	      <p>
		Now <m>q_D(\uvec) = -1 + 4u_1^2</m>, which has a
		minimum value of <m>-1</m> when <m>u_1=0</m>.
		Therefore, the minimum value of <m>q_D(\uvec)</m> is
		<m>-1</m> in the direction <m>\twovec01</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The graph of <m>q_A</m> appears to be similar to the
		graph of <m>q_D</m> only rotated by
		<m>45^\circ</m>. This means the maximum appears to
		occur in the direction <m>\twovec11</m> and the
		minimum in the direction <m>\twovec{-1}1</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Since <m>Q</m> is orthogonal, we have <m>QQ^T=I</m> so
		that 
		<me>|\vvec|^2 = (Q^T\uvec)\cdot(Q^T\uvec) =
		\uvec\cdot(QQ^T\uvec) = \uvec\cdot\uvec = |\uvec|^2 =
		1\text{.}</me>
	      </p>
	    </li>
	    <li>
	      <p>
		Since <m>q_A(\uvec) = q_D(\vvec)</m>, the maximum of
		<m>q_A(\uvec)</m> is <m>3</m>, which occurs when
		<m>\vvec = Q^T\uvec = \twovec10</m>.  This means that
		<m>\uvec=Q\twovec10 =
		\twovec{1/\sqrt{2}}{1/\sqrt{2}}</m>, the eigenvector
		of <m>A</m> associated to <m>\lambda=3</m>.
	      </p>
	      <p>
		In the same way, the minimum value of
		<m>q_A(\uvec)</m> is <m>-1</m>, which occurs when 
		<m>\uvec=\twovec{1/\sqrt{2}}{1/\sqrt{2}}</m>, the
		eigenvector of <m>A</m> associated to
		<m>\lambda=-1</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>

      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		The maximum appears to occur in the direction of
		<m>\twovec10</m> and the minimum appears to occur in
		the direction of <m>\twovec01</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>q_D(\xvec) = 3x_1^2 - x_2^2</m> so that
		<m>q_D\left(\twovec10\right) = 3</m> and 
		<m>q_D\left(\twovec01\right) = -1</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The maximum value of
		<m>q_D(\uvec)</m> is <m>3</m> in the direction
		<m>\twovec10</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The minimum value of
		<m>q_D(\uvec)</m> is <m>-1</m> in the direction
		<m>\twovec01</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The maximum appears to occur in the
		direction <m>\twovec11</m> and the minimum in the
		direction <m>\twovec{-1}1</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		Use the fact that <m>QQ^T=I</m>
	      </p>
	    </li>
	    <li>
	      <p>
		The maximum of
		<m>q_A(\uvec)</m> is <m>3</m>, which occurs when
		<m>\uvec=
		\twovec{1/\sqrt{2}}{1/\sqrt{2}}</m>.
	      </p>
	      <p>
		The maximum of
		<m>q_A(\uvec)</m> is <m>-1</m>, which occurs when
		<m>\uvec=
		\twovec{-1/\sqrt{2}}{1/\sqrt{2}}</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
    </activity>

    <p>
      This activity demonstrates how the eigenvalues of <m>A</m>
      determine the maximum and minimum values of the quadratic form
      <m>q_A(\uvec)</m> when evaluated on unit vectors and how the
      associated eigenvectors determine the directions in which the
      maximum and minimum values occur.  Let's look at another example
      so that this connection is clear.
    </p>

    <example>
      <statement>
	<p>
	  Consider the symmetric matrix
	  <m>A=\begin{bmatrix}
	  -7 \amp -6 \\
	  -6 \amp 2 \\
	  \end{bmatrix}</m>.  Because <m>A</m> is symmetric, we know
	  that it can be orthogonally diagonalized.  In fact, we have
	  <m>A=QDQ^T</m> where
	  <me>
	    D = \begin{bmatrix}
	    5 \amp 0 \\
	    0 \amp -10 \\
	    \end{bmatrix},\hspace{24pt}
	    Q = \begin{bmatrix}
	    1/\sqrt{5} \amp 2/\sqrt{5} \\
	    -2/\sqrt{5} \amp 1/\sqrt{5} \\
	    \end{bmatrix}\text{.}
	  </me>
	  From this diagonalization, we know that <m>\lambda_1=5</m> is
	  the largest eigenvalue of <m>A</m> with associated
	  eigenvector <m>\uvec_1 =
	  \twovec{1/\sqrt{5}}{-2/\sqrt{5}}</m> and that <m>\lambda_2 =
	  -10</m> is the smallest eigenvalue with associated
	  eigenvector <m>\uvec_2 =
	  \twovec{2/\sqrt{5}}{1/\sqrt{5}}</m>.
	</p>

	<p>
	  Let's first study the quadratic form <m>q_D(\uvec) = 5u_1^2
	  - 10u_2^2</m> because the absence of the cross-term makes it
	  comparatively simple.  Remembering that <m>\uvec</m> is a unit
	  vector, we have <m>u_1^2+u_2^2=1</m>, which means that
	  <m>u_1^2 = 1-u_2^2</m>.  Therefore,
	  <me>
	    q_D(\uvec) = 5u_1^2 - 10u_2^2 = 5(1-u_2^2)-10u_2^2 = 5 -
	    15u_2^2\text{.}
	  </me>
	  This tells us that <m>q_D(\uvec)</m> has a maximum value of
	  <m>5</m>, which occurs when <m>u_2=0</m> or in the direction
	  <m>\twovec10</m>.
	</p>

	<p>
	  In the same way, rewriting <m>u_2^2 = 1-u_1^2</m> allows us
	  to conclude that the minimum value of <m>q_D(\uvec)</m> is
	  <m>-10</m>, which occurs in the direction <m>\twovec01</m>.
	</p>

	<p>
	  Let's now return to the matrix <m>A</m> whose quadratic form
	  <m>q_A</m> is related to <m>q_D</m> because <m>A =
	  QDQ^T</m>.  In particular, we have
	  <me>
	    q_A(\uvec) = \uvec\cdot(A\uvec) = \uvec\cdot(QDQ^T\uvec) =
	    (Q^T\uvec)\cdot(DQ^T\uvec) = \vvec\cdot(D\vvec) =
	    q_D(\vvec)\text{.} 
	  </me>
	  In other words, we have <m>q_A(\uvec) = q_D(\vvec)</m>
	  where <m>\vvec=Q^T\uvec</m>.  This is quite useful because
	  it allows us to relate the values of <m>q_A</m> to those of
	  <m>q_D</m>, which we already understand quite well.
	</p>

	<p>
	  Now it turns out that <m>\vvec</m> is also a unit vector
	  because
	  <me>
	    |\vvec|^2 = \vvec\cdot\vvec = (Q^T\uvec)\cdot(Q^T\uvec) =
	    \uvec\cdot(QQ^T\uvec) = \uvec\cdot\uvec = |\uvec|^2 = 1\text.
	  </me>
	  Therefore, the maximum value of <m>q_A(\uvec)</m> is the
	  same as <m>q_D(\vvec)</m>, which we know to be <m>5</m> and
	  which occurs in the direction <m>\vvec=\twovec10</m>.  This
	  means that the maximum value of <m>q_A(\uvec)</m> is also
	  <m>5</m> and that this occurs in the direction <m>\uvec =
	  Q\vvec = Q\twovec10 = \twovec{1/\sqrt{5}}{-2/\sqrt{5}}</m>.
	  We now know that the maximum value of <m>q_A(\uvec)</m> is
	  the largest eigenvalue <m>\lambda_1=5</m> and that this
	  maximum value occurs in the direction of an associated
	  eigenvector. 
	</p>

	<p>
	  In the same way, we see that the minimum value of
	  <m>q_A(\uvec)</m> is the smallest eigenvalue
	  <m>\lambda_2=-10</m> and that this minimum occurs in the
	  direction of <m>\uvec=Q\twovec01 =
	  \twovec{2/\sqrt{5}}{1/\sqrt{5}}</m>, an associated
	  eigenvector.
	</p>
      </statement>
    </example>

    <p>
      More generally, we have
    </p>

    <proposition xml:id="prop-quadform-extrema">
      <statement>
	<p>
	  Suppose that <m>A</m> is a symmetric matrix, that we list
	  its eigenvalues in decreasing order 
	  <m>\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_m</m>, and that
	  <m>\uvec_1,\uvec_2,\ldots,\uvec_m</m> is a basis
	  of associated eigenvectors.
	  The maximum value of
	  <m>q_A(\uvec)</m> among all unit vectors <m>\uvec</m> is 
	  <m>\lambda_1</m>, which occurs in the direction
	  <m>\uvec_1</m>.  Similarly, the minimum value of
	  <m>q_A(\uvec)</m> is <m>\lambda_m</m>, which occurs in the
	  direction <m>\uvec_m</m>.
	</p>
      </statement>
    </proposition>

    <example>
      <statement>
	<p>
	  Suppose that <m>A</m> is the symmetric matrix
	  <m>A=\begin{bmatrix}
	  0 \amp 6 \amp 3 \\
	  6 \amp 3 \amp 6 \\
	  0 \amp 6 \amp 6 \\
	  \end{bmatrix}</m>, which may be orthogonally diagonalized as
	  <m>A=QDQ^T</m> where
	  <me>
	    D = \begin{bmatrix}
	    12 \amp 0 \amp 0 \\
	    0 \amp 3 \amp 0 \\
	    0 \amp 0 \amp -6 \\
	    \end{bmatrix}, \hspace{24pt}
	    Q = \begin{bmatrix}
	    1/3 \amp 2/3 \amp 2/3 \\
	    2/3 \amp 1/3 \amp -2/3 \\
	    2/3 \amp -2/3 \amp 1/3 \\
	    \end{bmatrix}\text{.}
	  </me>
	  We see that the maximum value of <m>q_A(\uvec)</m> is 12, which
	  occurs in the direction 
	  <m>\threevec{1/3}{2/3}{2/3}</m>, and the minimum value
	  is -6, which occurs in the direction
	  <m>\threevec{2/3}{-2/3}{1/3}</m>.
	</p>
      </statement>
    </example>

    <example>
      <statement>
	<p>
	  Suppose we have the matrix of demeaned data points
	  <m>A = \begin{bmatrix}
	    2 \amp 1 \amp -3 \\
	    1 \amp 2 \amp -3 \\
	    \end{bmatrix}</m> that we considered in <xref
	    ref="preview-quadforms" />.  The data points are shown in
	    <xref ref="fig-covariance-quad" />.
	</p>

	<figure xml:id="fig-covariance-quad">
	  <sidebyside width="50%">
	    <image source="images/quad-variance-data" />
	  </sidebyside>
	  <caption>
	      The set of demeaned data points from <xref
	      ref="preview-quadforms" />.
	  </caption>
	</figure>

	<p>
	  Constructing the covariance matrix <m>C=\frac13~AA^T</m> gives
	  <m>C=\begin{bmatrix}
	  14/3 \amp 13/3 \\
	  13/3 \amp 14/3 \end{bmatrix}</m>,
	  which has eigenvalues <m>\lambda_1
	  = 9</m>, with associated eigenvector
	  <m>\twovec{1/\sqrt{2}}{1/\sqrt{2}}</m>, and 
	  <m>\lambda_2=1/3</m>, with associated eigenvector
	  <m>\twovec{-1/\sqrt{2}}{1/\sqrt{2}}</m>.
	</p>

	<p>
	  Remember that the variance in a direction <m>\uvec</m> is
	  <m> V_{\uvec} = \uvec\cdot(C\uvec) = q_C(\uvec)</m>.
	  Therefore, the variance attains a maximum value of 9 in the
	  direction <m>\twovec{1/\sqrt{2}}{1/\sqrt{2}}</m> and a
	  minimum value of 1/3 in the 
	  direction <m>\twovec{-1/\sqrt{2}}{1/\sqrt{2}}</m>.  <xref
	  ref="fig-quad-project" 
	  /> shows the data projected onto the lines defined by these
	  vectors.
	</p>

	<figure xml:id="fig-quad-project">
	  <sidebyside widths="45% 45%">
	    <image source="images/quad-variance-a" />
	    <image source="images/quad-variance-b" />
	  </sidebyside>
	  <caption>
	    The demeaned data from <xref ref="preview-quadforms" />
	    is shown projected onto the lines of maximal and minimal
	    variance.
	  </caption>
	</figure>

	<p>
	  Remember that variance is additive, as stated in <xref
	  ref="prop-variance-additivity" />, which tells us that the
	  total variance is <m>V = 9 + 1/3 = 28/3</m>.
	</p>
      </statement>
    </example>

    <p>
      We've been focused on finding the directions in which a
      quadratic form attains its maximum and minimum values, but
      there's another important observation to make after this
      activity.  Recall how we used the fact that a symmetric matrix
      is orthogonally diagonalizable: if <m>A=QDQ^T</m>, then
      <m>q_A(\uvec) = q_D(\vvec)</m> where <m>\vvec = Q^T\uvec</m>.
    </p>

    <p>
      More generally, if we define <m>\yvec = Q^T\xvec</m>, we have
      <me>
	q_A(\xvec) = \xvec\cdot(A\xvec) =
	\xvec\cdot(QDQ^T\xvec) =
	(Q^T\xvec)\cdot(DQ^T\xvec) =
	\yvec\cdot(D\yvec) = q_D(\yvec)
      </me>
      Remembering that the quadratic form associated to a diagonal
      form has no cross terms, we obtain
      <me>
	q_A(\xvec) = q_D(\yvec) =
	\lambda_1y_1^2 + \lambda_2y_2^2 + \ldots + \lambda_my_m^2.
      </me>
      In other words, after a change of coordinates, the quadratic
      form <m>q_A</m> can be written without cross terms.  This is
      known as the Principal Axes Theorem.
    </p>

    <theorem>
      <title> Principal Axes Theorem </title>
      <statement>
	<p>
	  If <m>A</m> is a symmetric <m>m\times m</m> matrix with
	  eigenvalues <m>\lambda_1,\lambda_2,\ldots,\lambda_m</m>,
	  then the quadratic form <m>q_A</m> can be written, after an
	  orthogonal 
	  change of coordinates <m>\yvec=Q^T\xvec</m>, as
	  <me>
	    q_A(\xvec) =
	    \lambda_1y_1^2 + \lambda_2y_2^2 + \ldots +
	    \lambda_my_m^2.
	  </me>
	</p>
      </statement>
    </theorem>

    <p>
      We will put this to use in the next section.
    </p>

  </subsection>

  <subsection>
    <title>  Definite symmetric matrices </title>

    <p>
      While our questions about variance provide some motivation for
      exploring quadratic forms, these functions appear in a variety
      of other contexts so it's worth spending some more time with
      them.  For example, quadratic forms appear in multivariable
      calculus when describing the behavior of a function of several
      variables near a critical point and in physics when describing
      the kinetic energy of a rigid body.
    </p>

    <p>
      The following definition will be important in this section.
    </p>

    <definition>
      <statement>
	<p>
	  A symmetric matrix <m>A</m> is called <em>positive
	  definite</em> if its associated quadratic form satisfies
	  <m>q_A(\xvec) \gt 0</m> for any nonzero vector
	  <m>\xvec</m>.  If <m>q_A(\xvec) \geq 0</m> for all nonzero
	  vectors <m>\xvec</m>, we say that <m>A</m> is <em>positive 
	  semidefinite</em>.
	</p>

	<p>
	  Likewise, we say that <m>A</m> is <em>negative
	  definite</em> if <m>q_A(\xvec) \lt 0</m> for all nonzero
	  vectors <m>\xvec</m>. 
	</p>

	<p>
	  Finally, <m>A</m> is called <em>indefinite</em>
	  if <m>q_A(\xvec) \gt 0</m> for some <m>\xvec</m> and
	  <m>q_A(\xvec) \lt 0</m> for others.
	</p>
      </statement>
    </definition>

    <activity>
      <statement>
	<p>
	  This activity explores the relationship between the
	  eigenvalues 
	  of a symmetric matrix and its definiteness.
	  <ol marker="a.">
	    <li>
	      <p>
		Consider the diagonal matrix
		<m>D=\begin{bmatrix}
		4 \amp 0 \\
		0 \amp 2 \\
		\end{bmatrix}
		</m>
		and write its quadratic form <m>q_D(\xvec)</m> in terms
		of the components of <m>\xvec=\twovec{x_1}{x_2}</m>.
		How does this help you decide whether <m>D</m> is
		positive definite or not?
	      </p>
	    </li>

	    <li>
	      <p>
		Now consider
		<m>D=\begin{bmatrix}
		4 \amp 0 \\
		0 \amp 0 \\
		\end{bmatrix}
		</m>
		and write its quadratic form <m>q_D(\xvec)</m> in terms
		of <m>x_1</m> and <m>x_2</m>.  What can you say about
		the definiteness of <m>D</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		If <m>D</m> is a diagonal matrix, what condition on the
		diagonal entries guarantee that <m>D</m> is
		<ol marker="1.">
		  <li> <p> positive definite?</p></li>
		  <li> <p> positive semidefinite?</p></li>
		  <li> <p> negative definite?</p></li>
		  <li> <p> negative semidefinite?</p></li>
		  <li> <p> indefinite?</p></li>
		</ol>
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose that <m>A</m> is a symmetric matrix with
		eigenvalues 4 and 2 so that <m>A=QDQ^T</m> where
		<m>D=\begin{bmatrix}4 \amp 0 \\ 0 \amp 2
		\end{bmatrix}</m>.  If <m>\yvec = Q^T\xvec</m>, then we
		have <m>q_A(\xvec) = q_D(\yvec)</m>.  Explain why this
		tells us that <m>A</m> is positive definite.
	      </p>
	    </li>

	    <li>
	      <p>
		Suppose that <m>A</m> is a symmetric matrix with
		eigenvalues 4 and 0.  What can you say about the
		definiteness of <m>A</m> in this case?
	      </p>
	    </li>

	    <li>
	      <p>
		What condition on the eigenvalues of a symmetric matrix
		<m>A</m> guarantees that <m>A</m> is 
		<ol marker="1.">
		  <li> <p> positive definite?</p></li>
		  <li> <p> positive semidefinite?</p></li>
		  <li> <p> negative definite?</p></li>
		  <li> <p> negative semidefinite?</p></li>
		  <li> <p> indefinite?</p></li>
		</ol>
	      </p>
	    </li>
	  </ol>
	</p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>q_D(\xvec) = 4x_1^2 + 2x_2^2</m>.  Both addends are
		nonnegative, and one of them is positive if
		<m>\xvec</m> is nonzero.  This means that
		<m>q_D(\xvec) \gt 0</m> when <m>\xvec</m> is nonzero
		and so <m>D</m> is positive definite.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>q_D(\xvec) = 4x_1^2</m>, which is always
		nonnegative.  However, <m>q_D\left(\twovec01\right) =
		0</m> so <m>D</m> is positive semidefinite.
	      </p>
	    </li>
	    <li>
	      <p>
		<ol marker="1.">
		  <li>
		    <p>
		      They are all positive.
		    </p>
		  </li>
		  <li>
		    <p>
		      They are all nonnegative.
		    </p>
		  </li>
		  <li>
		    <p>
		      They are all negative. 
		    </p>
		  </li>
		  <li>
		    <p>
		      They are all nonpositive.
		    </p>
		  </li>
		  <li>
		    <p>
		      They are some positive eigenvalues and some
		      negative ones .
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>
	    <li>
	      <p>
		Since we know that <m>q_D(\yvec) \gt 0</m> when
		<m>\yvec</m> is nonzero, we know that <m>q_A(\xvec)
		\gt 0</m> when <m>\xvec</m> is nonzero.  Therefore,
		<m>A</m> is positive definite.
	      </p>
	    </li>
	    <li>
	      <p>
		It will be positive semidefinite.
	      </p>
	    </li>
	    <li>
	      <p>
		They will be the same as before.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
		      
      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		<m>q_D(\xvec) = 4x_1^2 + 2x_2^2</m>
		so <m>D</m> is positive definite.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>q_D(\xvec) = 4x_1^2</m>
		so <m>D</m> is positive semidefinite.
	      </p>
	    </li>
	    <li>
	      <p>
		<ol marker="1.">
		  <li>
		    <p>
		      They are all positive.
		    </p>
		  </li>
		  <li>
		    <p>
		      They are all nonnegative.
		    </p>
		  </li>
		  <li>
		    <p>
		      They are all negative. 
		    </p>
		  </li>
		  <li>
		    <p>
		      They are all nonpositive.
		    </p>
		  </li>
		  <li>
		    <p>
		      They are some positive eigenvalues and some
		      negative ones .
		    </p>
		  </li>
		</ol>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A</m> is positive definite.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A</m> is positive semidefinite.
	      </p>
	    </li>
	    <li>
	      <p>
		They will be the same as before.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
		      
    </activity>

    <p>
      As seen in this activity, it is straightforward to determine the
      definiteness of a diagonal matrix.  For instance, if
      <m>D=\begin{bmatrix} 7 \amp 0 \\ 0 \amp 5 \end{bmatrix}</m>,
      then
      <me>
	q_D(\xvec) = 7x_1^2 + 5x_2^2.
      </me>
      This shows that <m>q_D(\xvec) \gt 0</m> when either <m>x_1</m>
      or <m>x_2</m> is not zero so we conclude that <m>D</m> is
      positive definite.
      In the same way, we see that <m>D</m> is positive semidefinite
      if all the diagonal entries are nonnegative.
    </p>

    <p>
      Understanding this behavior for diagonal matrices enables us to
      understand more general symmetric matrices.  As we saw
      previously, the quadratic form for a symmetric matrix
      <m>A=QDQ^T</m> agrees with the quadratic form for the diagonal
      matrix <m>D</m> after a change of coordinates.  In particular,
      <me>
	q_A(\xvec) = q_D(\yvec)
      </me>
      where <m>\yvec=Q^T\xvec</m>.  Now the diagonal entries of
      <m>D</m> are the eigenvalues of <m>A</m> from which we conclude
      that <m>q_A(\xvec) \gt 0</m> if all the eigenvalues of <m>A</m>
      are positive.  Likewise, <m>q_A(\xvec)\geq 0</m> if all the
      eigenvalues are nonnegative.
    </p>

    <proposition xml:id="prop-definite-matrices">
      <statement>
	<p>
	  A symmetric matrix is positive definite if all its
	  eigenvalues are positive.  It is positive semidefinite if all
	  its eigenvalues are nonnegative.
	</p>

	<p>
	  Likewise, a symmetric matrix is indefinite if some
	  eigenvalues are positive and some are negative.
	</p>
      </statement>
    </proposition>

    <p>
      We will now apply what we've learned about quadratic forms to
      study the nature of critical points in multivariable calculus.
      The rest of this section assumes that the reader is familiar
      with ideas from multivariable calculus and can be skipped by
      others.
    </p>

    <p>
      First, suppose that <m>f(x,y)</m> is a differentiable function.
      We will use <m>f_x</m> and <m>f_y</m> to denote the partial
      derivatives of <m>f</m> with respect to <m>x</m> and <m>y</m>.
      Similarly, <m>f_{xx}</m>, <m>f_{xy}</m>, <m>f_{yx}</m> and
      <m>f_{yy}</m> denote the second partial derivatives.  You may
      recall that the 
      mixed partials, <m>f_{xy}</m> and <m>f_{yx}</m> are equal under
      a mild assumption on the function <m>f</m>.
      A typical question in calculus is to determine where this
      function has its maximum and minimum values.
    </p>

    <p>
      Any local maximum or minimum of <m>f</m> appears at a critical
      point <m>(x_0,y_0)</m> where
      <me>
	f_x(x_0,y_0) = 0,\hspace{24pt}
	f_y(x_0,y_0) = 0.
      </me>
      Near a critical point, the quadratic approximation of <m>f</m>
      tells us that
      <md>
	<mrow>
	  f(x,y)\approx f(x_0,y_0) \amp + \frac12
	  f_{xx}(x_0,y_0)(x-x_0)^2
	</mrow>
	<mrow>
	  \amp + f_{xy}(x_0,y_0)(x-x_0)(y-y_0) + \frac12
	  f_{yy}(x_0,y_0)(y-y_0)^2.
	</mrow>
      </md>
    </p>

    <activity>
      <statement>
	<p>
	  Let's explore how our understanding of quadratic forms helps
	  us determine the behavior of a function <m>f</m> near a
	  critical point. 
	  <ol marker="a.">
	    <li>
	      <p>
		Consider the function <m>f(x,y) = 2x^3 - 6xy +
		3y^2</m>.  Find the partial derivatives <m>f_{x}</m> and
		<m>f_y</m> and use these expressions to determine
		the critical points of <m>f</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Evaluate the second partial derivatives <m>f_{xx}</m>,
		<m>f_{xy}</m>, and <m>f_{yy}</m>.
	      </p>
	    </li>

	    <li>
	      <p>
		Let's first consider the critical point <m>(1,1)</m>.
		Use the quadratic approximation as written above to
		find an expression approximating <m>f</m> near the
		critical point.
	      </p>
	    </li>

	    <li>
	      <p>
		Using the vector <m>\wvec = \twovec{x-1}{y-1}</m>,
		rewrite your approximation as
		<me>
		  f(x,y) \approx f(1,1) + q_A(\wvec)
		</me>
		for some matrix <m>A</m>.  What is the matrix <m>A</m>
		in this case?
	      </p>
	    </li>

	    <li>
	      <p>
		Find the eigenvalues of <m>A</m>.  What can you conclude
		about the definiteness of <m>A</m>?
	      </p>
	    </li>

	    <li>
	      <p>
		Recall that <m>(x_0,y_0)</m> is a local minimum for
		<m>f</m> if <m>f(x,y) \gt f(x_0,y_0)</m> for nearby
		points <m>(x,y)</m>.
		Explain why our understanding of the eigenvalues of
		<m>A</m> shows that <m>(1,1)</m> is a local
		minimum for <m>f</m>.
		<sage>
		  <input>
x, y = var('x', 'y')		    
plot3d(2*x^3 - 6*x*y + 3*y^2, (x, 0.75,1.25), (y,0.75,1.25))
		  </input>
		</sage>
	      </p>
	    </li>

	  </ol>
	</p>
      </statement>

      <solution>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		We have
		<md>
		  <mrow>
		    f_x \amp {}={} 6x^2 - 6y = 0
		  </mrow>
		  <mrow>
		    f_y \amp {}={} -6x + 6y = 0
		  </mrow>
		  which leads to the conditions <m>y=x^2</m> and
		  <m>y=x</m>.  This gives the critical points
		  <m>(0,0)</m> and <m>(1,1)</m>.
		</md>
	      </p>
	    </li>
	    <li>
	      <p>
		We have
		<me>
		  f_{xx} = 12x, \hspace{24pt}
		  f_{xy} = -6, \hspace{24pt}
		  f_{yy} = 6
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		This gives
		<me>
		  f(x,y)\approx -1 + \frac12 12(x-1)^2 -6(x-1)(y-1) +
		  \frac12 6(y-1)^2
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		The matrix is <m>A = \begin{bmatrix}
		6 \amp -3 \\
		-3 \amp 3
		\end{bmatrix}
		</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		The eigenvalues are <m>\lambda_1=7.85</m> and
		<m>\lambda_2=1.15</m>, both of which are positive,
		which means that <m>A</m> is positive definite.
	      </p>
	    </li>
	    <li>
	      <p>
		We have <m>q_A(\wvec) \gt 0</m> if <m>\wvec</m> is
		nonzero so
		<me>
		  f(x,y) \approx f(1,1) + q_A(\wvec) \gt f(1,1)
		</me>
		for points <m>(x,y)</m> near to <m>(1,1)</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </solution>
	  
      <answer>
	<p>
	  <ol marker="a.">
	    <li>
	      <p>
		We have
		<md>
		  <mrow>
		    f_x \amp {}={} 6x^2 - 6y = 0
		  </mrow>
		  <mrow>
		    f_y \amp {}={} -6x + 6y = 0
		  </mrow>
		  with critical points
		  <m>(0,0)</m> and <m>(1,1)</m>.
		</md>
	      </p>
	    </li>
	    <li>
	      <p>
		We have
		<me>
		  f_{xx} = 12x, \hspace{24pt}
		  f_{xy} = -6, \hspace{24pt}
		  f_{yy} = 6
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		This gives
		<me>
		  f(x,y)\approx -1 + \frac12 12(x-1)^2 -6(x-1)(y-1) +
		  \frac12 6(y-1)^2
		</me>
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A = \begin{bmatrix}
		6 \amp -3 \\
		-3 \amp 3
		\end{bmatrix}
		</m>.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>A</m> is positive definite.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>
		  f(x,y) \approx f(1,1) + q_A(\wvec) \gt f(1,1)
		</m>
		for points <m>(x,y)</m> near to <m>(1,1)</m>.
	      </p>
	    </li>
	  </ol>
	</p>
      </answer>
	      
    </activity>

    <p>
      Near a critical point <m>(x_0,y_0)</m> of a function
      <m>f(x,y)</m>, we can write 
      <me>
	f(x,y) \approx f(x_0, y_0) + q_A(\wvec)
      </me>
      where <m>\wvec = \twovec{x-x_0}{y-y_0}</m> and
      <m>A = \frac12
      \begin{bmatrix}
      f_{xx}(x_0,y_0) \amp f_{xy}(x_0,y_0) \\
      f_{yx}(x_0,y_0) \amp f_{yy}(x_0,y_0)
      \end{bmatrix}</m>.  If <m>A</m> is positive definite, then
      <m>q_A(\wvec) \gt 0</m>, which tells us that
      <me>f(x,y) \approx f(x_0,y_0) + q_A(\wvec) \gt f(x_0,y_0)</me>
      and that the critical point <m>(x_0,y_0)</m> is therefore a local
      minimum. 
    </p>

    <p>
      The matrix
      <me>
	H = 
	\begin{bmatrix}
	f_{xx}(x_0,y_0) \amp f_{xy}(x_0,y_0) \\
	f_{yx}(x_0,y_0) \amp f_{yy}(x_0,y_0)
	\end{bmatrix}
      </me>
      is called the <em>Hessian</em> of <m>f</m>, and we see now that
      the eigenvalues of this symmetric matrix determine the nature of
      the critical point <m>(x_0,y_0)</m>.  In particular, if the
      eigenvalues are both positive, then <m>q_H</m> is positive
      definite, and the critical point is a local minimum.
    </p>

    <p>
      This observation leads to the Second Derivative Test for
      multivariable functions.
    </p>

    <proposition>
      <title> Second Derivative Test </title>
      <statement>
	<p>
	  The nature of a critical point of a multivariable function
	  is determined by the Hessian <m>H</m> of the function at the
	  critical point.  If
	  <ul>
	    <li>
	      <p>
		<m>H</m> has all positive eigenvalues, the
		critical point is a local minimum.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>H</m> has all negative eigenvalues, the
		critical point is a local maximum.
	      </p>
	    </li>
	    <li>
	      <p>
		<m>H</m> has both positive and negative eigenvalues, the
		critical point is neither a local maximum nor minimum.
	      </p>
	    </li>
	  </ul>
	</p>
      </statement>
    </proposition>

    <p>
      Most multivariable calculus texts assume that the reader is not
      familiar with linear algebra and so write the second derivative test
      for functions of two variables
      in terms of <m>D=\det(H)</m>.  If
      <ul>
	<li>
	  <p>
	    <m>D \gt 0</m> and <m>f_{xx}(x_0,y_0) \gt 0</m>, then
	    <m>(x_0, y_0)</m> is a local minimum.
	  </p>
	</li>
	<li>
	  <p>
	    <m>D \gt 0</m> and <m>f_{xx}(x_0,y_0) \lt 0</m>, then
	    <m>(x_0, y_0)</m> is a local maximum.
	  </p>
	</li>
	<li>
	  <p>	    
	    <m>D \lt 0</m>, then <m>(x_0,y_0)</m> is neither a local
	    maximum nor minimum.
	  </p>
	</li>
      </ul>
      The conditions in this version of the second derivative test are
      simply algebraic criteria that tell us about the definiteness of
      the Hessian matrix <m>H</m>.
    </p>

  </subsection>

  <subsection>
    <title> Summary </title>

    <p>
      This section explored quadratic forms, functions that are
      defined by symmetric matrices.
      <ul>
	<li>
	  <p>
	    If <m>A</m> is a symmetric matrix, then the quadratic form
	    defined by <m>A</m> is the function <m>q_A(\xvec) =
	    \xvec\cdot(A\xvec)</m>.
	    Quadratic forms appear when studying the variance of a
	    dataset.  If <m>C</m> is the covariance matrix, then the
	    variance in the direction defined by a unit vector
	    <m>\uvec</m> is <m>q_C(\uvec) =
	    \uvec\cdot(C\uvec)=V_{\uvec}</m>. 
	  </p>

	  <p>
	    Similarly, quadratic forms appear in multivariable
	    calculus when analyzing the behavior of a function of
	    several variables near a critical point.
	  </p>
	</li>

	<li>
	  <p>
	    If <m>\lambda_1</m> is the largest eigenvalue of a
	    symmetric matrix <m>A</m> and <m>\lambda_m</m> the
	    smallest, then the maximum value of <m>q_A(\uvec)</m>
	    among unit vectors <m>\uvec</m>, is <m>\lambda_1</m>,
	    and this maximum value occurs in the direction of
	    <m>\uvec_1</m>, a unit eigenvector associated to
	    <m>\lambda_1</m>.
	  </p>

	  <p>
	    Similarly, the minimum value of <m>q_A(\uvec)</m> is
	    <m>\lambda_m</m>, which appears in the direction of
	    <m>\uvec_m</m>, an eigenvector associated to
	    <m>\lambda_m</m>.
	  </p>
	</li>

	<li>
	  <p>
	    A symmetric matrix is positive definite if its eigenvalues
	    are all positive, positive semidefinite if its eigenvalues
	    are all nonnegative, and indefinite if it has both
	    positive and negative eigenvalues.
	  </p>
	</li>
	<li>
	  <p>
	    If the Hessian <m>H</m> of a multivariable function
	    <m>f</m> is positive definite at a critical point, then
	    the critical point is a local minimum.  Likewise, if the
	    Hessian is negative definite, the critical point is a
	    local maximum.
	  </p>
	</li>
      </ul>

    </p>

  </subsection>

  <xi:include href="exercises/exercises7-2.xml" />

</section>
