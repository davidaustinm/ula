<!DOCTYPE html>
<html lang="en-US" dir="ltr">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<h4 class="heading"><span class="type">Paragraph</span></h4>
<div class="para">At the time this is being written, Google is tracking 35 trillion web pages.  Clearly, this is too many for humans to evaluate.  Plus, human evaluators may inject their own biases into their evaluations, perhaps even unintentionally.  Googleâ€™s idea is to use the structure of the Internet to assess the quality of web pages without any human intervention.  For instance, if a web page has quality content, other web pages will link to it.  This means that the number of links to a page reflect the quality of that page.  In addition, we would expect a page to have even higher quality content if those links are coming from pages that are themselves assessed to have high quality.  Simply said, if many quality pages link to a page, that page must itself be of high quality.  This is the essence of the PageRank algorithm, which we introduce in the next activity.</div>
<span class="incontext"><a href="sec-stochastic.html#p-4847" class="internal">in-context</a></span>
</body>
</html>
