<!DOCTYPE html>
<html lang="en-US" dir="ltr">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<h4 class="heading"><span class="type">Paragraph</span></h4>
<div class="para">In <a href="sec-pca.html" class="internal" title="Section 7.3: Principal Component Analysis">Section 7.3</a>, we explored principal component analysis as a technique to reduce the dimension of a dataset. In particular, we constructed the covariance matrix <span class="process-math">\(C\)</span> from a demeaned data matrix and saw that the eigenvalues and eigenvectors of <span class="process-math">\(C\)</span> tell us about the variance of the dataset in different directions.  We referred to the eigenvectors of <span class="process-math">\(C\)</span> as <em class="emphasis">principal components</em> and found that projecting the data onto a subspace defined by the first few principal components frequently gave us a way to visualize the dataset.  As we added more principal components, we retained more information about the original dataset.  This feels similar to the rank <span class="process-math">\(k\)</span> approximations we have just seen so let’s explore the connection.</div>
<span class="incontext"><a href="sec-svd-uses.html#p-8239" class="internal">in-context</a></span>
</body>
</html>
