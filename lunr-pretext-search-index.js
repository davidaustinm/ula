var ptx_lunr_search_style = "textbook";
var ptx_lunr_docs = [
{
  "id": "dedication-1",
  "level": "1",
  "url": "dedication-1.html",
  "type": "Dedication",
  "number": "",
  "title": "Dedication",
  "body": " For Sam and Henry  "
},
{
  "id": "colophon-1",
  "level": "1",
  "url": "colophon-1.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": " 2023 Update   http:\/\/gvsu.edu\/s\/0Ck  http:\/\/gvsu.edu\/s\/0Ck   copyright  "
},
{
  "id": "acknowledgement-1",
  "level": "1",
  "url": "acknowledgement-1.html",
  "type": "Acknowledgements",
  "number": "",
  "title": "Acknowledgements",
  "body": " Many people have supported and shaped this project. First is my colleague Matt Boelkins, whose Active Calculus is a model for how an open textbook can encourage and facilitate good pedagogy. The debt that this book owes to Matt's cannot be overstated. In addition, he has provided a great deal of editorial feedback on this text and improved it in countless ways. Over many, many years, I have valued Matt's friendship and wise counsel.  I could not imagine a more supportive environment than the mathematics department at Grand Valley State University. The influence of my colleagues and their deep commitment to student growth is embedded in every page of this book. Conversations about the teaching of linear algebra with Paul Fishback have been especially helpful as has editorial feedback from Lauren Keough and Lora Bailey. I am also grateful for a sabbatical leave in 2017 during which I began this project.  In addition to my colleagues, I am grateful for the many students who have helped me grow as a teacher. Thank you for your willingness to engage in this very human art of learning and for sharing your experiences, frustrations, and successes with me.  The open textbook community that has grown around the PreTeXt authoring and publishing system is a continual source of support and inspiration. The goal of providing all students with high-quality, affordable textbooks is ambitious, but the commitment of this passionate and dedicated group makes clear that it is possible. As part of that community, Mitch Keller and Kathy Yoshiwara have read much of this book and provided detailed and insightful editorial feedback.  There coud be no better partners than Candice Price, Miloš Savić, and their team at 619 Wreath Publishing. Thank you for your support of this project and for everything you do to further your mission to foster creativity, equity, and scholarship.   Finally, a book is nothing without readers, and I am so thankful for all the instructors, students, and self-learners who have reached out with suggestions, comments, and questions. Hearing from those who are using the book gives meaning to this project, so please know that your voice is always welcomed.  "
},
{
  "id": "preface-1",
  "level": "1",
  "url": "preface-1.html",
  "type": "Preface",
  "number": "",
  "title": "Our goals",
  "body": " Our goals  This is a textbook for a first-year course in linear algebra. Of course, there are already many fine linear algebra textbooks available. Even if you are reading this one online for free, you should know that there are other free linear algebra textbooks available online. You have choices! So why would you choose this one?  This book arises from my belief that linear algebra, as presented in a traditional undergraduate curriculum, has for too long lived in the shadow of calculus. Many mathematics programs currently require their students to complete at least three semesters of calculus, but only one semester of linear algebra, which often has two semesters of calculus as a prerequisite.  In addition, what linear algebra students encounter is frequently presented in an overly formal way that does not fully represent the range of linear algebraic thinking. Indeed, many programs use a first course in linear algebra as an introduction to proofs course. While linear algebra provides an excellent introduction to mathematical reasoning, to only emphasize this aspect of the subject neglects some important student needs.  Of course, linear algebra is based on a set of abstract principles. However, these principles underlie an astonishingly wide range of technology that shapes our society in profound ways. The interplay between these principles and their applications provides a unique opportunity for working with students. First, the consideration of significant real-world problems grounds abstract mathematical thinking in a way that deepens students' understanding. At the same time, the variety of ways in which these abstract principles may be applied clearly demonstrates for students the power of mathematical abstraction. Linear algebra empowers students to experience what the physicist Eugene Wigner called the unreasonable effectiveness of mathematics in the natural sciences, an aspect of mathematics that is both fundamental and mysterious.  Neglecting this experience does not serve our students well. For instance, only about 15% of current mathematics majors will go on to attend graduate school. The remainder are headed for careers that will ask them to use their mathematical training in business, industry, and government. What do these careers look like? Right now, data analytics and data mining, computer graphics, software development, finance, and operations research. These careers depend much more on linear algebra than calculus. In addition to helping students appreciate the profound changes that mathematics has brought to our society, more training in linear algebra will help our students participate in the inevitable developments yet to come.  These thoughts are not uniquely mine nor are they particularly new. The Linear Algebra Curriculum Study Group, a broadly-based group of mathematicians and mathematics educators funded by the National Science Foundation, formed to improve the teaching of linear algebra. In their final report, they wrote   There is a growing concern that the linear algebra curriculum at many schools does not adequately address the needs of the students it attempts to serve. In recent years, demand for linear algebra training has risen in client disciplines such as engineering, computer science, operations research, economics, and statistics. At the same time, hardware and software improvements in computer science have raised the power of linear algebra to solve problems that are orders of magnitude greater than dreamed possible a few decades ago. Yet in many courses, the importance of linear algebra in applied fields is not communicated to students, and the influence of the computer is not felt in the classroom, in the selection of topics covered or in the mode of presentation. Furthermore, an overemphasis on abstraction may overwhelm beginning students to the point where they leave the course with little understanding or mastery of the basic concepts they may need in later courses and their careers.   Furthermore, among their recommendations is this:   We believe that a first course in linear algebra should be taught in a way that reflects its new role as a scientific tool. This implies less emphasis on abstraction and more emphasis on problem solving and motivating applications.   What may be surprising is that this was written in 1993; that is, before the introduction of Google's PageRank algorithm, before Pixar's Toy Story , and before the ascendence of what is often called data science or machine learning made these statements only more relevant.  With these thoughts in mind, the aim of this book is to facilitate a fuller, richer experience of linear algebra for all students, which informs the following decisions.   This book is written without the assumption that students have taken a calculus course. In making this decision, I hope that students will gain a more authentic experience of mathematics through linear algebra at an earlier stage of their academic careers.  Indeed, a common barrier to student success in calculus is its relatively high prerequisite tower culminating in a course often called Precalculus . By contrast, linear algebra begins with much simpler assumptions about our students' preparation: the expressions studied are linear so that may be manipulated using only the four basic arithmetic operations.  The most common explanation I hear for requiring calculus as a prerequisite for linear algebra is that calculus develops in students a beneficial mathematical maturity. Given persistent student struggles with calculus, however, it seems just as reasonable to develop students' abilities to reason mathematically through linear algebra.   The text includes a number of significant applications of important linear algebraic concepts, such as computer animation, the JPEG compression algorithm, and Google's PageRank algorithm. In my experience, students find these applications more authentic and compelling than typical applications presented in a calculus class. These applications also provide a strong justification for mathematical abstraction, which can seem unnecessary to beginning students, and demonstrate how mathematics is currently shaping our world.    Each section begins with a preview activity and includes a number of activities that can be used to facilitate active learning in a classroom. By now, active learning's effectiveness in helping students develop a deep understanding of important mathematical concepts is beyond dispute. The activities here are designed to reinforce ideas already encountered, motivate the need for upcoming ideas, and help students recognize various manifestations of simple underlying themes. As much as possible, students are asked to develop new ideas and take ownership of them.   The activities emphasize a broad range of mathematical thinking. Rather than providing the traditional cycle of Definition-Theorem-Proof, Understanding Linear Algebra aims to develop an appreciation of ideas as arising in response to a need that students perceive. Working much as research mathematicians do, students are asked to consider examples that illustrate the importance of key concepts so that definitions arise as natural labels used to identify these concepts. Again using examples as motivation, students are asked to reason mathematically and explain general phenomena they observe, which are then recorded as theorems and propositions. It is not, however, the intention of this book to develop students' formal proof-writing abilities.   There are frequent embedded Sage cells that help develop students' computational proficiency. The impact that linear algebra is having on our society is inextricably tied to the phenomenal increase in computing power witnessed in the last half-century. Indeed, Carl Cowen, former president of the Mathematical Association of America, has said, No serious application of linear algebra happens without a computer. This means that an understanding of linear algebra is not complete without an understanding of how linear algebraic ideas are deployed in a computational environment.   The text aims to leverage geometric intuition to enhance algebraic thinking. In spite of the fact that it may be difficult to visualize phenomena in thousands of dimensions, many linear algebraic concepts may be effectively illustrated in two or three dimensions and the resulting intuition applied more generally. Indeed, this useful interplay between geometry and algebra illustrates another mysterious mathematical connection between seemingly disparate areas.    I hope that Understanding Linear Algebra is useful for you, whether you are a student taking a linear algebra class, someone just interested in self-study, or an instructor seeking out some ideas to use with your students. I would be more than happy to hear your feedback.  "
},
{
  "id": "preface-2",
  "level": "1",
  "url": "preface-2.html",
  "type": "Preface",
  "number": "",
  "title": "A note on the print version",
  "body": " A note on the print version  This book aims to develop readers' ability to reason about linear algebraic concepts and to apply that reasoning in a computational environment. In particular, Sage is introduced as a platform for performing many linear algebraic computations since it is freely available and its syntax mirrors common mathematical notation.   Print readers may access Sage online using either the Sage cell server or a provided page of Sage cells.   Throughout the book, Sage cells appear in various places to encourage readers to use Sage to complete some relevant computation. In the print version, these may appear with some pre-populated code, such as the one below, that you will want to copy into an online Sage cell. Empty cells appear as shown below and are included to indicate part of an exercise or activity that is meant to be completed in Sage.   "
},
{
  "id": "sec-expect",
  "level": "1",
  "url": "sec-expect.html",
  "type": "Section",
  "number": "1.1",
  "title": "What can we expect",
  "body": " What can we expect   At its heart, the subject of linear algebra is about linear equations and, more specifically, sets of two or more linear equations. Google routinely deals with a set of trillions of equations each of which has trillions of unknowns. We will eventually understand how to deal with that kind of complexity. To begin, however, we will look at a more familiar situation in which there are a small number of equations and a small number of unknowns. In spite of its relative simplicity, this situation is rich enough to demonstrate some fundamental concepts that will motivate much of our exploration.    Some simple examples    In this activity, we consider sets of linear equations having just two unknowns. In this case, we can graph the solutions sets for the equations, which allows us to visualize different types of behavior.    On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the line .  How many points satisfy this equation?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy all three equations?            The graph of the two lines is as shown on the right. There is a single point, the point , at which the lines intersect. Therefore, there is a single point that satisfies both equations.       These two lines are parallel, which means there is no point at which the lines intersect. Therefore, there is no point that satisfies both equations.       There are infinitely many points that lie on this line and that, therefore, satisfy this single equation.       These three lines do not have a common intersection point. Consequently, there is no point satisfying all three equations.           This is exactly one point, the point , that satisfies both equations.    There are no points that satisfy both equations.    There are infinitely many points.    There are no points that satisfy all three equations.       The examples in this introductory activity demonstrate several possible outcomes for the solutions to a set of linear equations. Notice that we are interested in points that satisfy each equation in the set and that these are seen as intersection points of the lines. Similar to the examples considered in the activity, three types of outcomes are seen in .    Three possible graphs for sets of linear equations in two unknowns.   In this figure, we see that   With a single equation, there are infinitely many points satisfying that equation.  Adding a second equation adds another condition we place on the points resulting in a single point that satisfies both equations.  Adding a third equation adds a third condition on the points , and there is no point that satisfies all three equations.   Generally speaking, a single equation will have many solutions, in fact, infinitely many. As we add equations, we add conditions which lead, in a sense we will make precise later, to a smaller number of solutions. Eventually, we have too many equations and find there are no points that satisfy all of them.  This example illustrates a general principle to which we will frequently return.   Solutions to sets of linear equations  Given a set of linear equations, there are either:   infinitely many points,    exactly one point, or    no points  that satisfy every equation in the set.   Notice that we can see a bit more. In , we are looking at equations in two unknowns. Here we see that   One equation has infinitely many solutions.    Two equations have exactly one solution.   Three equations have no solutions.    It seems reasonable to wonder if the number of solutions depends on whether the number of equations is less than, equal to, or greater than the number of unknowns. Of course, one of the examples in the activity shows that there are exceptions to this simple rule, as seen in . For instance, two equations in two unknowns may correspond to parallel lines so that the set of equations has no solutions. It may also happen that a set of three equations in two unknowns has a single solution. However, it seems safe to think that the more equations we have, the smaller the set of solutions will be.   A set of two equations in two unknowns can have no solutions, and a set of three equations can have one solution.       Let's also consider some examples of equations having three unknowns, which we call , , and . Just as solutions to linear equations in two unknowns formed straight lines, solutions to linear equations in three unknowns form planes.  When we consider an equation in three unknowns graphically, we need to add a third coordinate axis, as shown in .       Coordinate systems in two and three dimensions.   As shown in , a linear equation in two unknowns, such as , is a line while a linear equation in three unknowns, such as , is a plane.       The solutions to the equation in two dimensions and in three.   In three unknowns, the set of solutions to one linear equation forms a plane. The set of solutions to a pair of linear equations is seen graphically as the intersection of the two planes. As in , we typically expect this intersection to be a line.       A single plane and the intersection of two planes.   When we add a third equation, we are looking for the intersection of three planes, which we expect to form a point, as in the left of . However, in certain special cases, it may happen that there are no solutions, as seen on the right.       Two examples showing the intersections of three planes.     This activity considers sets of equations having three unknowns. In this case, we know that the solutions of a single equation form a plane. If it helps with visualization, consider using -inch index cards to represent planes.   Is it possible that there are no solutions to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that there is exactly one solution to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that the solutions to four equations in three unknowns form a line? Either sketch an example or state a reason why it can't happen.    What would you usually expect for the set of solutions to four equations in three unknowns?    Suppose we have a set of 500 linear equations in 10 unknowns. Which of the three possibilities would you expect to hold?   Suppose we have a set of 10 linear equations in 500 unknowns. Which of the three possibilities would you expect to hold?       Yes, it is possible if the two planes are parallel to one another.    No, this is not possible. Two planes will either intersect in a line, if they are not parallel, or not intersect at all, if they are parallel.    Yes, it is possible that four planes intersect in a line. One may sketch four planes that intersect in, say, the -axis.    In general, we would expect there to be no solutions to four equations in three unknowns because there are more equations than unknowns.    Since there are more equations than unknowns, we would expect there to be no solutions. We cannot guarantee this, however.    Since there are fewer equations than unknowns, we would expect there to be infinitely many solutions. We cannot guarantee this, however.        Yes.   No.   Yes.   We would expect there to be no solutions.   We would expect there to be no solutions.   We would expect there to be infinitely many solutions.       Systems of linear equations  Now that we have seen some simple examples, let's agree on some terminology to help us think more precisely about sets of equations.  First, we considered a linear equation having the form . It will be convenient for us to rewrite this so that all the unknowns are on one side of the equation: . More generally, the equation of a line can always be expressed in the form which gives us the flexibility to describe all lines. For instance, vertical lines, such as , may be represented in this form.  Notice that each term on the left is the product of a constant and the first power of an unknown. In the future, we will want to consider equations having many more unknowns, which we will sometimes denote as . This leads to the following definition:     linear equation A linear equation in the unknowns may be written in the form , where are real numbers known as coefficients . We also say that are the variables in the equation.  By a system of linear equations or a  linear system  linear system , we mean a set of linear equations written in a common set of unknowns.   For instance, is an example of a linear system.   solution A solution to a linear system is simply a set of numbers that satisfy all the equations in the system.   For instance, we earlier considered the linear system To check that is a solution, we verify that the following equations are true.     solution space  We call the set of all solutions the solution space of the linear system.    Linear equations and their solutions     Which of the following equations are linear? Please provide a justification for your response.   .   .   .    Consider the system of linear equations:    Is a solution?   Is a solution?    Is a solution?   Can you find a solution in which ?   Do you think there are other solutions? Please explain your response.          There are two linear equations in this list.  This is not a linear equation due to the presence of in the third term on the left.  This is a linear equation.  This is a linear equation since it can be rewritten in the form .     We will see that the system of linear equations has infinitely many solutions.  Yes, this is a solution since all three equations are satisfied if we set , , and .  No, this is not a solution since the first equation is not satisfied if and .  This is also not a solution.  If , then we arrive at the system of three linear equations: We have a solution when and . Therefore, is a solution to the original system of equations.  Since we have found more than one solution to the system of equations, we should expect that there are infinitely many. Therefore, there should be many other solutions.         There are two linear equations in this list.  This is not a linear equation.  This is a linear equation.  This is a linear equation.     We will see that the system of linear equations has infinitely many solutions.  This is a solution.  This is a not solution.  This is a not solution.  Yes, is a solution.  Yes, because there should be infinitely many solutions.          Summary  The point of this section is to build some intuition about the behavior of solutions to linear systems through consideration of some simple examples. We will develop a deeper and more precise understanding of these phenomena in our future explorations.   A linear equation is one that may be written in the form .  A linear system is a set of linear equations and a solution is a set of values assigned to the unknowns that make each equation true.   We came to expect that a linear system has either infinitely many solutions, exactly one solution, or no solutions.   When we add more equations to a system, the solution space usually seems to become smaller.    "
},
{
  "id": "activity-1",
  "level": "2",
  "url": "sec-expect.html#activity-1",
  "type": "Activity",
  "number": "1.1.1",
  "title": "",
  "body": "  In this activity, we consider sets of linear equations having just two unknowns. In this case, we can graph the solutions sets for the equations, which allows us to visualize different types of behavior.    On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy both equations?       On the grid below, graph the line .  How many points satisfy this equation?       On the grid below, graph the lines   At what point or points , do the lines intersect? How many points satisfy all three equations?            The graph of the two lines is as shown on the right. There is a single point, the point , at which the lines intersect. Therefore, there is a single point that satisfies both equations.       These two lines are parallel, which means there is no point at which the lines intersect. Therefore, there is no point that satisfies both equations.       There are infinitely many points that lie on this line and that, therefore, satisfy this single equation.       These three lines do not have a common intersection point. Consequently, there is no point satisfying all three equations.           This is exactly one point, the point , that satisfies both equations.    There are no points that satisfy both equations.    There are infinitely many points.    There are no points that satisfy all three equations.      "
},
{
  "id": "fig-three-possibilities",
  "level": "2",
  "url": "sec-expect.html#fig-three-possibilities",
  "type": "Figure",
  "number": "1.1.1",
  "title": "",
  "body": "  Three possible graphs for sets of linear equations in two unknowns.  "
},
{
  "id": "solution-exceptions",
  "level": "2",
  "url": "sec-expect.html#solution-exceptions",
  "type": "Figure",
  "number": "1.1.2",
  "title": "",
  "body": " A set of two equations in two unknowns can have no solutions, and a set of three equations can have one solution.      "
},
{
  "id": "fig-coordinates",
  "level": "2",
  "url": "sec-expect.html#fig-coordinates",
  "type": "Figure",
  "number": "1.1.3",
  "title": "",
  "body": "     Coordinate systems in two and three dimensions.  "
},
{
  "id": "fig-plane-z0",
  "level": "2",
  "url": "sec-expect.html#fig-plane-z0",
  "type": "Figure",
  "number": "1.1.4",
  "title": "",
  "body": "     The solutions to the equation in two dimensions and in three.  "
},
{
  "id": "fig-two-planes",
  "level": "2",
  "url": "sec-expect.html#fig-two-planes",
  "type": "Figure",
  "number": "1.1.5",
  "title": "",
  "body": "     A single plane and the intersection of two planes.  "
},
{
  "id": "fig-three-planes",
  "level": "2",
  "url": "sec-expect.html#fig-three-planes",
  "type": "Figure",
  "number": "1.1.6",
  "title": "",
  "body": "     Two examples showing the intersections of three planes.  "
},
{
  "id": "activity-2",
  "level": "2",
  "url": "sec-expect.html#activity-2",
  "type": "Activity",
  "number": "1.1.2",
  "title": "",
  "body": "  This activity considers sets of equations having three unknowns. In this case, we know that the solutions of a single equation form a plane. If it helps with visualization, consider using -inch index cards to represent planes.   Is it possible that there are no solutions to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that there is exactly one solution to two linear equations in three unknowns? Either sketch an example or state a reason why it can't happen.    Is it possible that the solutions to four equations in three unknowns form a line? Either sketch an example or state a reason why it can't happen.    What would you usually expect for the set of solutions to four equations in three unknowns?    Suppose we have a set of 500 linear equations in 10 unknowns. Which of the three possibilities would you expect to hold?   Suppose we have a set of 10 linear equations in 500 unknowns. Which of the three possibilities would you expect to hold?       Yes, it is possible if the two planes are parallel to one another.    No, this is not possible. Two planes will either intersect in a line, if they are not parallel, or not intersect at all, if they are parallel.    Yes, it is possible that four planes intersect in a line. One may sketch four planes that intersect in, say, the -axis.    In general, we would expect there to be no solutions to four equations in three unknowns because there are more equations than unknowns.    Since there are more equations than unknowns, we would expect there to be no solutions. We cannot guarantee this, however.    Since there are fewer equations than unknowns, we would expect there to be infinitely many solutions. We cannot guarantee this, however.        Yes.   No.   Yes.   We would expect there to be no solutions.   We would expect there to be no solutions.   We would expect there to be infinitely many solutions.    "
},
{
  "id": "definition-1",
  "level": "2",
  "url": "sec-expect.html#definition-1",
  "type": "Definition",
  "number": "1.1.7",
  "title": "",
  "body": "   linear equation A linear equation in the unknowns may be written in the form , where are real numbers known as coefficients . We also say that are the variables in the equation.  By a system of linear equations or a  linear system  linear system , we mean a set of linear equations written in a common set of unknowns.  "
},
{
  "id": "definition-2",
  "level": "2",
  "url": "sec-expect.html#definition-2",
  "type": "Definition",
  "number": "1.1.8",
  "title": "",
  "body": " solution A solution to a linear system is simply a set of numbers that satisfy all the equations in the system.  "
},
{
  "id": "definition-3",
  "level": "2",
  "url": "sec-expect.html#definition-3",
  "type": "Definition",
  "number": "1.1.9",
  "title": "",
  "body": "  solution space  We call the set of all solutions the solution space of the linear system.  "
},
{
  "id": "activity-3",
  "level": "2",
  "url": "sec-expect.html#activity-3",
  "type": "Activity",
  "number": "1.1.3",
  "title": "Linear equations and their solutions.",
  "body": " Linear equations and their solutions     Which of the following equations are linear? Please provide a justification for your response.   .   .   .    Consider the system of linear equations:    Is a solution?   Is a solution?    Is a solution?   Can you find a solution in which ?   Do you think there are other solutions? Please explain your response.          There are two linear equations in this list.  This is not a linear equation due to the presence of in the third term on the left.  This is a linear equation.  This is a linear equation since it can be rewritten in the form .     We will see that the system of linear equations has infinitely many solutions.  Yes, this is a solution since all three equations are satisfied if we set , , and .  No, this is not a solution since the first equation is not satisfied if and .  This is also not a solution.  If , then we arrive at the system of three linear equations: We have a solution when and . Therefore, is a solution to the original system of equations.  Since we have found more than one solution to the system of equations, we should expect that there are infinitely many. Therefore, there should be many other solutions.         There are two linear equations in this list.  This is not a linear equation.  This is a linear equation.  This is a linear equation.     We will see that the system of linear equations has infinitely many solutions.  This is a solution.  This is a not solution.  This is a not solution.  Yes, is a solution.  Yes, because there should be infinitely many solutions.       "
},
{
  "id": "sec-finding-solutions",
  "level": "1",
  "url": "sec-finding-solutions.html",
  "type": "Section",
  "number": "1.2",
  "title": "Finding solutions to linear systems",
  "body": " Finding solutions to linear systems   In the previous section, we looked at systems of linear equations from a graphical perspective. Since the equations had only two or three variables, we could study the solution spaces as the intersections of lines and planes.  Because we will eventually consider systems with many equations and many variables, this graphical approach will not generally be a useful strategy. Instead, we will approach this problem algebraically and develop a technique to describe the solution spaces of general linear systems.    Gaussian elimination   Gaussian elimination We will develop an algorithm, which is usually called Gaussian elimination , that allows us to describe the solution space of a linear system. This algorithm plays a central role in much of what is to come.    In this activity, we will consider some simple examples that will guide us in finding a more general approach.    Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:   Describe the solution space to the linear equation .  Describe the solution space to the linear equation .       The equations tell us the value of both variables so there is only one solution .    We first use the third equation to determine that . We next substitute this value into the first two equations to obtain Now we can use the second equation to determine that , substitute that value into the first equation, and determine that . This tells us that the linear system has one solution .    Notice that we can rewrite the first equation as and substitute this into the second equation to obtain This tells us that and . The linear system therefore has one solution .    No matter the value of , we have . Therefore, the solution space to the equation is all real numbers . In other words, this equation does not place a restriction on the value of .    By contrast, the equation has no solutions since no value of , when multiplied by , can produce .        .   .   .  Any real number is a solution.  There are no solutions.     These examples lead to a few observations that motivate a general approach to finding solutions of linear systems.   First, finding the solution space to some systems is simple. For example, because each equation in the following system has only one variable, it prescribes a specific value for that variable. We therefore see that there is exactly one solution, which is . We call such a system decoupled . decoupled system     Second, there is a process that can be used to find solutions to certain types of linear systems. For instance, let's consider the system Multiplying both sides of the last equation by gives us Any solution to this linear system must then have .  Once we know that, we can substitute into the first and second equations and simplify to obtain a new system of equations having the same solutions: The second equation, after multiplying both sides by , tells us that . We can then substitute this value into the first equation to determine that .  In this way, we arrive at a decoupled system, which shows that there is exactly one solution, namely .  Our original system,  triangular system  back substitution is called a triangular system due to the shape formed by the coefficients. As this example demonstrates, triangular systems are easily solved by this process, which is called back substitution .    We can use substitution in a more general way to solve linear systems. For example, a natural approach to the system is to use the first equation to express in terms of : and then substitute this into the second equation and simplify: From here, we can substitute into the first equation to arrive at the solution .  However, the two-step process of solving for in terms of and substituting into the second equation may be performed more efficiently by adding a multiple of the first equation to the second. In this case, we will multiply the first equation by -2 and add to the second equation to obtain     which gives us       In this way, the system is transformed into the new triangular system Notice that this process can be reversed. Beginning with the triangular system, we can recover the original system by multiplying the first equation by 2 and adding it to the second. Because of this, the two systems have the same solution space. We will revisit this point later and give what may be a more convincing explanation.  Of course, the choice to multiply the first equation by -2 was made so that the terms involving in the two equations will cancel leading to a triangular system that can be solved using back substitution.   Based on these observations, we take note of three operations that transform a system of linear equations into a new system of equations having the same solution space. Our goal is to create a new system whose solution space is the same as the original system's and may be easily described.   Scaling  We can multiply one equation by a nonzero number. For instance, has the same set of solutions as or .    Interchange  Interchanging equations will not change the set of solutions. For instance, has the same set of solutions as     Replacement  As we saw above, we may multiply one equation by a real number and add it to another equation. We call this process replacement .       Let's illustrate the use of these operations to find the solution space to the system of equations:   We will first transform the system into a triangular system so we start by eliminating from the second and third equations.  We begin with a replacement operation where we multiply the first equation by -2 and add the result to the second equation.         Another replacement operation eliminates from the third equation. We multiply the first equation by 3 and add to the third.         Scale the second equation by multiplying it by .         Eliminate from the third equation by multiplying the second equation by -4 and adding it to the third. Notice that we now have a triangular system that can be solved using back substitution.         After scaling the third equation by , we have found the value for .         We eliminate from the second equation by multiplying the third equation by -1 and adding to the second.         Finally, multiply the second equation by -2 and add to the first to obtain:       Now that we have arrived at a decoupled system, we know that there is exactly one solution to our original system of equations, which is .    One could find the same result by applying a different sequence of replacement and scaling operations. However, we chose this particular sequence guided by our desire to first transform the system into a triangular one. To do this, we eliminated the first variable from all but one equation and then proceeded to the next variables working left to right. Once we had a triangular system, we used back substitution moving through the variables right to left.  We call this process Gaussian elimination and note that it is our primary tool for solving systems of linear equations.   Gaussian Elimination   For each of the following linear systems, use Gaussian elimination to describe the solutions to the following systems of linear equations. In particular, determine whether each linear system has exactly one solution, infinitely many solutions, or no solutions.                   Our aim is to apply a sequence of scaling, interchange, and replacement operations to first put the system into a triangular form. We begin by multiplying the first equation by and adding it to the second equation. Next, we add the first equation to the third. This leads us to: We will now apply a scaling operation to make the coefficient of equal in the second equation. Another replacement operation brings the system into a triangular form.   From here, we begin the process of back substitution seeking a decoupled system. Finally, we have the decoupled system which tells us that the solution space consists of the single solution .  Once again, we begin with a sequence of replacement and scaling operations that lead to the triangular system Back substitution gives us the system The third equation does not impose a restriction on the solutions since it is satisfied for any . The second equation tells us that must equal ; however, there are infinitely many solutions to the first equation that have . Therefore, this system has infinitely many solutions.  After applying two replacement and one scaling operation, we find Another replacement operation leads to the system Since the third equation has no solutions, the original system can have no solutions as well.      There is a single solution .  There are infinitely many solutions.  There are no solutions.       Augmented matrices  After performing Gaussian elimination a few times, you probably noticed that you spent most of the time concentrating on the coefficients and simply recorded the variables as place holders. Based on this observation, we will introduce a shorthand description of linear systems.   augmented matrix When writing a linear system, we always write the variables in the same order in each equation. We then construct an augmented matrix by simply forgetting about the variables and recording the numerical data in a rectangular array. For instance, the system of equations below has the following augmented matrix          The vertical line reminds us where the equals signs appear in the equations. Entries in the matrix to the left of the vertical line correspond to coefficients of the equations. We sometimes choose to focus only on the coefficients of the system in which case we write the coefficient matrix as coefficient matrix    The three operations we perform on systems of equations translate naturally into operations on matrices. For instance, the replacement operation that multiplies the first equation by 2 and adds it to the second may be performed by multiplying the first row of the augmented matrix by 2 and adding it to the second row:       The symbol between the matrices indicates that the two matrices are related by a sequence of scaling, interchange, and replacement operations. Since these operations act on the rows of the matrices, we say that the matrices are row equivalent . Notice that the linear systems corresponding to two row equivalent augmented matrices have the same solution space. row equivalent    Augmented matrices and solution spaces      Write the augmented matrix for the linear system and perform Gaussian elimination to describe the solution space in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that the augmented matrix of a linear system has the following shape where could be any real number.    How many equations are there in this system and how many variables?   Based on our earlier discussion in , do you think it's possible that this system has exactly one solution, infinitely many solutions, or no solutions?   Suppose that this augmented matrix is row equivalent to Make a choice for the names of the variables and write the corresponding linear system. Does the system have exactly one solution, infinitely many solutions, or no solutions?         The augmented matrix for this linear system is This corresponds to the system of equations showing that there is a single solution .  The corresponding system of equations is The third equation is satisfied for any values of and . Therefore, we see that the only solution to the system is .  Here, the corresponding system of equations is Since the third equation is not satisfied for any values of and , there are no solutions to the system.  The system corresponding to this augmented matrix has three equations and five variables. Our first guess is there are infinitely many solutions. If we write out the equations corresponding to the augmented matrix, we find since the third row of the augmented matrix does not restrict the solution space. From here, we see that there are infinitely many solutions: if we make any choice for the variables , , and , we can find values for and that give a solution.      There is a single solution .  There is a single solution .  There are no solutions.  This system has three equations in five variables, and there are infinitely many solutions.       Reduced row echelon form  There is a special class of matrices whose form makes it especially easy to describe the solution space of the corresponding linear system. As we describe the properties of this class of matrices, it may be helpful to consider an example, such as the following matrix.    reduced row echelon form  We say that a matrix is in reduced row echelon form if the following properties are satisfied.   If the entries in a row are all zero, then the same is true of any row below it.    If we move across a row from left to right, the first nonzero entry we encounter is 1. We call this entry the leading entry in the row.    The leading entry in any row is to the right of the leading entries in all the rows above it.    A leading entry is the only nonzero entry in its column.    reduced row echelon matrix We call a matrix in reduced row echelon form a reduced row echelon matrix .   We have been intentionally vague about whether the matrix we are considering is an augmented matrix corresponding to a linear system or a coefficient matrix since we will consider both possibilities in the future.   Identifying reduced row echelon matrices   Consider each of the following augmented matrices. Determine if the matrix is in reduced row echelon form. If it is not, perform a sequence of scaling, interchange, and replacement operations to obtain a row equivalent matrix that is in reduced row echelon form. Then use the reduced row echelon matrix to describe the solution space.                            Because the leading entry in the first row is not a , this is not in reduced row echelon form. If we scale the first row by , however, we have a matrix in reduced row echelon form. We may write the corresponding linear system as which may be rewritten as Since may take on any value, this shows that there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .   This is not in reduced row echelon form because the row of zeroes should be at the bottom of the matrix. We also need another interchange so that the leading entry in the second row is to the right of the leading entry in the first row. Once again, there are infinitely many solutions.   This is not in reduced row echelon form because the leading entry in the second and third rows are not the only nonzero elements in their columns. We can use replacement operations to remedy this and see that the system has the single solution .      The row equivalent reduced row echelon form is and there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .  The row equivalent reduced row echelon form is There are infinitely many solutions.  The row equivalent reduced row echelon form is This system has the single solution .     The examples in the previous activity indicate that there is a sequence of row operations that transforms any matrix into one in reduced row echelon form. Moreover, the conditions that define reduced row echelon matrices guarantee that this matrix is unique.   For any given matrix, there is exactly one reduced row echelon matrix to which it is row equivalent.   Once we have this reduced row echelon matrix, we may describe the set of solutions to the corresponding linear system with relative ease.   Describing the solution space from a reduced row echelon matrix     Consider the reduced row echelon matrix and its corresponding linear system as Let's rewrite the equations as From this description, it is clear that we obtain a solution for any value of the variable . For instance, if , then and so that is a solution. Similarly, if , we see that is also a solution.  Because there is no restriction on the value of , we call it a free variable , and note that the linear system has infinitely many solutions. The variables and are called basic variables as they are determined once we make a choice of the free variable. free variable  basic variable   We will call this description of the solution space, in which the basic variables are written in terms of the free variables, a parametric description of the solution space. parametric description     Consider the matrix The last equation gives , which is true for any . We may safely ignore this equation since it does not impose a restriction on . We then see that there is a unique solution .    Consider the matrix Beginning with the last equation, we see that , which is not true for any . There is no solution to this particular equation and therefore no solution to the system of equations.        Summary  We saw several important concepts in this section.  We can describe the solution space to a linear system by transforming it into a new linear system having the same solution space through a sequence of scaling, interchange, and replacement operations.  We can represent a linear system by an augmented matrix. Using scaling, interchange, and replacement operations, the augmented matrix is row equivalent to exactly one reduced row echelon matrix. The process of constructing this reduced row echelon matrix is called Gaussian elimination.  The reduced row echelon matrix allows us to easily describe the solution space of a linear system.       For each of the linear systems below, write the associated augmented matrix and find the reduced row echelon matrix that is row equivalent to it. Identify the basic and free variables and then describe the solution space of the original linear system using a parametric description, if appropriate.                     The reduced row echelon form is and there is a single solution. Every variable is basic.  The reduced row echelon form is All the variables are basic so there is a unique solution: , , and .  The reduced row echelon form is which gives the equations This shows that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.     The augmented matrix and its reduced row echelon form are The linear system corresponding to the reduced row echelon form is This shows that there is a single solution and that every variable is a basic variable.  We have the augmented matrix This shows that all the variables are basic variables and that there is a unique solution: , , and .  The augmented matrix and its reduced row echelon form are This gives the equations showing that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.     Consider each matrix below and determine if it is in reduced row echelon form. If not, indicate the reason and apply a sequence of row operations to find its reduced row echelon matrix. For each matrix, indicate whether the corresponding linear system has infinitely many solutions, exactly one solution, or no solutions.                           The reduced row echelon form is and there are infinitely many solutions.    The reduced row echelon form is and there are no solutions.    The reduced row echelon form is and this linear system has a single solution.    The reduced row echelon form is and there are infinitely many solutions.        This is not in reduced row echelon form since the leading entry in the second row is not the only nonzero entry in its column. Applying a replacement operation gives, There are infinitely many solutions since is a free variable.    This is not in reduced row echelon form since the leading entries are not all . We can scale those rows to find the reduced row echelon form. There are no solutions since the last row gives the equation .    This is not in reduced row echelon form since the leading entry in the last row is not and is not the only nonzero entry in its column. This linear system has a single solution.    This is not in reduced row echelon form since the leading entries appear in the wrong order. Since is a free variable, there are infinitely many solutions.       Give an example of a reduced row echelon matrix that describes a linear system having the stated properties. If it is not possible to find such an example, explain why not.   Write a reduced row echelon matrix for a linear system having five equations and three variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having three equations and three variables and having no solution.   Write a reduced row echelon matrix for a linear system having three equations and five variables and having infinitely many solutions.   Write a reduced row echelon matrix for a linear system having three equations and four variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having four equations and four variables and having exactly one solution.               This is not possible.        Our matrix should have five rows and four columns. Here is an example.   Here is an example.   Here is an example.   This is not possible. If there is a solution, there will always be a free variable so there will be infinitely many solutions.   Here is an example.      For any given matrix, tells us that there is a reduced row echelon matrix that is row equivalent to it. This exercise demonstrates why this is the case. Each of the following matrices satisfies three of the four conditions required of a reduced row echelon matrix as prescribed by . For each, indicate how a sequence of row operations can be applied to form a row equivalent reduced row echelon matrix.                            Interchange the second and third rows.    Scale the second row by .    Perform interchanges among the first three rows.    Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.         Interchange the second and third rows to form     Scale the second row by to form     Perform interchanges among the first three rows so that     Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.        For each of the questions below, provide a justification for your response.   What does the presence of a row whose entries are all zero in an augmented matrix tell us about the solution space of the linear system?   How can you determine if a linear system has no solutions directly from its reduced row echelon matrix?   How can you determine if a linear system has infinitely many solutions directly from its reduced row echelon matrix?   What can you say about the solution space of a linear system if there are more variables than equations and at least one solution exists?      Nothing.  The leading entry of some row appears in the rightmost column of the augmented matrix.  There is a column that is not the rightmost and that does not contain the leading entry of a row. Also, the rightmost column does not contain a leading entry.  There are infinitely many solutions.     It doesn't tell us anything. The equation is true for any values of the variables. Therefore, it does not provide a restriction on the solution space.  There must be an equation having the form so there is a row whose leading entry is in the rightmost column of the augmented matrix.  First, we know there are solutions so no row can have its leading entry in the rightmost column of the augmented matrix. Second, there must be a free variable to have infinitely many solutions. Therefore, there must be at least one column that corresponds to a variable that does not contain the leading entry of a row.  There must be infinitely many solutions since there will be a free variable.     Determine whether the following statements are true or false and explain your reasoning.    If every variable is basic, then the linear system has exactly one solution.   If two augmented matrices are row equivalent to one another, then they describe two linear systems having the same solution spaces.   The presence of a free variable indicates that there are no solutions to the linear system.   If a linear system has exactly one solution, then it must have the same number of equations as variables.    If a linear system has the same number of equations as variables, then it has exactly one solution.     True.  True.  False.  False.  False.     This is true provided that there is some solution to the linear system. We have to avoid the situation when there are infinitely many solutions, and this happens only when there is a free variable.  This is true. When two matrices are row equivalent, there is a sequence of scaling, interchange, and replacement operations that transforms one matrix into the other. These operations do not change the solution space of the matrix.  This is false. The presence of a free variable tells us there are infinitely many solutions.  This is false. The reduced row echelon form of the augmented matrix could look like In this case, there are three equations in two variables, and the system has exactly one solution.  This is false, and here is a reduced row echelon matrix that illustrates why.      "
},
{
  "id": "exploration-1",
  "level": "2",
  "url": "sec-finding-solutions.html#exploration-1",
  "type": "Preview Activity",
  "number": "1.2.1",
  "title": "",
  "body": "  In this activity, we will consider some simple examples that will guide us in finding a more general approach.    Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:    Give a description of the solution space to the linear system:   Describe the solution space to the linear equation .  Describe the solution space to the linear equation .       The equations tell us the value of both variables so there is only one solution .    We first use the third equation to determine that . We next substitute this value into the first two equations to obtain Now we can use the second equation to determine that , substitute that value into the first equation, and determine that . This tells us that the linear system has one solution .    Notice that we can rewrite the first equation as and substitute this into the second equation to obtain This tells us that and . The linear system therefore has one solution .    No matter the value of , we have . Therefore, the solution space to the equation is all real numbers . In other words, this equation does not place a restriction on the value of .    By contrast, the equation has no solutions since no value of , when multiplied by , can produce .        .   .   .  Any real number is a solution.  There are no solutions.    "
},
{
  "id": "observation-1",
  "level": "2",
  "url": "sec-finding-solutions.html#observation-1",
  "type": "Observation",
  "number": "1.2.1",
  "title": "",
  "body": " First, finding the solution space to some systems is simple. For example, because each equation in the following system has only one variable, it prescribes a specific value for that variable. We therefore see that there is exactly one solution, which is . We call such a system decoupled . decoupled system   "
},
{
  "id": "observation-2",
  "level": "2",
  "url": "sec-finding-solutions.html#observation-2",
  "type": "Observation",
  "number": "1.2.2",
  "title": "",
  "body": " Second, there is a process that can be used to find solutions to certain types of linear systems. For instance, let's consider the system Multiplying both sides of the last equation by gives us Any solution to this linear system must then have .  Once we know that, we can substitute into the first and second equations and simplify to obtain a new system of equations having the same solutions: The second equation, after multiplying both sides by , tells us that . We can then substitute this value into the first equation to determine that .  In this way, we arrive at a decoupled system, which shows that there is exactly one solution, namely .  Our original system,  triangular system  back substitution is called a triangular system due to the shape formed by the coefficients. As this example demonstrates, triangular systems are easily solved by this process, which is called back substitution .  "
},
{
  "id": "observation-3",
  "level": "2",
  "url": "sec-finding-solutions.html#observation-3",
  "type": "Observation",
  "number": "1.2.3",
  "title": "",
  "body": " We can use substitution in a more general way to solve linear systems. For example, a natural approach to the system is to use the first equation to express in terms of : and then substitute this into the second equation and simplify: From here, we can substitute into the first equation to arrive at the solution .  However, the two-step process of solving for in terms of and substituting into the second equation may be performed more efficiently by adding a multiple of the first equation to the second. In this case, we will multiply the first equation by -2 and add to the second equation to obtain     which gives us       In this way, the system is transformed into the new triangular system Notice that this process can be reversed. Beginning with the triangular system, we can recover the original system by multiplying the first equation by 2 and adding it to the second. Because of this, the two systems have the same solution space. We will revisit this point later and give what may be a more convincing explanation.  Of course, the choice to multiply the first equation by -2 was made so that the terms involving in the two equations will cancel leading to a triangular system that can be solved using back substitution.  "
},
{
  "id": "example-1",
  "level": "2",
  "url": "sec-finding-solutions.html#example-1",
  "type": "Example",
  "number": "1.2.4",
  "title": "",
  "body": "  Let's illustrate the use of these operations to find the solution space to the system of equations:   We will first transform the system into a triangular system so we start by eliminating from the second and third equations.  We begin with a replacement operation where we multiply the first equation by -2 and add the result to the second equation.         Another replacement operation eliminates from the third equation. We multiply the first equation by 3 and add to the third.         Scale the second equation by multiplying it by .         Eliminate from the third equation by multiplying the second equation by -4 and adding it to the third. Notice that we now have a triangular system that can be solved using back substitution.         After scaling the third equation by , we have found the value for .         We eliminate from the second equation by multiplying the third equation by -1 and adding to the second.         Finally, multiply the second equation by -2 and add to the first to obtain:       Now that we have arrived at a decoupled system, we know that there is exactly one solution to our original system of equations, which is .   "
},
{
  "id": "activity-4",
  "level": "2",
  "url": "sec-finding-solutions.html#activity-4",
  "type": "Activity",
  "number": "1.2.2",
  "title": "Gaussian Elimination.",
  "body": " Gaussian Elimination   For each of the following linear systems, use Gaussian elimination to describe the solutions to the following systems of linear equations. In particular, determine whether each linear system has exactly one solution, infinitely many solutions, or no solutions.                   Our aim is to apply a sequence of scaling, interchange, and replacement operations to first put the system into a triangular form. We begin by multiplying the first equation by and adding it to the second equation. Next, we add the first equation to the third. This leads us to: We will now apply a scaling operation to make the coefficient of equal in the second equation. Another replacement operation brings the system into a triangular form.   From here, we begin the process of back substitution seeking a decoupled system. Finally, we have the decoupled system which tells us that the solution space consists of the single solution .  Once again, we begin with a sequence of replacement and scaling operations that lead to the triangular system Back substitution gives us the system The third equation does not impose a restriction on the solutions since it is satisfied for any . The second equation tells us that must equal ; however, there are infinitely many solutions to the first equation that have . Therefore, this system has infinitely many solutions.  After applying two replacement and one scaling operation, we find Another replacement operation leads to the system Since the third equation has no solutions, the original system can have no solutions as well.      There is a single solution .  There are infinitely many solutions.  There are no solutions.    "
},
{
  "id": "activity-5",
  "level": "2",
  "url": "sec-finding-solutions.html#activity-5",
  "type": "Activity",
  "number": "1.2.3",
  "title": "Augmented matrices and solution spaces.",
  "body": " Augmented matrices and solution spaces      Write the augmented matrix for the linear system and perform Gaussian elimination to describe the solution space in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that you have a linear system in the variables and whose augmented matrix is row equivalent to Write the linear system corresponding to this augmented matrix and describe its solution set in as much detail as you can.    Suppose that the augmented matrix of a linear system has the following shape where could be any real number.    How many equations are there in this system and how many variables?   Based on our earlier discussion in , do you think it's possible that this system has exactly one solution, infinitely many solutions, or no solutions?   Suppose that this augmented matrix is row equivalent to Make a choice for the names of the variables and write the corresponding linear system. Does the system have exactly one solution, infinitely many solutions, or no solutions?         The augmented matrix for this linear system is This corresponds to the system of equations showing that there is a single solution .  The corresponding system of equations is The third equation is satisfied for any values of and . Therefore, we see that the only solution to the system is .  Here, the corresponding system of equations is Since the third equation is not satisfied for any values of and , there are no solutions to the system.  The system corresponding to this augmented matrix has three equations and five variables. Our first guess is there are infinitely many solutions. If we write out the equations corresponding to the augmented matrix, we find since the third row of the augmented matrix does not restrict the solution space. From here, we see that there are infinitely many solutions: if we make any choice for the variables , , and , we can find values for and that give a solution.      There is a single solution .  There is a single solution .  There are no solutions.  This system has three equations in five variables, and there are infinitely many solutions.    "
},
{
  "id": "reduced-row-echelon",
  "level": "2",
  "url": "sec-finding-solutions.html#reduced-row-echelon",
  "type": "Definition",
  "number": "1.2.5",
  "title": "",
  "body": " reduced row echelon form  We say that a matrix is in reduced row echelon form if the following properties are satisfied.   If the entries in a row are all zero, then the same is true of any row below it.    If we move across a row from left to right, the first nonzero entry we encounter is 1. We call this entry the leading entry in the row.    The leading entry in any row is to the right of the leading entries in all the rows above it.    A leading entry is the only nonzero entry in its column.    reduced row echelon matrix We call a matrix in reduced row echelon form a reduced row echelon matrix .  "
},
{
  "id": "activity-6",
  "level": "2",
  "url": "sec-finding-solutions.html#activity-6",
  "type": "Activity",
  "number": "1.2.4",
  "title": "Identifying reduced row echelon matrices.",
  "body": " Identifying reduced row echelon matrices   Consider each of the following augmented matrices. Determine if the matrix is in reduced row echelon form. If it is not, perform a sequence of scaling, interchange, and replacement operations to obtain a row equivalent matrix that is in reduced row echelon form. Then use the reduced row echelon matrix to describe the solution space.                            Because the leading entry in the first row is not a , this is not in reduced row echelon form. If we scale the first row by , however, we have a matrix in reduced row echelon form. We may write the corresponding linear system as which may be rewritten as Since may take on any value, this shows that there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .   This is not in reduced row echelon form because the row of zeroes should be at the bottom of the matrix. We also need another interchange so that the leading entry in the second row is to the right of the leading entry in the first row. Once again, there are infinitely many solutions.   This is not in reduced row echelon form because the leading entry in the second and third rows are not the only nonzero elements in their columns. We can use replacement operations to remedy this and see that the system has the single solution .      The row equivalent reduced row echelon form is and there are infinitely many solutions.   This matrix is in reduced row echelon form. There is a single solution .   This matrix is also in reduced row echelon form. However, this are no solutions since the third equation is .  The row equivalent reduced row echelon form is There are infinitely many solutions.  The row equivalent reduced row echelon form is This system has the single solution .    "
},
{
  "id": "thm-rref-is-unique",
  "level": "2",
  "url": "sec-finding-solutions.html#thm-rref-is-unique",
  "type": "Theorem",
  "number": "1.2.6",
  "title": "",
  "body": " For any given matrix, there is exactly one reduced row echelon matrix to which it is row equivalent.  "
},
{
  "id": "example-2",
  "level": "2",
  "url": "sec-finding-solutions.html#example-2",
  "type": "Example",
  "number": "1.2.7",
  "title": "Describing the solution space from a reduced row echelon matrix.",
  "body": " Describing the solution space from a reduced row echelon matrix     Consider the reduced row echelon matrix and its corresponding linear system as Let's rewrite the equations as From this description, it is clear that we obtain a solution for any value of the variable . For instance, if , then and so that is a solution. Similarly, if , we see that is also a solution.  Because there is no restriction on the value of , we call it a free variable , and note that the linear system has infinitely many solutions. The variables and are called basic variables as they are determined once we make a choice of the free variable. free variable  basic variable   We will call this description of the solution space, in which the basic variables are written in terms of the free variables, a parametric description of the solution space. parametric description     Consider the matrix The last equation gives , which is true for any . We may safely ignore this equation since it does not impose a restriction on . We then see that there is a unique solution .    Consider the matrix Beginning with the last equation, we see that , which is not true for any . There is no solution to this particular equation and therefore no solution to the system of equations.     "
},
{
  "id": "exercise-1",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-1",
  "type": "Exercise",
  "number": "1.2.5.1",
  "title": "",
  "body": " For each of the linear systems below, write the associated augmented matrix and find the reduced row echelon matrix that is row equivalent to it. Identify the basic and free variables and then describe the solution space of the original linear system using a parametric description, if appropriate.                     The reduced row echelon form is and there is a single solution. Every variable is basic.  The reduced row echelon form is All the variables are basic so there is a unique solution: , , and .  The reduced row echelon form is which gives the equations This shows that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.     The augmented matrix and its reduced row echelon form are The linear system corresponding to the reduced row echelon form is This shows that there is a single solution and that every variable is a basic variable.  We have the augmented matrix This shows that all the variables are basic variables and that there is a unique solution: , , and .  The augmented matrix and its reduced row echelon form are This gives the equations showing that is a free variable and , , and are basic variables. There are therefore infinitely many solutions.   "
},
{
  "id": "exercise-2",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-2",
  "type": "Exercise",
  "number": "1.2.5.2",
  "title": "",
  "body": " Consider each matrix below and determine if it is in reduced row echelon form. If not, indicate the reason and apply a sequence of row operations to find its reduced row echelon matrix. For each matrix, indicate whether the corresponding linear system has infinitely many solutions, exactly one solution, or no solutions.                           The reduced row echelon form is and there are infinitely many solutions.    The reduced row echelon form is and there are no solutions.    The reduced row echelon form is and this linear system has a single solution.    The reduced row echelon form is and there are infinitely many solutions.        This is not in reduced row echelon form since the leading entry in the second row is not the only nonzero entry in its column. Applying a replacement operation gives, There are infinitely many solutions since is a free variable.    This is not in reduced row echelon form since the leading entries are not all . We can scale those rows to find the reduced row echelon form. There are no solutions since the last row gives the equation .    This is not in reduced row echelon form since the leading entry in the last row is not and is not the only nonzero entry in its column. This linear system has a single solution.    This is not in reduced row echelon form since the leading entries appear in the wrong order. Since is a free variable, there are infinitely many solutions.     "
},
{
  "id": "exercise-3",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-3",
  "type": "Exercise",
  "number": "1.2.5.3",
  "title": "",
  "body": " Give an example of a reduced row echelon matrix that describes a linear system having the stated properties. If it is not possible to find such an example, explain why not.   Write a reduced row echelon matrix for a linear system having five equations and three variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having three equations and three variables and having no solution.   Write a reduced row echelon matrix for a linear system having three equations and five variables and having infinitely many solutions.   Write a reduced row echelon matrix for a linear system having three equations and four variables and having exactly one solution.   Write a reduced row echelon matrix for a linear system having four equations and four variables and having exactly one solution.               This is not possible.        Our matrix should have five rows and four columns. Here is an example.   Here is an example.   Here is an example.   This is not possible. If there is a solution, there will always be a free variable so there will be infinitely many solutions.   Here is an example.    "
},
{
  "id": "exercise-4",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-4",
  "type": "Exercise",
  "number": "1.2.5.4",
  "title": "",
  "body": " For any given matrix, tells us that there is a reduced row echelon matrix that is row equivalent to it. This exercise demonstrates why this is the case. Each of the following matrices satisfies three of the four conditions required of a reduced row echelon matrix as prescribed by . For each, indicate how a sequence of row operations can be applied to form a row equivalent reduced row echelon matrix.                            Interchange the second and third rows.    Scale the second row by .    Perform interchanges among the first three rows.    Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.         Interchange the second and third rows to form     Scale the second row by to form     Perform interchanges among the first three rows so that     Perform row replacement operations so that the leading entry in the third row is the only nonzero entry in its column.      "
},
{
  "id": "exercise-5",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-5",
  "type": "Exercise",
  "number": "1.2.5.5",
  "title": "",
  "body": " For each of the questions below, provide a justification for your response.   What does the presence of a row whose entries are all zero in an augmented matrix tell us about the solution space of the linear system?   How can you determine if a linear system has no solutions directly from its reduced row echelon matrix?   How can you determine if a linear system has infinitely many solutions directly from its reduced row echelon matrix?   What can you say about the solution space of a linear system if there are more variables than equations and at least one solution exists?      Nothing.  The leading entry of some row appears in the rightmost column of the augmented matrix.  There is a column that is not the rightmost and that does not contain the leading entry of a row. Also, the rightmost column does not contain a leading entry.  There are infinitely many solutions.     It doesn't tell us anything. The equation is true for any values of the variables. Therefore, it does not provide a restriction on the solution space.  There must be an equation having the form so there is a row whose leading entry is in the rightmost column of the augmented matrix.  First, we know there are solutions so no row can have its leading entry in the rightmost column of the augmented matrix. Second, there must be a free variable to have infinitely many solutions. Therefore, there must be at least one column that corresponds to a variable that does not contain the leading entry of a row.  There must be infinitely many solutions since there will be a free variable.   "
},
{
  "id": "exercise-6",
  "level": "2",
  "url": "sec-finding-solutions.html#exercise-6",
  "type": "Exercise",
  "number": "1.2.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.    If every variable is basic, then the linear system has exactly one solution.   If two augmented matrices are row equivalent to one another, then they describe two linear systems having the same solution spaces.   The presence of a free variable indicates that there are no solutions to the linear system.   If a linear system has exactly one solution, then it must have the same number of equations as variables.    If a linear system has the same number of equations as variables, then it has exactly one solution.     True.  True.  False.  False.  False.     This is true provided that there is some solution to the linear system. We have to avoid the situation when there are infinitely many solutions, and this happens only when there is a free variable.  This is true. When two matrices are row equivalent, there is a sequence of scaling, interchange, and replacement operations that transforms one matrix into the other. These operations do not change the solution space of the matrix.  This is false. The presence of a free variable tells us there are infinitely many solutions.  This is false. The reduced row echelon form of the augmented matrix could look like In this case, there are three equations in two variables, and the system has exactly one solution.  This is false, and here is a reduced row echelon matrix that illustrates why.    "
},
{
  "id": "sec-sage-introduction",
  "level": "1",
  "url": "sec-sage-introduction.html",
  "type": "Section",
  "number": "1.3",
  "title": "Computation with Sage",
  "body": " Computation with Sage   Linear algebra owes its prominence as a powerful scientific tool to the ever-growing power of computers. Carl Cowen, a former president of the Mathematical Association of America, has said, No serious application of linear algebra happens without a computer. Indeed, Cowen notes that, in the 1950s, working with a system of 100 equations in 100 variables was difficult. Today, scientists and mathematicians routinely work on problems that are vastly larger. This is only possible because of today's computing power.  It is therefore important for any student of linear algebra to become comfortable solving linear algebraic problems on a computer. This section will introduce you to a program called Sage that can help. While you may be able to do much of this work on a graphing calculator, you are encouraged to become comfortable with Sage as we will use increasingly powerful features as we encounter their need.    Introduction to Sage  There are several ways to access Sage.   If you are reading this book online, there will be embedded Sage cells at appropriate places in the text. You have the opportunity to type Sage commands into these cells and execute them, provided you are connected to the Internet. Please be aware that your work will be lost if you reload the page.  Here is a Sage cell containing a command that asks Sage to multiply 5 and 3. You may execute the command by pressing the Evaluate button.     You may also go to , sign up for an account, open a new project, and create a Sage worksheet. Once inside the worksheet, you may enter commands as shown here, and evaluate them by pressing Enter on your keyboard while holding down the Shift key.    There is a page of Sage cells at . Any results obtained by evaluating one cell are available in other cells. However, your work will be lost when the page is reloaded.     Throughout the text, we will introduce new Sage commands that allow us to explore linear algebraic concepts. These commands are collected and summarized in the reference found in Appendix A .   Basic Sage commands      Sage uses the standard operators +, -, *, \/, and ^ for the usual arithmetic operations. By entering text in the cell below, ask Sage to evaluate      Notice that we can create new lines by pressing Enter and entering additional commands on them. What happens when you evaluate this Sage cell?   Notice that we only see the result from the last command. With the print command, we may see earlier results, if we wish.     We may give a name to the result of one command and refer to it in a later command.   Suppose you have three tests in your linear algebra class and your scores are 90, 100, and 98. In the Sage cell below, add your scores together and call the result total . On the next line, find the average of your test scores and print it.     If you are not a programmer, you may ignore this part. If you are an experienced programmer, however, you should know that Sage is written in the Python programming language and that you may enter Python code into a Sage cell.           3 + 4*(2^4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cells print the integers from 0 to 9.         3 + 4*(2^4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cells print the integers from 0 to 9.        Sage and matrices  When we encounter a matrix, tells us that there is exactly one reduced row echelon matrix that is row equivalent to it.  In fact, the uniqueness of this reduced row echelon matrix is what motivates us to define this particular form. When solving a system of linear equations using Gaussian elimination, there are other row equivalent matrices that reveal the structure of the solution space. The reduced row echelon matrix is simply a convenience as it is an agreement we make with one another to seek the same matrix.  An added benefit is that we can ask a computer program, like Sage, to find reduced row echelon matrices for us. We will learn how to do this now that we have a little familiarity with Sage.  First, notice that a matrix has a certain number of rows and columns. For instance, the matrix has three rows and five columns. We consequently refer to this as a matrix.  We may ask Sage to create the matrix by entering When evaluated, Sage will confirm the matrix by writing out the rows of the matrix, each inside square brackets.  Notice that there are three separate things (we call them arguments ) inside the parentheses: the number of rows, the number of columns, and the entries of the matrix listed by row inside square brackets. These three arguments are separated by commas. Notice that there is no way of specifying whether this is an augmented or coefficient matrix so it will be up to us to interpret our results appropriately.   Sage syntax  Some common mistakes are   to forget the square brackets around the list of entries,   to omit an entry from the list or to add an extra one,   to forget to separate the rows, columns, and entries by commas, and   to omit the parentheses around the arguments after matrix .  If you see an error message, carefully proofread your input and try again.   Alternatively, you can create a matrix by simply listing its rows, like this matrix([ [-1, 0, 2, 7], [ 2, 1,-3,-1] ])    Using Sage to find row reduced echelon matrices      Enter the following matrix into Sage.      Give the matrix the name by entering A = matrix( ..., ..., [ ... ]) We may then find its reduced row echelon form by entering A = matrix( ..., ..., [ ... ]) A.rref() A common mistake is to forget the parentheses after rref .  Use Sage to find the reduced row echelon form of the matrix from of this activity.     Use Sage to describe the solution space of the system of linear equations      Consider the two matrices: We say that is an augmentation of because it is obtained from by adding some more columns.  Using Sage, define the matrices and compare their reduced row echelon forms. What do you notice about the relationship between the two reduced row echelon forms?       Using the system of equations in , write the augmented matrix corresponding to the system of equations. What did you find for the reduced row echelon form of the augmented matrix?  Now write the coefficient matrix of this system of equations. What does of this activity tell you about its reduced row echelon form?           matrix(3, 4, [-1,-2, 2,-1, 2, 4,-1, 5, 1, 2, 0, 3])     The reduced row echelon form of the matrix is     Sage tells us that the reduced row echelon form of the corresponding augmented matrix is so there is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is            matrix(3, 4, [-1,-2, 2,-1, 2, 4,-1, 5, 1, 2, 0, 3])     The reduced row echelon form of the matrix is     There is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is         Sage practices  Here are some practices that you may find helpful when working with matrices in Sage.   Break the matrix entries across lines, one for each row, for better readability by pressing Enter between rows. A = matrix(2, 4, [ 1, 2, -1, 0, -3, 0, 4, 3 ])    Print your original matrix to check that you have entered it correctly. You may want to also print a dividing line to separate matrices. A = matrix(2, 2, [ 1, 2, 2, 2]) print (A) print (\"---------\") A.rref()       The last part of the previous activity, , demonstrates something that will be helpful for us in the future. In that activity, we started with a matrix , which we augmented by adding some columns to obtain a matrix . We then noticed that the reduced row echelon form of is itself an augmentation of the reduced row echelon form of .  To illustrate, we can consider the reduced row echelon form of the augmented matrix:   We can then determine the reduced row echelon form of the coefficient matrix by looking inside the augmented matrix.   If we trace through the steps in the Gaussian elimination algorithm carefully, we see that this is a general principle, which we now state.   Augmentation Principle  If matrix is an augmentation of matrix , then the reduced row echelon form of is an augmentation of the reduced row echelon form of .     Computational effort  At the beginning of this section, we indicated that linear algebra has become more prominent as computers have grown more powerful. Computers, however, still have limits. Let's consider how much effort is expended when we ask to find the reduced row echelon form of a matrix. We will measure, very roughly, the effort by the number of times the algorithm requires us to multiply or add two numbers.  We will assume that our matrix has the same number of rows as columns, which we call . We are mainly interested in the case when is very large, which is when we need to worry about how much effort is required.  Let's first consider the effort required for each of our row operations.   Scaling a row multiplies each of the entries in a row by some number, which requires operations.   Interchanging two rows requires no multiplications or additions so we won't worry about the effort required by an interchange.   A replacement requires us to multiply each entry in a row by some number, which takes operations, and then add the resulting entries to another row, which requires another operations. The total number of operations is .    Our goal is to transform a matrix to its reduced row echelon form, which looks something like this: . We roughly perform one replacement operation for every 0 entry in the reduced row echelon matrix. When is very large, most of the entries in the reduced row echelon form are 0 so we need roughly replacements. Since each replacement operation requires operations, the number of operations resulting from the needed replacements is roughly .  Each row is scaled roughly one time so there are roughly scaling operations, each of which requires operations. The number of operations due to scaling is roughly .  Therefore, the total number of operations is roughly . When is very large, the term is much smaller than the term. We therefore state that    The number of operations required to find the reduced row echelon form of an matrix is roughly proportional to .    This is a very rough measure of the effort required to find the reduced row echelon form; a more careful accounting shows that the number of arithmetic operations is roughly . As we have seen, some matrices require more effort than others, but the upshot of this observation is that the effort is proportional to . We can think of this in the following way: If the size of the matrix grows by a factor of 10, then the effort required grows by a factor of .  While today's computers are powerful, they cannot handle every problem we might ask of them. Eventually, we would like to be able to consider matrices that have (a trillion) rows and columns. In very broad terms, the effort required to find the reduced row echelon matrix will require roughly operations.  To put this into context, imagine we need to solve a linear system with a trillion equations and a trillion variables and that we have a computer that can perform a trillion, , operations every second. Finding the reduced row echelon form would take about years. At this time, the universe is estimated to be approximately years old. If we started the calculation when the universe was born, we'd be about one-millionth of the way through.  This may seem like an absurd situation, but we'll see in how we use the results of such a computation every day. Clearly, we will need some better tools to deal with really big problems like this one.    Summary  We learned some basic features of Sage with an emphasis on finding the reduced row echelon form of a matrix.   Sage can perform basic arithmetic using standard operators. Sage can also save results from one command to be reused in a later command.   We may define matrices in Sage and find the reduced row echelon form using the rref command.   We saw an example of the Augmentation Principle , which we then stated as a general principle.   We saw that the computational effort required to find the reduced row echelon form of an matrix is proportional to .   Appendix A contains a reference outlining the Sage commands that we have encountered.     Consider the linear system Write this system as an augmented matrix and use Sage to find a description of the solution space.    There is exactly one solution .   We can use Sage to find the reduced row echelon form of the corresponding augmented matrix: This shows that there is exactly one solution .    Shown below are some traffic patterns in the downtown area of a large city. The figures give the number of cars per hour traveling along each road. Any car that drives into an intersection must also leave the intersection. This means that the number of cars entering an intersection in an hour is equal to the number of cars leaving the intersection.   Let's begin with the following traffic pattern.   How many cars per hour enter the upper left intersection? How many cars per hour leave this intersection? Use this to form a linear equation in the variables , , , and .      Form three more linear equations from the other three intersections to form a linear system having four equations in four variables. Then use Sage to find the solution space to this system.    Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.      Another traffic pattern is shown below.      Once again, write a linear system for the quantities , , , and and solve the system using the Sage cell below.   What can you say about the solution of this linear system? Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.  What is the smallest possible amount of traffic flowing through ?           There is one solution: , , , and .    There are infinitely many solutions described by         We will form a linear system by considering each intersection and equating the number of cars that enter the intersection to the number of cars that leave.   For the intersection in the upper left, the number of cars that enter is while the number of cars that leave is . This gives the equation or .    By studying the other three intersections, we form a linear system consisting of four equations in four variables: If we express these equations in terms of an augmented matrix and use Sage to find its reduced row echelon form, we find     There is exactly one solution: , , , and . This is what we expect since the traffic flow is determined by the upper right intersection. From here, all the other flows are determined as well.       Let's now look at another traffic pattern.   In this case, we find the equations: This gives the augmented matrix:     Here, we may view as a free variable and write This shows that there are infinitely many solutions. This makes sense since the unknown flows , , , and form a loop. For instance, we can add one car to each of , , and provided we remove one from .    Since we must have , it follows that the free variable satisfies . This means that satisfies .         A typical problem in thermodynamics is to find the steady-state temperature distribution inside a thin plate if we know the temperature around the boundary. Let be the temperatures at the six nodes inside the plate as shown below.   The temperature at a node is approximately the average of the four nearest nodes: for instance, , which we may rewrite as .  Set up a linear system to find the temperature at these six points inside the plate. Then use Sage to solve the linear system. If all the entries of the matrix are integers, Sage will compute the reduced row echelon form using rational numbers. To view a decimal approximation of the results, you may use A.rref().numerical_approx(digits=4)    In the real world, the approximation becomes better as we add more and more points into the grid. This is a situation where we may want to solve a linear system having millions of equations and millions of variables.       We set up the equations Setting up an augmented matrix and using Sage to find its reduced row echelon form, we obtain     The fuel inside model rocket motors is a black powder mixture that ideally consists of 60% charcoal, 30% potassium nitrate, and 10% sulfur by weight.  Suppose you work at a company that makes model rocket motors. When you come into work one morning, you learn that yesterday's first shift made a perfect batch of fuel. The second shift, however, misread the recipe and used 50% charcoal, 20% potassium nitrate and 30% sulfur. Then the two batches were mixed together. A chemical analysis shows that there are 100.3 pounds of charcoal in the mixture and 46.9 pounds of potassium nitrate.   Assuming the first shift produced pounds of fuel and the second pounds, set up a linear system in terms of and . How many pounds of fuel did the first shift produce and how many did the second shift produce?   How much sulfur would you expect to find in the mixture?     The first shift produces pounds while the second shift produces pounds. There are pounds of sulfur.      Writing equations for the amount of charcoal and the amount of potassium nitrate, we have Solving this gives pounds and pounds.    The amount of sulfur will be pounds.       This exercise is about balancing chemical reactions.  Chemists denote a molecule of water as , which means it is composed of two atoms of hydrogen (H) and one atom of oxygen (O). The process by which hydrogen burns is described by the chemical reaction This means that molecules of hydrogen combine with molecules of oxygen to produce water molecules. The number of hydrogen atoms is the same before and after the reaction; the same is true of the oxygen atoms.     In terms of , , and , how many hydrogen atoms are there before the reaction? How many hydrogen atoms are there after the reaction? Find a linear equation in , , and by equating these quantities.   Find a second linear equation in , , and by equating the number of oxygen atoms before and after the reaction.   Find the solutions of this linear system. Why are there infinitely many solutions?    In this chemical setting, , , and should be positive integers. Find the solution where , , and are the smallest possible positive integers.      Now consider the reaction where potassium permanganate and manganese sulfate combine with water to produce manganese dioxide, potassium sulfate, and sulfuric acid: As in the previous exercise, find the appropriate values for to balance the chemical reaction.          We have the reaction     We have the reaction          We will equate the number of hydrogen and oxygen atoms before and after the reaction.   Before the reaction, there are two hydrogen atoms for every hydrogen molecule. Since there are hydrogen molecules, we have hydrogen atoms before the reaction.  After the reaction, there are two hydrogen atoms for every water molecule. Since there are hydrogen molecules, we have hydrogen atoms after the reaction. This gives the equation .    Similarly, there are oxygen atoms before the reaction and oxygen atoms after. This gives the equation .    We have the linear system When we create the augmented matrix and find its reduced row echelon form, we see that is a free variable so that We need all the quantities to be integers, and the smallest value of that makes an integer is . Therefore, we have , , and . The reaction is then        Proceeding as in the previous part, we have Solving this linear system, we find Once again, we choose to be the smallest positive integer that causes all of the variables to be integers. This means that so that we have This gives the reaction:        We began this section by stating that increasing computational power has helped linear algebra assume a prominent role as a scientific tool. Later, we looked at one computational limitation: once a matrix gets to be too big, it is not reasonable to apply Gaussian elimination to find its reduced row echelon form.  In this exercise, we will see another limitation: computer arithmetic with real numbers is only an approximation because computers represent real numbers with only a finite number of bits. For instance, the number pi would be approximated inside a computer by, say, Most of the time, this is not a problem. However, when we perform millions or even billions of arithmetic operations, the error in these approximations starts to accumulate and can lead to results that are wildly inaccurate. Here are two examples demonstrating this.   Let's first see an example showing that computer arithmetic really is an approximation. First, consider the linear system If the coefficients are entered into Sage as fractions, Sage will find the exact reduced row echelon form. Find the exact solution to this linear system.   Now let's ask Sage to compute with real numbers. We can do this by representing one of the coefficients as a decimal. For instance, the same linear system can be represented as Most computers do arithmetic using either 32 or 64 bits. To magnify the problem so that we can see it better, we will ask Sage to do arithmetic using only 10 bits as follows. What does Sage give for the solution now? Compare this to the exact solution that you found previously.   Some types of linear systems are particularly sensitive to errors resulting from computers' approximate arithmetic. For instance, suppose we are interested in the linear system Find the solution to this linear system.   Suppose now that the computer has accumulated some error in one of the entries of this system so that it incorrectly stores the system as Find the solution to this linear system.   Notice how a small error in one of the entries in the linear system leads to a solution that has a dramatically large error. Fortunately, this is an issue that has been well studied, and there are techniques that mitigate this type of behavior.         The exact solution is while the approximate solution is     This solution to the first system is and . However, with just a small change in the system, we find the solution and .         The exact solution is However, if we compute with only 10 bits, we find the approximate solution     This solution to the first system is and . However, with just a small change in the system, we find the solution and .       "
},
{
  "id": "activity-7",
  "level": "2",
  "url": "sec-sage-introduction.html#activity-7",
  "type": "Activity",
  "number": "1.3.1",
  "title": "Basic Sage commands.",
  "body": " Basic Sage commands      Sage uses the standard operators +, -, *, \/, and ^ for the usual arithmetic operations. By entering text in the cell below, ask Sage to evaluate      Notice that we can create new lines by pressing Enter and entering additional commands on them. What happens when you evaluate this Sage cell?   Notice that we only see the result from the last command. With the print command, we may see earlier results, if we wish.     We may give a name to the result of one command and refer to it in a later command.   Suppose you have three tests in your linear algebra class and your scores are 90, 100, and 98. In the Sage cell below, add your scores together and call the result total . On the next line, find the average of your test scores and print it.     If you are not a programmer, you may ignore this part. If you are an experienced programmer, however, you should know that Sage is written in the Python programming language and that you may enter Python code into a Sage cell.           3 + 4*(2^4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cells print the integers from 0 to 9.         3 + 4*(2^4 - 1)     Only the results of the last line are displayed.     total = 90 + 100 + 98 average = total \/ 3 print(average)     This cells print the integers from 0 to 9.     "
},
{
  "id": "activity-8",
  "level": "2",
  "url": "sec-sage-introduction.html#activity-8",
  "type": "Activity",
  "number": "1.3.2",
  "title": "Using Sage to find row reduced echelon matrices.",
  "body": " Using Sage to find row reduced echelon matrices      Enter the following matrix into Sage.      Give the matrix the name by entering A = matrix( ..., ..., [ ... ]) We may then find its reduced row echelon form by entering A = matrix( ..., ..., [ ... ]) A.rref() A common mistake is to forget the parentheses after rref .  Use Sage to find the reduced row echelon form of the matrix from of this activity.     Use Sage to describe the solution space of the system of linear equations      Consider the two matrices: We say that is an augmentation of because it is obtained from by adding some more columns.  Using Sage, define the matrices and compare their reduced row echelon forms. What do you notice about the relationship between the two reduced row echelon forms?       Using the system of equations in , write the augmented matrix corresponding to the system of equations. What did you find for the reduced row echelon form of the augmented matrix?  Now write the coefficient matrix of this system of equations. What does of this activity tell you about its reduced row echelon form?           matrix(3, 4, [-1,-2, 2,-1, 2, 4,-1, 5, 1, 2, 0, 3])     The reduced row echelon form of the matrix is     Sage tells us that the reduced row echelon form of the corresponding augmented matrix is so there is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is            matrix(3, 4, [-1,-2, 2,-1, 2, 4,-1, 5, 1, 2, 0, 3])     The reduced row echelon form of the matrix is     There is a unique solution .    The first four columns of the reduced row echelon form of form the reduced row echelon form of .    The reduced row echelon form of the coefficient matrix is       "
},
{
  "id": "principle-augmentation-principle",
  "level": "2",
  "url": "sec-sage-introduction.html#principle-augmentation-principle",
  "type": "Proposition",
  "number": "1.3.1",
  "title": "Augmentation Principle.",
  "body": " Augmentation Principle  If matrix is an augmentation of matrix , then the reduced row echelon form of is an augmentation of the reduced row echelon form of .  "
},
{
  "id": "observation-4",
  "level": "2",
  "url": "sec-sage-introduction.html#observation-4",
  "type": "Observation",
  "number": "1.3.2",
  "title": "",
  "body": "  The number of operations required to find the reduced row echelon form of an matrix is roughly proportional to .   "
},
{
  "id": "exercise-7",
  "level": "2",
  "url": "sec-sage-introduction.html#exercise-7",
  "type": "Exercise",
  "number": "1.3.5.1",
  "title": "",
  "body": " Consider the linear system Write this system as an augmented matrix and use Sage to find a description of the solution space.    There is exactly one solution .   We can use Sage to find the reduced row echelon form of the corresponding augmented matrix: This shows that there is exactly one solution .  "
},
{
  "id": "exercise-8",
  "level": "2",
  "url": "sec-sage-introduction.html#exercise-8",
  "type": "Exercise",
  "number": "1.3.5.2",
  "title": "",
  "body": " Shown below are some traffic patterns in the downtown area of a large city. The figures give the number of cars per hour traveling along each road. Any car that drives into an intersection must also leave the intersection. This means that the number of cars entering an intersection in an hour is equal to the number of cars leaving the intersection.   Let's begin with the following traffic pattern.   How many cars per hour enter the upper left intersection? How many cars per hour leave this intersection? Use this to form a linear equation in the variables , , , and .      Form three more linear equations from the other three intersections to form a linear system having four equations in four variables. Then use Sage to find the solution space to this system.    Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.      Another traffic pattern is shown below.      Once again, write a linear system for the quantities , , , and and solve the system using the Sage cell below.   What can you say about the solution of this linear system? Is there exactly one solution or infinitely many solutions? Explain why you would expect this given the information provided.  What is the smallest possible amount of traffic flowing through ?           There is one solution: , , , and .    There are infinitely many solutions described by         We will form a linear system by considering each intersection and equating the number of cars that enter the intersection to the number of cars that leave.   For the intersection in the upper left, the number of cars that enter is while the number of cars that leave is . This gives the equation or .    By studying the other three intersections, we form a linear system consisting of four equations in four variables: If we express these equations in terms of an augmented matrix and use Sage to find its reduced row echelon form, we find     There is exactly one solution: , , , and . This is what we expect since the traffic flow is determined by the upper right intersection. From here, all the other flows are determined as well.       Let's now look at another traffic pattern.   In this case, we find the equations: This gives the augmented matrix:     Here, we may view as a free variable and write This shows that there are infinitely many solutions. This makes sense since the unknown flows , , , and form a loop. For instance, we can add one car to each of , , and provided we remove one from .    Since we must have , it follows that the free variable satisfies . This means that satisfies .       "
},
{
  "id": "exercise-9",
  "level": "2",
  "url": "sec-sage-introduction.html#exercise-9",
  "type": "Exercise",
  "number": "1.3.5.3",
  "title": "",
  "body": " A typical problem in thermodynamics is to find the steady-state temperature distribution inside a thin plate if we know the temperature around the boundary. Let be the temperatures at the six nodes inside the plate as shown below.   The temperature at a node is approximately the average of the four nearest nodes: for instance, , which we may rewrite as .  Set up a linear system to find the temperature at these six points inside the plate. Then use Sage to solve the linear system. If all the entries of the matrix are integers, Sage will compute the reduced row echelon form using rational numbers. To view a decimal approximation of the results, you may use A.rref().numerical_approx(digits=4)    In the real world, the approximation becomes better as we add more and more points into the grid. This is a situation where we may want to solve a linear system having millions of equations and millions of variables.       We set up the equations Setting up an augmented matrix and using Sage to find its reduced row echelon form, we obtain   "
},
{
  "id": "exercise-10",
  "level": "2",
  "url": "sec-sage-introduction.html#exercise-10",
  "type": "Exercise",
  "number": "1.3.5.4",
  "title": "",
  "body": " The fuel inside model rocket motors is a black powder mixture that ideally consists of 60% charcoal, 30% potassium nitrate, and 10% sulfur by weight.  Suppose you work at a company that makes model rocket motors. When you come into work one morning, you learn that yesterday's first shift made a perfect batch of fuel. The second shift, however, misread the recipe and used 50% charcoal, 20% potassium nitrate and 30% sulfur. Then the two batches were mixed together. A chemical analysis shows that there are 100.3 pounds of charcoal in the mixture and 46.9 pounds of potassium nitrate.   Assuming the first shift produced pounds of fuel and the second pounds, set up a linear system in terms of and . How many pounds of fuel did the first shift produce and how many did the second shift produce?   How much sulfur would you expect to find in the mixture?     The first shift produces pounds while the second shift produces pounds. There are pounds of sulfur.      Writing equations for the amount of charcoal and the amount of potassium nitrate, we have Solving this gives pounds and pounds.    The amount of sulfur will be pounds.     "
},
{
  "id": "exercise-11",
  "level": "2",
  "url": "sec-sage-introduction.html#exercise-11",
  "type": "Exercise",
  "number": "1.3.5.5",
  "title": "",
  "body": " This exercise is about balancing chemical reactions.  Chemists denote a molecule of water as , which means it is composed of two atoms of hydrogen (H) and one atom of oxygen (O). The process by which hydrogen burns is described by the chemical reaction This means that molecules of hydrogen combine with molecules of oxygen to produce water molecules. The number of hydrogen atoms is the same before and after the reaction; the same is true of the oxygen atoms.     In terms of , , and , how many hydrogen atoms are there before the reaction? How many hydrogen atoms are there after the reaction? Find a linear equation in , , and by equating these quantities.   Find a second linear equation in , , and by equating the number of oxygen atoms before and after the reaction.   Find the solutions of this linear system. Why are there infinitely many solutions?    In this chemical setting, , , and should be positive integers. Find the solution where , , and are the smallest possible positive integers.      Now consider the reaction where potassium permanganate and manganese sulfate combine with water to produce manganese dioxide, potassium sulfate, and sulfuric acid: As in the previous exercise, find the appropriate values for to balance the chemical reaction.          We have the reaction     We have the reaction          We will equate the number of hydrogen and oxygen atoms before and after the reaction.   Before the reaction, there are two hydrogen atoms for every hydrogen molecule. Since there are hydrogen molecules, we have hydrogen atoms before the reaction.  After the reaction, there are two hydrogen atoms for every water molecule. Since there are hydrogen molecules, we have hydrogen atoms after the reaction. This gives the equation .    Similarly, there are oxygen atoms before the reaction and oxygen atoms after. This gives the equation .    We have the linear system When we create the augmented matrix and find its reduced row echelon form, we see that is a free variable so that We need all the quantities to be integers, and the smallest value of that makes an integer is . Therefore, we have , , and . The reaction is then        Proceeding as in the previous part, we have Solving this linear system, we find Once again, we choose to be the smallest positive integer that causes all of the variables to be integers. This means that so that we have This gives the reaction:      "
},
{
  "id": "exercise-12",
  "level": "2",
  "url": "sec-sage-introduction.html#exercise-12",
  "type": "Exercise",
  "number": "1.3.5.6",
  "title": "",
  "body": " We began this section by stating that increasing computational power has helped linear algebra assume a prominent role as a scientific tool. Later, we looked at one computational limitation: once a matrix gets to be too big, it is not reasonable to apply Gaussian elimination to find its reduced row echelon form.  In this exercise, we will see another limitation: computer arithmetic with real numbers is only an approximation because computers represent real numbers with only a finite number of bits. For instance, the number pi would be approximated inside a computer by, say, Most of the time, this is not a problem. However, when we perform millions or even billions of arithmetic operations, the error in these approximations starts to accumulate and can lead to results that are wildly inaccurate. Here are two examples demonstrating this.   Let's first see an example showing that computer arithmetic really is an approximation. First, consider the linear system If the coefficients are entered into Sage as fractions, Sage will find the exact reduced row echelon form. Find the exact solution to this linear system.   Now let's ask Sage to compute with real numbers. We can do this by representing one of the coefficients as a decimal. For instance, the same linear system can be represented as Most computers do arithmetic using either 32 or 64 bits. To magnify the problem so that we can see it better, we will ask Sage to do arithmetic using only 10 bits as follows. What does Sage give for the solution now? Compare this to the exact solution that you found previously.   Some types of linear systems are particularly sensitive to errors resulting from computers' approximate arithmetic. For instance, suppose we are interested in the linear system Find the solution to this linear system.   Suppose now that the computer has accumulated some error in one of the entries of this system so that it incorrectly stores the system as Find the solution to this linear system.   Notice how a small error in one of the entries in the linear system leads to a solution that has a dramatically large error. Fortunately, this is an issue that has been well studied, and there are techniques that mitigate this type of behavior.         The exact solution is while the approximate solution is     This solution to the first system is and . However, with just a small change in the system, we find the solution and .         The exact solution is However, if we compute with only 10 bits, we find the approximate solution     This solution to the first system is and . However, with just a small change in the system, we find the solution and .     "
},
{
  "id": "sec-pivots",
  "level": "1",
  "url": "sec-pivots.html",
  "type": "Section",
  "number": "1.4",
  "title": "Pivots and their influence on solution spaces",
  "body": " Pivots and their influence on solution spaces   By now, we have seen several examples illustrating how the reduced row echelon matrix leads to a convenient description of the solution space to a linear system. In this section, we will use this understanding to make some general observations about how certain features of the reduced row echelon matrix reflect the nature of the solution space.  Remember that a leading entry in a reduced row echelon matrix is the leftmost nonzero entry in a row of the matrix. As we'll see, the positions of these leading entries encode a lot of information about the solution space of the corresponding linear system. For this reason, we make the following definition.   pivot position  A pivot position in a matrix is the position of a leading entry in the reduced row echelon matrix of .   For instance, in this reduced row echelon matrix, the pivot positions are indicated in bold: We can refer to pivot positions by their row and column number saying, for instance, that there is a pivot position in the second row and fourth column.   Some basic observations about pivots      Shown below is a matrix and its reduced row echelon form. Indicate the pivot positions. .   How many pivot positions can there be in one row? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   How many pivots can there be in one column? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   Give an example of a matrix with a pivot position in every row and every column. What is special about such a matrix?        The pivot positions are indicated below   A row contains at most one pivot position. Therefore, a matrix, which has three rows, contains at most three pivot positions. Here is an example:   A column contains at most one pivot position. Therefore, a matrix, which has three columns, contains at most three pivot positions. Here is an example   A matrix with a pivot position in every row and every column would have the following reduced row echelon form: Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.        The pivot positions are indicated below   Three.  Three.  Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.      When we have looked at solution spaces of linear systems, we have frequently asked whether there are infinitely many solutions, exactly one solution, or no solutions. We will now break this question into two separate questions.   Two Fundamental Questions  When we encounter a linear system, we often ask   Existence  Is there a solution to the linear system? If so, we say that the system is consistent ; if not, we say it is inconsistent . consistent system  inconsistent system     Uniqueness  If the linear system is consistent, is the solution unique or are there infinitely many solutions?      These two questions represent two sides of a coin that appear in many variations throughout our explorations. In this section, we will study how the location of the pivots influence the answers to these two questions. We begin by considering the first question concerning the existence of solutions.    The existence of solutions       Shown below are three augmented matrices in reduced row echelon form.             For each matrix, identify the pivot positions and determine if the corresponding linear system is consistent. Explain how the location of the pivots determines whether the system is consistent or inconsistent.    Each of the augmented matrices above has a row in which each entry is zero. What, if anything, does the presence of such a row tell us about the consistency of the corresponding linear system?   Give an example of a augmented matrix in reduced row echelon form that represents a consistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee a consistent system.   Give an example of a augmented matrix in reduced row echelon form that represents an inconsistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee an inconsistent system.   Write the reduced row echelon form of the coefficient matrix of the corresponding linear system in ? (Remember that the Augmentation Principle says that the reduced row echelon form of the coefficient matrix simply consists of the first four columns of the augmented matrix.) What do you notice about the pivot positions in this coefficient matrix?   Suppose we have a linear system for which the coefficient matrix has the following reduced row echelon form. What can you say about the consistency of the linear system?        The pivot positions are indicated below.           The first two augmented matrices correspond to consistent linear systems. The third does not, however, since the third row corresponds to the equation .  In general, a linear system is inconsistent exactly when there is a pivot position in the rightmost column of the augmented matrix.  A row in which every entry is zero corresponds to the equation , which is always true. Such an equation has no bearing on the consistency of the linear system.   This corresponds to a consistent system because there is not a pivot in the rightmost column.   This is an inconsistent system because the third row corresponds to the equation , which is never satisfied.   In the coefficient matrix, there is a row without a pivot position so that each entry is . This allows a pivot position to appear in the rightmost column of the augmented matrix.  This linear system must be consistent because the augmented matrix cannot have a pivot position in the rightmost column.      Let's summarize the results of this activity by considering the following reduced row echelon matrix: . In terms of variables , , and , the final equation says . If we evaluate the left-hand side with any values of , , and , we get 0, which means that the equation always holds. Therefore, its presence has no effect on the solution space defined by the other three equations.  The third equation, however, says that . Again, if we evaluate the left-hand side with any values of , , and , we get 0 so this equation cannot be satisfied for any . This means that the entire linear system has no solution and is therefore inconsistent.  An equation like this appears in the reduced row echelon matrix as . The pivot positions make this condition clear: the system is inconsistent if there is a pivot position in the rightmost column of the corresponding augmented matrix.   In fact, we will soon see that the system is consistent if there is not a pivot in the rightmost column of the corresponding augmented matrix. This leaves us with the following    A linear system is inconsistent if and only if there is a pivot position in the rightmost column of the corresponding augmented matrix.    This also says something about the pivot positions of the coefficient matrix. Consider an example of an inconsistent system corresponding to the reduced row echelon form of the following augmented matrix . The Augmentation Principle says that that the reduced row echelon form of the coefficient matrix is which shows that the coefficient matrix has a row without a pivot position. To turn this around, we see that if every row of the coefficient matrix has a pivot position, then the system must be consistent. For instance, if our linear system has a coefficient matrix whose reduced row echelon form is , then we can guarantee that the linear system is consistent because there is no way to obtain a pivot in the rightmost column of the augmented matrix.   If every row of the coefficient matrix has a pivot position, then the corresponding system of linear equations is consistent.     The uniqueness of solutions  Now that we have studied the role that pivot positions play in the existence of solutions, let's turn to the question of uniqueness.     Here are the three augmented matrices in reduced row echelon form that we considered in the previous section.             For each matrix, identify the pivot positions and state whether the corresponding linear system is consistent. If the system is consistent, explain whether the solution is unique or whether there are infinitely many solutions.   If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.  If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.   What condition on the pivot positions guarantees that a linear system has a unique solution?   If a linear system has a unique solution, what can we say about the relationship between the number of equations and the number of variables?        The pivot positions are indicated below.           The third linear system is inconsistent. The first system is consistent and has exactly one solution because , , and . The second system is consistent and has infinitely many solutions since we can write the equations as   This is not possible as we see by considering the shape of a typical matrix: In this case, the variable is free meaning there are infinitely many solutions.  This is possible as the following matrix illustrates:   If every column of the coefficient matrix has a pivot position, we can guarantee that the solution is unique.  If the coefficient matrix has a pivot position in every column, there must be at least as many rows as columns. Therefore, the number of equations must be less than or equal to the number of variables.      Let's consider what we've learned in this activity. Since we are interested in the question of whether a consistent linear system has a unique solution or infinitely many, we will only consider consistent systems. By the results of the previous section, this means that there is not a pivot in the rightmost column of the augmented matrix. Here are two possible examples:          In the first example, we have the equations demonstrating the fact that there is a unique solution .  In the second example, we have the equations that we may rewrite in parametric form as . Here we see that and are basic variables that may be expressed in terms of the free variable . In this case, the presence of the free variable leads to infinitely many solutions.  Remember that every column of the coefficient matrix corresponds to a variable in our linear system. In the first example, we see that every column of the coefficient contains a pivot position, which means that every variable is uniquely determined. In the second example, the column of the coefficient matrix corresponding to does not contain a pivot position, which results in appearing as a free variable. This illustrates the following principle.    Suppose that we consider a consistent linear system.  If every column of the coefficient matrix contains a pivot position, then the system has a unique solution.  If there is a column in the coefficient matrix that contains no pivot position, then the system has infinitely many solutions.  Columns that contain a pivot position correspond to basic variables while columns that do not correspond to free variables.      When a linear system has a unique solution, every column of the coefficient matrix has a pivot position. Since every row contains at most one pivot position, there must be at least as many rows as columns in the coefficient matrix. Therefore, the linear system has at least as many equations as variables, which is something we intuitively suspected in .  It is reasonable to ask how we choose the free variables. For instance, if we have a single equation , then we may write or, equivalently, . Clearly, either variable may be considered as a free variable in this case.  As we'll see in the future, we are more interested in the number of free variables rather than in their choice. For convenience, we will adopt the convention that free variables correspond to columns without a pivot position, which allows us to quickly identify them. For example, the variables and appear as free variables in the following linear system: .    Summary  We have seen how the locations of pivot positions, in both the augmented and coefficient matrices, give vital information about the existence and uniqueness of solutions to linear systems. More specifically,     A linear system is inconsistent exactly when a pivot position appears in the rightmost column of the augmented matrix.   If a linear system is consistent, the solution is unique when every column of the coefficient matrix contains a pivot position. There are infinitely many solutions when there is a column of the coefficient matrix without a pivot position.   If a linear system is consistent, the columns of the coefficient matrix containing pivot positions correspond to basic variables and the columns without pivot positions correspond to free variables.       For each of the augmented matrices in reduced row echelon form given below, determine whether the corresponding linear system is consistent and, if so, determine whether the solution is unique. If the system is consistent, identify the free variables and the basic variables and give a description of the solution space in parametric form.    .    .    .    .       The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent, and there is a unique solution.       The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent with , , and all being basic variables. There is a unique solution.      For each of the following linear systems, determine whether the system is consistent, and, if so, determine whether there are infinitely many solutions.                        The linear system is consistent with a unique solution.    The linear system is consistent with infinitely many solutions.    The linear system is inconsistent.         The corresponding augmented matrix is so this linear system is consistent with a unique solution.    The corresponding augmented matrix is so this linear system is consistent with infinitely many solutions.    The corresponding augmented matrix is so this linear system is inconsistent.       Include an example of an appropriate matrix as you justify your responses to the following questions.  Suppose a linear system having six equations and three variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely many solutions?  Suppose that a linear system having three equations and six variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely many solutions?  Suppose that a linear system is consistent and has a unique solution. What can you guarantee about the pivot positions in the augmented matrix?       We cannot guarantee either possibility.  We can guarantee there are infinitely many solutions.  There is a pivot position in each column of the coefficient matrix.       Consider the augmented matrices These examples show that anything is possible: the system may or may not be consistent, and, if it is consistent, there may or may not be a unique solution.  Consider the augmented matrix There must be a column without a pivot position so we can guarantee that there are infinitely many solutions.  Consider the augmented matrix We know that every column of the coefficient matrix has a pivot position and that the rightmost column does not. There must be at least as many equations as variables.      Determine whether the following statements are true or false and provide a justification for your response.  If the coefficient matrix of a linear system has a pivot in the rightmost column, then the system is inconsistent.  If a linear system has two equations and four variables, then it must be consistent.  If a linear system having four equations and three variables is consistent, then the solution is unique.  Suppose that a linear system has four equations and four variables and that the coefficient matrix has four pivots. Then the linear system is consistent and has a unique solution.  Suppose that a linear system has five equations and three variables and that the coefficient matrix has a pivot position in every column. Then the linear system is consistent and has a unique solution.       False.  False.  False.  True.  False.       This statement is false. If the augmented matrix has a pivot in the rightmost column, then the system is inconsistent.  This statement is false as illustrated by the matrix   This statement is false as illustrated by the matrix   This statement is true as illustrated by the matrix   This statement is false as illustrated by the matrix       We began our explorations in by noticing that the solution spaces of linear systems with more equations seem to be smaller. Let's reexamine this idea using what we know about pivot positions.   Remember that the solution space of a single linear equation in three variables is a plane. Can two planes ever intersect in a single point? What are the possible ways in which two planes can intersect? How can our understanding of pivot positions help answer these questions?    Suppose that a consistent linear system has more variables than equations. By considering the possible pivot positions, what can you say with certainty about the solution space?    If a linear system has many more equations than variables, why is it reasonable to expect the system to be inconsistent?         The planes will most likely intersect either in a line or not at all.    There must be infinitely many solutions.    There will most likely be a pivot position in the rightmost column of the augmented matrix.         The intersection of two planes is described by a linear system having two equations and three variables. Two possible reduced row echelon forms are indicating that the planes either intersect in a line or do not intersect at all. In particular, it is not possible for there to be a single point of intersection.    If there are more variables than equations, there must be a column in the coefficient matrix that does not contain a pivot position: so we can guarantee that there are infinitely many solutions.    If there are many more equations than variables, the coefficient matrix will have many rows without a pivot position: This makes it likely that there will be a pivot position in the rightmost column of the augmented matrix, which means that the linear system is inconsistent.       The following linear systems contain either one or two parameters.  For what values of the parameter is the following system consistent? For which of those values is the solution unique? .  For what values of the parameters and l is the following system consistent? For which of those values is the solution unique? .      The system is consistent when , and the solution can never be unique. If , the system is inconsistent.  The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .       We have This shows that the system is consistent when , and the solution can never be unique. If , the system is inconsistent.  We have The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .      Consider the linear system described by the following augmented matrix. .  Find a choice for the parameters , , and that causes the linear system to be inconsistent. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have a unique solution. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have infinitely many solutions. Explain why your choice has this property.     Some example choices are given here, but there are many more.   .   and .   , , and .     We have   If and , then the system will be inconsistent. We can choose .  We want there to be a pivot position in every column so we choose . For instance , , and works.  We need for the third and fourth columns to lack pivot positions so we choose and . For instance, , , and works.      A linear system where the right hand side of every equation is 0 is called homogeneous . The augmented matrix of a homogeneous system, for instance, has the following form: .   Using the concepts we've seen in this section, explain why a homogeneous linear system must be consistent.    What values for the variables are guaranteed to give a solution? Use this to offer another explanation for why a homogeneous linear system is consistent.   Suppose that a homogeneous linear system has a unique solution.   Give an example of such a system by writing its augmented matrix in reduced row echelon form.   Write just the coefficient matrix for the example you gave in the previous part. What can you say about the pivot positions in the coefficient matrix? Explain why your observation must hold for any homogeneous system having a unique solution.   If a homogeneous system of equations has a unique solution, what can you say about the number of equations compared to the number of variables?         There cannot be a pivot in the rightmost column.  There is a solution when all the variables are set to zero.  There are at least as many equations as variables.       Since all the entries in the rightmost column are zero, there cannot be a pivot in the rightmost column.  We have a solution when all the variables are zero. Therefore, the system must be consistent.  We must have a pivot position in every column of the coefficient matrix so the augmented matrix could look like Since every column of the coefficient matrix has a pivot position, there must be at least as many rows as columns. This means that there must be at least as many equations as variables.      In a previous math class, you have probably seen the fact that, if we are given two points in the plane, then there is a unique line passing through both of them. In this problem, we will begin with the four points on the left below and ask to find a polynomial that passes through these four points as shown on the right.      A degree three polynomial can be written as where , , , and are coefficients that we would like to determine. Since we want the polynomial to pass through the point , we should require that . In this way, we obtain a linear equation for the coefficients , , , and .   Write the four linear equations for the coefficients obtained by requiring that the graph of the polynomial passes through the four points above.   Write the augmented matrix corresponding to this system of equations and use the Sage cell below to solve for the coefficients.    Write the polynomial that you found and check your work by graphing it in the Sage cell below and verifying that it passes through the four points. To plot a function over a range, you may use a command like plot(1 + x- 2*x^2, xmin = -1, xmax = 4) .    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree two, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree two polynomial passing through these four points?    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree four, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree four polynomial passing through these four points?    Suppose you had 10 points and you wanted to find a polynomial passing through each of them. What should the degree of the polynomial be to guarantee that there is exactly one such polynomial? Explain your response.       The four equations we find are   The coefficients are , , , and .  The polynomial is   The linear system is inconsistent.  The linear system is consistent with infinitely many solutions.  Nine.       The four equations we find are   This leads to the augmented matrix This gives the coefficients , , , and .  The polynomial passes through the four given points.  We have the augmented matrix This system is therefore inconsistent, which means there is no degree 2 polynomial passing through the four points.  The augmented matrix we find is This shows that there are infinitely many polynomials whose degree is four and that pass through the four points.  If there are 10 points, we should find a polynomial having 10 coefficients. This means that the degree of the polynomial should be 9.      "
},
{
  "id": "definition-5",
  "level": "2",
  "url": "sec-pivots.html#definition-5",
  "type": "Definition",
  "number": "1.4.1",
  "title": "",
  "body": " pivot position  A pivot position in a matrix is the position of a leading entry in the reduced row echelon matrix of .  "
},
{
  "id": "exploration-2",
  "level": "2",
  "url": "sec-pivots.html#exploration-2",
  "type": "Preview Activity",
  "number": "1.4.1",
  "title": "Some basic observations about pivots.",
  "body": " Some basic observations about pivots      Shown below is a matrix and its reduced row echelon form. Indicate the pivot positions. .   How many pivot positions can there be in one row? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   How many pivots can there be in one column? In a matrix, what is the largest possible number of pivot positions? Give an example of a matrix that has the largest possible number of pivot positions.   Give an example of a matrix with a pivot position in every row and every column. What is special about such a matrix?        The pivot positions are indicated below   A row contains at most one pivot position. Therefore, a matrix, which has three rows, contains at most three pivot positions. Here is an example:   A column contains at most one pivot position. Therefore, a matrix, which has three columns, contains at most three pivot positions. Here is an example   A matrix with a pivot position in every row and every column would have the following reduced row echelon form: Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.        The pivot positions are indicated below   Three.  Three.  Such a matrix must necessarily have the same number of rows and columns, which means it is what we call a square matrix.     "
},
{
  "id": "fundamental-questions",
  "level": "2",
  "url": "sec-pivots.html#fundamental-questions",
  "type": "Question",
  "number": "1.4.2",
  "title": "Two Fundamental Questions.",
  "body": " Two Fundamental Questions  When we encounter a linear system, we often ask   Existence  Is there a solution to the linear system? If so, we say that the system is consistent ; if not, we say it is inconsistent . consistent system  inconsistent system     Uniqueness  If the linear system is consistent, is the solution unique or are there infinitely many solutions?     "
},
{
  "id": "activity-9",
  "level": "2",
  "url": "sec-pivots.html#activity-9",
  "type": "Activity",
  "number": "1.4.2",
  "title": "",
  "body": "     Shown below are three augmented matrices in reduced row echelon form.             For each matrix, identify the pivot positions and determine if the corresponding linear system is consistent. Explain how the location of the pivots determines whether the system is consistent or inconsistent.    Each of the augmented matrices above has a row in which each entry is zero. What, if anything, does the presence of such a row tell us about the consistency of the corresponding linear system?   Give an example of a augmented matrix in reduced row echelon form that represents a consistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee a consistent system.   Give an example of a augmented matrix in reduced row echelon form that represents an inconsistent system. Indicate the pivot positions in your matrix and explain why these pivot positions guarantee an inconsistent system.   Write the reduced row echelon form of the coefficient matrix of the corresponding linear system in ? (Remember that the Augmentation Principle says that the reduced row echelon form of the coefficient matrix simply consists of the first four columns of the augmented matrix.) What do you notice about the pivot positions in this coefficient matrix?   Suppose we have a linear system for which the coefficient matrix has the following reduced row echelon form. What can you say about the consistency of the linear system?        The pivot positions are indicated below.           The first two augmented matrices correspond to consistent linear systems. The third does not, however, since the third row corresponds to the equation .  In general, a linear system is inconsistent exactly when there is a pivot position in the rightmost column of the augmented matrix.  A row in which every entry is zero corresponds to the equation , which is always true. Such an equation has no bearing on the consistency of the linear system.   This corresponds to a consistent system because there is not a pivot in the rightmost column.   This is an inconsistent system because the third row corresponds to the equation , which is never satisfied.   In the coefficient matrix, there is a row without a pivot position so that each entry is . This allows a pivot position to appear in the rightmost column of the augmented matrix.  This linear system must be consistent because the augmented matrix cannot have a pivot position in the rightmost column.     "
},
{
  "id": "thm-pivot-inconsistency",
  "level": "2",
  "url": "sec-pivots.html#thm-pivot-inconsistency",
  "type": "Proposition",
  "number": "1.4.3",
  "title": "",
  "body": "  A linear system is inconsistent if and only if there is a pivot position in the rightmost column of the corresponding augmented matrix.   "
},
{
  "id": "proposition-3",
  "level": "2",
  "url": "sec-pivots.html#proposition-3",
  "type": "Proposition",
  "number": "1.4.4",
  "title": "",
  "body": " If every row of the coefficient matrix has a pivot position, then the corresponding system of linear equations is consistent.  "
},
{
  "id": "activity-10",
  "level": "2",
  "url": "sec-pivots.html#activity-10",
  "type": "Activity",
  "number": "1.4.3",
  "title": "",
  "body": "   Here are the three augmented matrices in reduced row echelon form that we considered in the previous section.             For each matrix, identify the pivot positions and state whether the corresponding linear system is consistent. If the system is consistent, explain whether the solution is unique or whether there are infinitely many solutions.   If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.  If possible, give an example of a augmented matrix that corresponds to a linear system having a unique solution. If it is not possible, explain why.   What condition on the pivot positions guarantees that a linear system has a unique solution?   If a linear system has a unique solution, what can we say about the relationship between the number of equations and the number of variables?        The pivot positions are indicated below.           The third linear system is inconsistent. The first system is consistent and has exactly one solution because , , and . The second system is consistent and has infinitely many solutions since we can write the equations as   This is not possible as we see by considering the shape of a typical matrix: In this case, the variable is free meaning there are infinitely many solutions.  This is possible as the following matrix illustrates:   If every column of the coefficient matrix has a pivot position, we can guarantee that the solution is unique.  If the coefficient matrix has a pivot position in every column, there must be at least as many rows as columns. Therefore, the number of equations must be less than or equal to the number of variables.     "
},
{
  "id": "principle-1",
  "level": "2",
  "url": "sec-pivots.html#principle-1",
  "type": "Principle",
  "number": "1.4.5",
  "title": "",
  "body": "  Suppose that we consider a consistent linear system.  If every column of the coefficient matrix contains a pivot position, then the system has a unique solution.  If there is a column in the coefficient matrix that contains no pivot position, then the system has infinitely many solutions.  Columns that contain a pivot position correspond to basic variables while columns that do not correspond to free variables.     "
},
{
  "id": "exercise-13",
  "level": "2",
  "url": "sec-pivots.html#exercise-13",
  "type": "Exercise",
  "number": "1.4.4.1",
  "title": "",
  "body": " For each of the augmented matrices in reduced row echelon form given below, determine whether the corresponding linear system is consistent and, if so, determine whether the solution is unique. If the system is consistent, identify the free variables and the basic variables and give a description of the solution space in parametric form.    .    .    .    .       The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column, and there are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent, and there is a unique solution.       The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is consistent because there is not a pivot position in the rightmost column. The variable is free while , , and are basic. There are infinitely many solutions.  The linear system is inconsistent because there is a pivot position in the rightmost column.  The linear system is consistent with , , and all being basic variables. There is a unique solution.    "
},
{
  "id": "exercise-14",
  "level": "2",
  "url": "sec-pivots.html#exercise-14",
  "type": "Exercise",
  "number": "1.4.4.2",
  "title": "",
  "body": " For each of the following linear systems, determine whether the system is consistent, and, if so, determine whether there are infinitely many solutions.                        The linear system is consistent with a unique solution.    The linear system is consistent with infinitely many solutions.    The linear system is inconsistent.         The corresponding augmented matrix is so this linear system is consistent with a unique solution.    The corresponding augmented matrix is so this linear system is consistent with infinitely many solutions.    The corresponding augmented matrix is so this linear system is inconsistent.     "
},
{
  "id": "exercise-15",
  "level": "2",
  "url": "sec-pivots.html#exercise-15",
  "type": "Exercise",
  "number": "1.4.4.3",
  "title": "",
  "body": " Include an example of an appropriate matrix as you justify your responses to the following questions.  Suppose a linear system having six equations and three variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely many solutions?  Suppose that a linear system having three equations and six variables is consistent. Can you guarantee that the solution is unique? Can you guarantee that there are infinitely many solutions?  Suppose that a linear system is consistent and has a unique solution. What can you guarantee about the pivot positions in the augmented matrix?       We cannot guarantee either possibility.  We can guarantee there are infinitely many solutions.  There is a pivot position in each column of the coefficient matrix.       Consider the augmented matrices These examples show that anything is possible: the system may or may not be consistent, and, if it is consistent, there may or may not be a unique solution.  Consider the augmented matrix There must be a column without a pivot position so we can guarantee that there are infinitely many solutions.  Consider the augmented matrix We know that every column of the coefficient matrix has a pivot position and that the rightmost column does not. There must be at least as many equations as variables.    "
},
{
  "id": "exercise-16",
  "level": "2",
  "url": "sec-pivots.html#exercise-16",
  "type": "Exercise",
  "number": "1.4.4.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If the coefficient matrix of a linear system has a pivot in the rightmost column, then the system is inconsistent.  If a linear system has two equations and four variables, then it must be consistent.  If a linear system having four equations and three variables is consistent, then the solution is unique.  Suppose that a linear system has four equations and four variables and that the coefficient matrix has four pivots. Then the linear system is consistent and has a unique solution.  Suppose that a linear system has five equations and three variables and that the coefficient matrix has a pivot position in every column. Then the linear system is consistent and has a unique solution.       False.  False.  False.  True.  False.       This statement is false. If the augmented matrix has a pivot in the rightmost column, then the system is inconsistent.  This statement is false as illustrated by the matrix   This statement is false as illustrated by the matrix   This statement is true as illustrated by the matrix   This statement is false as illustrated by the matrix     "
},
{
  "id": "exercise-17",
  "level": "2",
  "url": "sec-pivots.html#exercise-17",
  "type": "Exercise",
  "number": "1.4.4.5",
  "title": "",
  "body": " We began our explorations in by noticing that the solution spaces of linear systems with more equations seem to be smaller. Let's reexamine this idea using what we know about pivot positions.   Remember that the solution space of a single linear equation in three variables is a plane. Can two planes ever intersect in a single point? What are the possible ways in which two planes can intersect? How can our understanding of pivot positions help answer these questions?    Suppose that a consistent linear system has more variables than equations. By considering the possible pivot positions, what can you say with certainty about the solution space?    If a linear system has many more equations than variables, why is it reasonable to expect the system to be inconsistent?         The planes will most likely intersect either in a line or not at all.    There must be infinitely many solutions.    There will most likely be a pivot position in the rightmost column of the augmented matrix.         The intersection of two planes is described by a linear system having two equations and three variables. Two possible reduced row echelon forms are indicating that the planes either intersect in a line or do not intersect at all. In particular, it is not possible for there to be a single point of intersection.    If there are more variables than equations, there must be a column in the coefficient matrix that does not contain a pivot position: so we can guarantee that there are infinitely many solutions.    If there are many more equations than variables, the coefficient matrix will have many rows without a pivot position: This makes it likely that there will be a pivot position in the rightmost column of the augmented matrix, which means that the linear system is inconsistent.     "
},
{
  "id": "exercise-18",
  "level": "2",
  "url": "sec-pivots.html#exercise-18",
  "type": "Exercise",
  "number": "1.4.4.6",
  "title": "",
  "body": " The following linear systems contain either one or two parameters.  For what values of the parameter is the following system consistent? For which of those values is the solution unique? .  For what values of the parameters and l is the following system consistent? For which of those values is the solution unique? .      The system is consistent when , and the solution can never be unique. If , the system is inconsistent.  The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .       We have This shows that the system is consistent when , and the solution can never be unique. If , the system is inconsistent.  We have The system is inconsistent if and . There are infinitely many solutions if and . There is a unique solution if .    "
},
{
  "id": "exercise-19",
  "level": "2",
  "url": "sec-pivots.html#exercise-19",
  "type": "Exercise",
  "number": "1.4.4.7",
  "title": "",
  "body": " Consider the linear system described by the following augmented matrix. .  Find a choice for the parameters , , and that causes the linear system to be inconsistent. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have a unique solution. Explain why your choice has this property.  Find a choice for the parameters , , and that causes the linear system to have infinitely many solutions. Explain why your choice has this property.     Some example choices are given here, but there are many more.   .   and .   , , and .     We have   If and , then the system will be inconsistent. We can choose .  We want there to be a pivot position in every column so we choose . For instance , , and works.  We need for the third and fourth columns to lack pivot positions so we choose and . For instance, , , and works.    "
},
{
  "id": "exercise-20",
  "level": "2",
  "url": "sec-pivots.html#exercise-20",
  "type": "Exercise",
  "number": "1.4.4.8",
  "title": "",
  "body": " A linear system where the right hand side of every equation is 0 is called homogeneous . The augmented matrix of a homogeneous system, for instance, has the following form: .   Using the concepts we've seen in this section, explain why a homogeneous linear system must be consistent.    What values for the variables are guaranteed to give a solution? Use this to offer another explanation for why a homogeneous linear system is consistent.   Suppose that a homogeneous linear system has a unique solution.   Give an example of such a system by writing its augmented matrix in reduced row echelon form.   Write just the coefficient matrix for the example you gave in the previous part. What can you say about the pivot positions in the coefficient matrix? Explain why your observation must hold for any homogeneous system having a unique solution.   If a homogeneous system of equations has a unique solution, what can you say about the number of equations compared to the number of variables?         There cannot be a pivot in the rightmost column.  There is a solution when all the variables are set to zero.  There are at least as many equations as variables.       Since all the entries in the rightmost column are zero, there cannot be a pivot in the rightmost column.  We have a solution when all the variables are zero. Therefore, the system must be consistent.  We must have a pivot position in every column of the coefficient matrix so the augmented matrix could look like Since every column of the coefficient matrix has a pivot position, there must be at least as many rows as columns. This means that there must be at least as many equations as variables.    "
},
{
  "id": "exercise-poly-fit",
  "level": "2",
  "url": "sec-pivots.html#exercise-poly-fit",
  "type": "Exercise",
  "number": "1.4.4.9",
  "title": "",
  "body": " In a previous math class, you have probably seen the fact that, if we are given two points in the plane, then there is a unique line passing through both of them. In this problem, we will begin with the four points on the left below and ask to find a polynomial that passes through these four points as shown on the right.      A degree three polynomial can be written as where , , , and are coefficients that we would like to determine. Since we want the polynomial to pass through the point , we should require that . In this way, we obtain a linear equation for the coefficients , , , and .   Write the four linear equations for the coefficients obtained by requiring that the graph of the polynomial passes through the four points above.   Write the augmented matrix corresponding to this system of equations and use the Sage cell below to solve for the coefficients.    Write the polynomial that you found and check your work by graphing it in the Sage cell below and verifying that it passes through the four points. To plot a function over a range, you may use a command like plot(1 + x- 2*x^2, xmin = -1, xmax = 4) .    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree two, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree two polynomial passing through these four points?    Rather than looking for a degree three polynomial, suppose we wanted to find a polynomial that passes through the four points and that has degree four, such as . Solve the linear system for the coefficients. What can you say about the existence and uniqueness of a degree four polynomial passing through these four points?    Suppose you had 10 points and you wanted to find a polynomial passing through each of them. What should the degree of the polynomial be to guarantee that there is exactly one such polynomial? Explain your response.       The four equations we find are   The coefficients are , , , and .  The polynomial is   The linear system is inconsistent.  The linear system is consistent with infinitely many solutions.  Nine.       The four equations we find are   This leads to the augmented matrix This gives the coefficients , , , and .  The polynomial passes through the four given points.  We have the augmented matrix This system is therefore inconsistent, which means there is no degree 2 polynomial passing through the four points.  The augmented matrix we find is This shows that there are infinitely many polynomials whose degree is four and that pass through the four points.  If there are 10 points, we should find a polynomial having 10 coefficients. This means that the degree of the polynomial should be 9.    "
},
{
  "id": "sec-vectors-lin-combs",
  "level": "1",
  "url": "sec-vectors-lin-combs.html",
  "type": "Section",
  "number": "2.1",
  "title": "Vectors and linear combinations",
  "body": " Vectors and linear combinations   It is a remarkable fact that algebra, which is about symbolic equations and their solutions, and geometry are intimately connected. For instance, the solution set of a linear equation in two unknowns, such as , can be represented graphically by a straight line. The aim of this section is to further this connection by introducing vectors, which will help us to apply geometric intuition to our thinking about linear systems.    Vectors   vector A vector is most simply thought of as a matrix with a single column. For instance, and are both vectors. The entries in a vector are called its components. Since the vector has two components, we say that it is a two-dimensional vector; in the same way, the vector is a four-dimensional vector.  We denote the set of all -dimensional vectors by . Consequently, if is a 3-dimensional vector, we say that is in .  While it can be difficult to visualize a four-dimensional vector, we can draw a simple picture describing the two-dimensional vector , as shown in .  A graphical representation of the vector .       We can think of as describing a walk in the plane where we move two units horizontally and one unit vertically. Though we allow ourselves to begin walking from any point in the plane, we will most frequently begin at the origin in which case we arrive at the the point , as shown in the figure.  There are two simple algebraic operations we often perform on vectors.  Scalar Multiplication  scalar multiplication  We multiply a vector by a real number by multiplying each of the components of by . For instance, We will frequently refer to real numbers, such as -3 in this example, as scalars to distinguish them from vectors.   Vector Addition  vector addition  We add two vectors of the same dimension by adding their components. For instance,       Scalar Multiplication and Vector Addition   Suppose that      Find expressions for the vectors and sketch them using .  Sketch the vectors on this grid.         What geometric effect does scalar multiplication have on a vector? Also, describe the effect that multiplying by a negative scalar has.   Sketch the vectors using .  Sketch the vectors on this grid.         Consider vectors that have the form where is any scalar. Sketch a few of these vectors when, say, and . Give a geometric description of this set of vectors.  Sketch the vectors on this grid.        If and are two scalars, then the vector is called a linear combination of the vectors and . Find the vector that is the linear combination when and .    Can the vector be represented as a linear combination of and ? Asked differently, can we find scalars and such that .      Solutions to this preview activity are given in the text below.    The preview activity demonstrates how we may interpret scalar multiplication and vector addition geometrically.  First, we see that scalar multiplication has the effect of stretching or compressing a vector. Multiplying by a negative scalar changes the direction of the vector. In either case, shows that a scalar multiple of a vector lies on the same line defined by .     Scalar multiples of the vector .    To represent the sum , we imagine walking from the origin with the appropriate horizontal and vertical changes given by . From there, we continue our walk using the horizontal and vertical changes prescribed by , after which we arrive at the sum . This is illustrated on the left of where the tail of is placed on the tip of .      Vector addition as a simple walk in the plane is illustrated on the left. The vector sum is represented as the diagonal of a parallelogram on the right.    Alternatively, we may construct the parallelogram with and as two sides. The sum is then the diagonal of the parallelogram, as illustrated on the right of .  We have now seen that the set of vectors having the form is a line. To form the set of vectors , we can begin with the vector and add multiples of . Geometrically, this means that we begin from the tip of and move in a direction parallel to . The effect is to translate the line by the vector , as shown in .     The set of vectors form a line.    At times, it will be useful for us to think of vectors and points interchangeably. That is, we may wish to think of the vector as describing the point and vice-versa. When we say that the vectors having the form form a line, we really mean that the tips of the vectors all lie on the line passing through and parallel to .   Even though these vector operations are new, it is straightforward to check that some familiar properties hold.   Commutativity   .   Distributivity   .     Sage can perform scalar multiplication and vector addition. We define a vector using the vector command; then * and + denote scalar multiplication and vector addition.     Linear combinations  Linear combinations, which we encountered in the preview activity, provide the link between vectors and linear systems. In particular, they will help us apply geometric intuition to problems involving linear systems.   linear combination  weights  The linear combination of the vectors with scalars is the vector The scalars are called the weights of the linear combination.     In this activity, we will look at linear combinations of a pair of vectors, and .   Linear combinations of vectors and .      The weight is initially set to 0. Explain what happens as you vary while keeping . How is this related to scalar multiplication?   What is the linear combination of and when and ? You may find this result using the diagram, but you should also verify it by computing the linear combination.   Describe the vectors that arise when the weight is set to 1 and is varied. How is this related to our investigations in the preview activity?   Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?   Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?   Verify the result from the previous part by algebraically finding the weights and that form the linear combination .   Can the vector be expressed as a linear combination of and ? What about the vector ?   Are there any two-dimensional vectors that cannot be expressed as linear combinations of and ?       When we vary with , the linear combination moves along the line defined by .  When and , we find   When and is allowed to vary, the linear combinations lie on the line through parallel to .  If the weights and , then the linear combination is the vector .  If the weights and , then the linear combination is the vector .  We find the linear system for the weights: If we construct the corresponding augmented matrix and determine its reduced row echelon matrix, we find the weights and .  In the same way, we construct a linear system for the weights whose augmented matrix is which shows that there are weights that produce the desired linear combination. The same will happen for any vector that we ask to write as a linear combination of and .  Every two-dimensional vector can be written as a linear combination of and because the coefficient matrix of the linear system remains the same. Since that coefficient matrix has a pivot position in every row, the augmented matrix can never have a pivot position in the rightmost column.        The linear combinations lie on the line defined by .   .  They lie on the line through parallel to .  Yes, with weights .  Yes, with weights and .  This can be done by writing the appropriate linear system for the weights.  No, any two-dimensional vector can be expressed as a linear combination of and .      This activity illustrates how linear combinations are constructed geometrically: the linear combination is found by walking along a total of times followed by walking along a total of times. When one of the weights is held constant while the other varies, the vector moves along a line.    The previous activity also shows that questions about linear combinations lead naturally to linear systems. Suppose we have vectors and . Let's determine whether we can describe the vector as a linear combination of and . In other words, we would like to know whether there are weights and such that   This leads to the equations   Equating the components of the vectors on each side of the equation, we arrive at the linear system This means that is a linear combination of and if this linear system is consistent.  To solve this linear system, we construct its corresponding augmented matrix and find its reduced row echelon form, giving us the weights and ; that is, . In fact, we know more because the reduced row echelon matrix tells us that these are the only possible weights. Therefore, may be expressed as a linear combination of and in exactly one way.    This example demonstrates the connection between linear combinations and linear systems. Asking whether a vector is a linear combination of vectors is equivalent to asking whether an associated linear system is consistent.  In fact, we may easily describe the associated linear system in terms of the vectors , , and . Notice that the augmented matrix we found in our example was The first two columns of this matrix are and and the rightmost column is . As shorthand, we will write this augmented matrix replacing the columns with their vector representation: . This fact is generally true so we record it in the following proposition.    The vector is a linear combination of the vectors if and only if the linear system corresponding to the augmented matrix is consistent. A solution to this linear system gives weights such that .    The next activity puts this proposition to use.   Linear combinations and linear systems     Given the vectors , can be expressed as a linear combination of , , and ? Rephrase this question by writing a linear system for the weights , , and and use the Sage cell below to answer this question.   Consider the following linear system. Identify vectors , , , and such that the question \"Is this linear system consistent?\" is equivalent to the question \"Can be expressed as a linear combination of , , and ?\"   Consider the vectors . Can be expressed as a linear combination of , , and ? If so, can be written as a linear combination of these vectors in more than one way?    Considering the vectors , , and from the previous part, can we write every three-dimensional vector as a linear combination of these vectors? Explain how the pivot positions of the matrix help answer this question.   Now consider the vectors . Can be expressed as a linear combination of , , and ? If so, can be written as a linear combination of these vectors in more than one way?    Considering the vectors , , and from the previous part, can we write every three-dimensional vector as a linear combination of these vectors? Explain how the pivot positions of the matrix help answer this question.        We find the linear system with corresponding augmented matrix This shows that the linear system is inconsistent so there are no such weights , , and . This means that is not a linear combination of , , and .  We find vectors   This is the same as asking if the linear system corresponding to the following augmented matrix is consistent: From the reduced row echelon form, we see that the system is consistent, which means that can be expressed as a linear combination of , , and . Moreover, there are infinitely many ways in which we can do so.  No, it is not possible to write every three-dimensional as a linear combination of , , and because the matrix does not have a pivot position in every row. This means that, with some choice of vector , we will obtain an inconsistent system.  We find the augmented matrix This shows that can be expressed as a linear combination of , , and in exactly one way.  Every vector can be expressed as a linear combination of , , and in exactly one way because has a pivot position in every row and every column.        The vector cannot be expressed as a linear combination of , , and .  We find vectors   Yes, can be expressed as a linear combination of , , and in infinitely many ways.  No.  Yes, can be expressed as a linear combination of , , and in exactly one way.  Any vector can be expressed as a linear combination of , , and in exactly one way.        Consider the vectors and , as shown in .   Vectors and .      These vectors appear to lie on the same line, a fact that becomes apparent once we notice that . Intuitively, we think of the linear combination as the result of walking times in the direction and times in the direction. With these vectors, we are always walking along the same line so it would seem that any linear combination of these vectors should lie on the same line. In addition, a vector that is not on the line, say , should be not be expressible as a linear combination of and .  We can verify this by checking This shows that the associated linear system is inconsistent, which means that the vector cannot be written as a linear combination of and .  Notice that the reduced row echelon form of the coefficient matrix tells us to expect this. Since there is not a pivot position in the second row of the coefficient matrix , it is possible for a pivot position to appear in the rightmost column of the augmented matrix for some choice of .      Summary  This section has introduced vectors, linear combinations, and their connection to linear systems.   There are two operations we can perform with vectors: scalar multiplication and vector addition. Both of these operations have geometric meaning.   Given a set of vectors and a set of scalars we call weights, we can create a linear combination using scalar multiplication and vector addition.   A solution to the linear system whose augmented matrix is is a set of weights that expresses as a linear combination of .       Consider the vectors    Sketch these vectors below.     Compute the vectors , , , and and add them into the sketch above.  Sketch below the set of vectors having the form where is any scalar.     Sketch below the line . Then identify two vectors and so that this line is described by . Are there other choices for the vectors and ?                   This forms the line passing through parallel to .  There are many possibilities. One is and .               This forms the line passing through parallel to .  There are many possibilities. One is and .     Shown below are two vectors and       Express the labeled points as linear combinations of and .   Sketch the line described parametrically as .      We have   This is the line passing through parallel to .     We have   This is the line passing through parallel to .     Consider the vectors    Find the linear combination with weights , , and .   Can you write the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.    Can you write the vector as a linear combination using just the first two vectors  ? If so, describe all the ways in which you can do so.    Can you write as a linear combination of and ? If so, in how many ways?      The linear combination    and .   .   .     The linear combination   The appropriate linear system corresponds to the augmented matrix This shows we obtain when the weights are chosen so that   In this case, we want , which means that .  We see that .     Nutritional information about a breakfast cereal is printed on the box. For instance, one serving of Frosted Flakes has 111 calories, 140 milligrams of sodium, and 1.2 grams of protein. We may represent this as a vector . One serving of Cocoa Puffs has 120 calories, 105 milligrams of sodium, and 1.0 grams of protein.    Write the vector describing the nutritional content of Cocoa Puffs.   Suppose you eat servings of Frosted Flakes and servings of Cocoa Puffs. Use the language of vectors and linear combinations to express the quantities of calories, sodium, and protein you have consumed.   How many servings of each cereal have you eaten if you have consumed 342 calories, 385 milligrams of sodium, and 3.4 grams of protein.   Suppose your sister consumed 250 calories, 200 milligrams of sodium, and 4 grams of protein. What can you conclude about her breakfast?      .  The totals consumed are expressed by the vector   You had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Your sister must have eaten something else.      .  The totals consumed are expressed by the vector   We ask to write the vector as a linear combination of the two cereal vectors. This leads to the augmented matrix This means that you had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Now the augmented matrix is which represents an inconsistent system. This means that your sister must have eaten something else.     Consider the vectors     Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Show that can be written as a linear combination of and .   Explain why any linear combination of , , and , can be rewritten as a linear combination of just and .     The vector may be expressed as a linear combination of , , and provided that the weights are related by and .  The vector cannot be written as a linear combination of , , and .   .   .     We have the system corresponding to the augmented matrix from which we conclude that may be expressed as a linear combination of , , and provided that the weights are related by and .  Here we have which represents an inconsistent system and shows that cannot be written as a linear combination of , , and .  The augmented matrix is which shows that .   .     Consider the vectors For what value(s) of , if any, can the vector be written as a linear combination of and ?    .   We form the augmented matrix and find a triangular matrix that is row equivalent: This represents a consistent system when .    Determine whether the following statements are true or false and provide a justification for your response.  Given two vectors and , the vector is a linear combination of and .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row. If is any -dimensional vector, then can be written as a linear combination of .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row and every column. If is any -dimensional vector, then can be written as a linear combination of in exactly one way.  It is possible to find two 3-dimensional vectors and such that every 3-dimensional vector can be written as a linear combination of and .      True  True  True  False     True, because we can choose the weights and .  True, because the augmented matrix can never have a pivot position in the rightmost column.  True, because the augmented matrix can never have a pivot position in the rightmost column and the corresponding linear system cannot have a free variable.  False, because it is possible to choose a vector such that the augmented matrix has a pivot in the rightmost column.     A theme that will later unfold concerns the use of coordinate systems. We can identify the point with the tip of the vector , drawn emanating from the origin. We can then think of the usual Cartesian coordinate system in terms of linear combinations of the vectors For instance, the point is identified with the vector as shown on the left in .       The usual Cartesian coordinate system, defined by the vectors and , is shown on the left along with the representation of the point . The right shows a nonstandard coordinate system defined by vectors and .   If instead we have vectors , we may define a new coordinate system in which a point will correspond to the vector . For instance, the point is shown on the right side of .  Write the point in standard coordinates; that is, find and such that .   Write the point in the new coordinate system; that is, find and such that .   Convert a general point , expressed in the new coordinate system, into standard Cartesian coordinates .   What is the general strategy for converting a point from standard Cartesian coordinates to the new coordinates ? Actually implementing this strategy in general may take a bit of work so just describe the strategy. We will study this in more detail later.      .   .     Solve the linear system corresponding to the augmented matrix      We have   We have . Solving this equation, we find and , which means that .  As in the first part of this problem, we write   We want to solve by constructing the augmented matrix and finding its reduced row echelon form.      "
},
{
  "id": "fig-vector",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#fig-vector",
  "type": "Figure",
  "number": "2.1.1",
  "title": "",
  "body": " A graphical representation of the vector .     "
},
{
  "id": "exploration-3",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#exploration-3",
  "type": "Preview Activity",
  "number": "2.1.1",
  "title": "Scalar Multiplication and Vector Addition.",
  "body": " Scalar Multiplication and Vector Addition   Suppose that      Find expressions for the vectors and sketch them using .  Sketch the vectors on this grid.         What geometric effect does scalar multiplication have on a vector? Also, describe the effect that multiplying by a negative scalar has.   Sketch the vectors using .  Sketch the vectors on this grid.         Consider vectors that have the form where is any scalar. Sketch a few of these vectors when, say, and . Give a geometric description of this set of vectors.  Sketch the vectors on this grid.        If and are two scalars, then the vector is called a linear combination of the vectors and . Find the vector that is the linear combination when and .    Can the vector be represented as a linear combination of and ? Asked differently, can we find scalars and such that .      Solutions to this preview activity are given in the text below.   "
},
{
  "id": "fig-scalar-mult",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#fig-scalar-mult",
  "type": "Figure",
  "number": "2.1.5",
  "title": "",
  "body": "    Scalar multiples of the vector .  "
},
{
  "id": "fig-vector-sum",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#fig-vector-sum",
  "type": "Figure",
  "number": "2.1.6",
  "title": "",
  "body": "     Vector addition as a simple walk in the plane is illustrated on the left. The vector sum is represented as the diagonal of a parallelogram on the right.  "
},
{
  "id": "fig-parametric-line",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#fig-parametric-line",
  "type": "Figure",
  "number": "2.1.7",
  "title": "",
  "body": "    The set of vectors form a line.  "
},
{
  "id": "observation-5",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#observation-5",
  "type": "Observation",
  "number": "2.1.8",
  "title": "",
  "body": " Even though these vector operations are new, it is straightforward to check that some familiar properties hold.   Commutativity   .   Distributivity   .    "
},
{
  "id": "definition-6",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#definition-6",
  "type": "Definition",
  "number": "2.1.9",
  "title": "",
  "body": " linear combination  weights  The linear combination of the vectors with scalars is the vector The scalars are called the weights of the linear combination.  "
},
{
  "id": "activity-11",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#activity-11",
  "type": "Activity",
  "number": "2.1.2",
  "title": "",
  "body": "  In this activity, we will look at linear combinations of a pair of vectors, and .   Linear combinations of vectors and .      The weight is initially set to 0. Explain what happens as you vary while keeping . How is this related to scalar multiplication?   What is the linear combination of and when and ? You may find this result using the diagram, but you should also verify it by computing the linear combination.   Describe the vectors that arise when the weight is set to 1 and is varied. How is this related to our investigations in the preview activity?   Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?   Can the vector be expressed as a linear combination of and ? If so, what are the weights and ?   Verify the result from the previous part by algebraically finding the weights and that form the linear combination .   Can the vector be expressed as a linear combination of and ? What about the vector ?   Are there any two-dimensional vectors that cannot be expressed as linear combinations of and ?       When we vary with , the linear combination moves along the line defined by .  When and , we find   When and is allowed to vary, the linear combinations lie on the line through parallel to .  If the weights and , then the linear combination is the vector .  If the weights and , then the linear combination is the vector .  We find the linear system for the weights: If we construct the corresponding augmented matrix and determine its reduced row echelon matrix, we find the weights and .  In the same way, we construct a linear system for the weights whose augmented matrix is which shows that there are weights that produce the desired linear combination. The same will happen for any vector that we ask to write as a linear combination of and .  Every two-dimensional vector can be written as a linear combination of and because the coefficient matrix of the linear system remains the same. Since that coefficient matrix has a pivot position in every row, the augmented matrix can never have a pivot position in the rightmost column.        The linear combinations lie on the line defined by .   .  They lie on the line through parallel to .  Yes, with weights .  Yes, with weights and .  This can be done by writing the appropriate linear system for the weights.  No, any two-dimensional vector can be expressed as a linear combination of and .     "
},
{
  "id": "example-3",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#example-3",
  "type": "Example",
  "number": "2.1.11",
  "title": "",
  "body": "  The previous activity also shows that questions about linear combinations lead naturally to linear systems. Suppose we have vectors and . Let's determine whether we can describe the vector as a linear combination of and . In other words, we would like to know whether there are weights and such that   This leads to the equations   Equating the components of the vectors on each side of the equation, we arrive at the linear system This means that is a linear combination of and if this linear system is consistent.  To solve this linear system, we construct its corresponding augmented matrix and find its reduced row echelon form, giving us the weights and ; that is, . In fact, we know more because the reduced row echelon matrix tells us that these are the only possible weights. Therefore, may be expressed as a linear combination of and in exactly one way.   "
},
{
  "id": "prop-system-comb",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#prop-system-comb",
  "type": "Proposition",
  "number": "2.1.12",
  "title": "",
  "body": "  The vector is a linear combination of the vectors if and only if the linear system corresponding to the augmented matrix is consistent. A solution to this linear system gives weights such that .   "
},
{
  "id": "activity-12",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#activity-12",
  "type": "Activity",
  "number": "2.1.3",
  "title": "Linear combinations and linear systems.",
  "body": " Linear combinations and linear systems     Given the vectors , can be expressed as a linear combination of , , and ? Rephrase this question by writing a linear system for the weights , , and and use the Sage cell below to answer this question.   Consider the following linear system. Identify vectors , , , and such that the question \"Is this linear system consistent?\" is equivalent to the question \"Can be expressed as a linear combination of , , and ?\"   Consider the vectors . Can be expressed as a linear combination of , , and ? If so, can be written as a linear combination of these vectors in more than one way?    Considering the vectors , , and from the previous part, can we write every three-dimensional vector as a linear combination of these vectors? Explain how the pivot positions of the matrix help answer this question.   Now consider the vectors . Can be expressed as a linear combination of , , and ? If so, can be written as a linear combination of these vectors in more than one way?    Considering the vectors , , and from the previous part, can we write every three-dimensional vector as a linear combination of these vectors? Explain how the pivot positions of the matrix help answer this question.        We find the linear system with corresponding augmented matrix This shows that the linear system is inconsistent so there are no such weights , , and . This means that is not a linear combination of , , and .  We find vectors   This is the same as asking if the linear system corresponding to the following augmented matrix is consistent: From the reduced row echelon form, we see that the system is consistent, which means that can be expressed as a linear combination of , , and . Moreover, there are infinitely many ways in which we can do so.  No, it is not possible to write every three-dimensional as a linear combination of , , and because the matrix does not have a pivot position in every row. This means that, with some choice of vector , we will obtain an inconsistent system.  We find the augmented matrix This shows that can be expressed as a linear combination of , , and in exactly one way.  Every vector can be expressed as a linear combination of , , and in exactly one way because has a pivot position in every row and every column.        The vector cannot be expressed as a linear combination of , , and .  We find vectors   Yes, can be expressed as a linear combination of , , and in infinitely many ways.  No.  Yes, can be expressed as a linear combination of , , and in exactly one way.  Any vector can be expressed as a linear combination of , , and in exactly one way.     "
},
{
  "id": "example-4",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#example-4",
  "type": "Example",
  "number": "2.1.13",
  "title": "",
  "body": "  Consider the vectors and , as shown in .   Vectors and .      These vectors appear to lie on the same line, a fact that becomes apparent once we notice that . Intuitively, we think of the linear combination as the result of walking times in the direction and times in the direction. With these vectors, we are always walking along the same line so it would seem that any linear combination of these vectors should lie on the same line. In addition, a vector that is not on the line, say , should be not be expressible as a linear combination of and .  We can verify this by checking This shows that the associated linear system is inconsistent, which means that the vector cannot be written as a linear combination of and .  Notice that the reduced row echelon form of the coefficient matrix tells us to expect this. Since there is not a pivot position in the second row of the coefficient matrix , it is possible for a pivot position to appear in the rightmost column of the augmented matrix for some choice of .   "
},
{
  "id": "exercise-22",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#exercise-22",
  "type": "Exercise",
  "number": "2.1.4.1",
  "title": "",
  "body": " Consider the vectors    Sketch these vectors below.     Compute the vectors , , , and and add them into the sketch above.  Sketch below the set of vectors having the form where is any scalar.     Sketch below the line . Then identify two vectors and so that this line is described by . Are there other choices for the vectors and ?                   This forms the line passing through parallel to .  There are many possibilities. One is and .               This forms the line passing through parallel to .  There are many possibilities. One is and .   "
},
{
  "id": "exercise-23",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#exercise-23",
  "type": "Exercise",
  "number": "2.1.4.2",
  "title": "",
  "body": " Shown below are two vectors and       Express the labeled points as linear combinations of and .   Sketch the line described parametrically as .      We have   This is the line passing through parallel to .     We have   This is the line passing through parallel to .   "
},
{
  "id": "exercise-24",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#exercise-24",
  "type": "Exercise",
  "number": "2.1.4.3",
  "title": "",
  "body": " Consider the vectors    Find the linear combination with weights , , and .   Can you write the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.    Can you write the vector as a linear combination using just the first two vectors  ? If so, describe all the ways in which you can do so.    Can you write as a linear combination of and ? If so, in how many ways?      The linear combination    and .   .   .     The linear combination   The appropriate linear system corresponds to the augmented matrix This shows we obtain when the weights are chosen so that   In this case, we want , which means that .  We see that .   "
},
{
  "id": "exercise-25",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#exercise-25",
  "type": "Exercise",
  "number": "2.1.4.4",
  "title": "",
  "body": " Nutritional information about a breakfast cereal is printed on the box. For instance, one serving of Frosted Flakes has 111 calories, 140 milligrams of sodium, and 1.2 grams of protein. We may represent this as a vector . One serving of Cocoa Puffs has 120 calories, 105 milligrams of sodium, and 1.0 grams of protein.    Write the vector describing the nutritional content of Cocoa Puffs.   Suppose you eat servings of Frosted Flakes and servings of Cocoa Puffs. Use the language of vectors and linear combinations to express the quantities of calories, sodium, and protein you have consumed.   How many servings of each cereal have you eaten if you have consumed 342 calories, 385 milligrams of sodium, and 3.4 grams of protein.   Suppose your sister consumed 250 calories, 200 milligrams of sodium, and 4 grams of protein. What can you conclude about her breakfast?      .  The totals consumed are expressed by the vector   You had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Your sister must have eaten something else.      .  The totals consumed are expressed by the vector   We ask to write the vector as a linear combination of the two cereal vectors. This leads to the augmented matrix This means that you had two servings of Frosted Flakes and one serving of Cocoa Puffs.  Now the augmented matrix is which represents an inconsistent system. This means that your sister must have eaten something else.   "
},
{
  "id": "exercise-26",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#exercise-26",
  "type": "Exercise",
  "number": "2.1.4.5",
  "title": "",
  "body": " Consider the vectors     Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Can you express the vector as a linear combination of , , and ? If so, describe all the ways in which you can do so.   Show that can be written as a linear combination of and .   Explain why any linear combination of , , and , can be rewritten as a linear combination of just and .     The vector may be expressed as a linear combination of , , and provided that the weights are related by and .  The vector cannot be written as a linear combination of , , and .   .   .     We have the system corresponding to the augmented matrix from which we conclude that may be expressed as a linear combination of , , and provided that the weights are related by and .  Here we have which represents an inconsistent system and shows that cannot be written as a linear combination of , , and .  The augmented matrix is which shows that .   .   "
},
{
  "id": "exercise-27",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#exercise-27",
  "type": "Exercise",
  "number": "2.1.4.6",
  "title": "",
  "body": " Consider the vectors For what value(s) of , if any, can the vector be written as a linear combination of and ?    .   We form the augmented matrix and find a triangular matrix that is row equivalent: This represents a consistent system when .  "
},
{
  "id": "exercise-28",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#exercise-28",
  "type": "Exercise",
  "number": "2.1.4.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  Given two vectors and , the vector is a linear combination of and .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row. If is any -dimensional vector, then can be written as a linear combination of .  Suppose is a collection of -dimensional vectors and that the matrix has a pivot position in every row and every column. If is any -dimensional vector, then can be written as a linear combination of in exactly one way.  It is possible to find two 3-dimensional vectors and such that every 3-dimensional vector can be written as a linear combination of and .      True  True  True  False     True, because we can choose the weights and .  True, because the augmented matrix can never have a pivot position in the rightmost column.  True, because the augmented matrix can never have a pivot position in the rightmost column and the corresponding linear system cannot have a free variable.  False, because it is possible to choose a vector such that the augmented matrix has a pivot in the rightmost column.   "
},
{
  "id": "exercise-29",
  "level": "2",
  "url": "sec-vectors-lin-combs.html#exercise-29",
  "type": "Exercise",
  "number": "2.1.4.8",
  "title": "",
  "body": " A theme that will later unfold concerns the use of coordinate systems. We can identify the point with the tip of the vector , drawn emanating from the origin. We can then think of the usual Cartesian coordinate system in terms of linear combinations of the vectors For instance, the point is identified with the vector as shown on the left in .       The usual Cartesian coordinate system, defined by the vectors and , is shown on the left along with the representation of the point . The right shows a nonstandard coordinate system defined by vectors and .   If instead we have vectors , we may define a new coordinate system in which a point will correspond to the vector . For instance, the point is shown on the right side of .  Write the point in standard coordinates; that is, find and such that .   Write the point in the new coordinate system; that is, find and such that .   Convert a general point , expressed in the new coordinate system, into standard Cartesian coordinates .   What is the general strategy for converting a point from standard Cartesian coordinates to the new coordinates ? Actually implementing this strategy in general may take a bit of work so just describe the strategy. We will study this in more detail later.      .   .     Solve the linear system corresponding to the augmented matrix      We have   We have . Solving this equation, we find and , which means that .  As in the first part of this problem, we write   We want to solve by constructing the augmented matrix and finding its reduced row echelon form.    "
},
{
  "id": "sec-matrices-lin-combs",
  "level": "1",
  "url": "sec-matrices-lin-combs.html",
  "type": "Section",
  "number": "2.2",
  "title": "Matrix multiplication and linear combinations",
  "body": " Matrix multiplication and linear combinations   The previous section introduced vectors and linear combinations and demonstrated how they provide a way to think about linear systems geometrically. In particular, we saw that the vector is a linear combination of the vectors precisely when the linear system corresponding to the augmented matrix is consistent.  Our goal in this section is to introduce matrix multiplication, another algebraic operation that deepens the connection between linear systems and linear combinations.    Scalar multiplication and addition of matrices   matrix, shape We first thought of a matrix as a rectangular array of numbers. If we say that the shape of a matrix is , we mean that it has rows and columns. For instance, the shape of the matrix below is : .  We may also think of the columns of a matrix as a set of vectors. For instance, the matrix above may be represented as where . In this way, we see that the matrix is equivalent to an ordered set of 4 vectors in .   matrix, addition  matrix, scalar multiplication This means that we may define scalar multiplication and matrix addition operations using the corresponding column-wise vector operations. For instance,    Matrix operations     Compute the scalar multiple .   Find the sum .   Suppose that and are two matrices. What do we need to know about their shapes before we can form the sum ?   matrix, identity  The matrix , which we call the identity matrix, is the matrix whose entries are zero except for the diagonal entries, all of which are 1. For instance, . If we can form the sum , what must be true about the matrix ?   Find the matrix where .            The shapes must be the same.  The shape of must be .               The shapes must be the same.  The shape of must be .        As this preview activity shows, the operations of scalar multiplication and addition of matrices are natural extensions of their vector counterparts. Some care, however, is required when adding matrices. Since we need the same number of vectors to add and since those vectors must be of the same dimension, two matrices must have the same shape if we wish to form their sum.    Matrix-vector multiplication and linear combinations  A more important operation will be matrix multiplication as it allows us to compactly express linear systems. We now introduce the product of a matrix and a vector with an example.   Matrix-vector multiplication   Suppose we have the matrix and vector : . Their product will be defined to be the linear combination of the columns of using the components of as weights. This means that   Because has two columns, we need two weights to form a linear combination of those columns, which means that must have two components. In other words, the number of columns of must equal the dimension of the vector .  Similarly, the columns of are 3-dimensional so any linear combination of them is 3-dimensional as well. Therefore, will be 3-dimensional.  We then see that if is a matrix, must be a 2-dimensional vector and will be 3-dimensional.    More generally, we have the following definition.   Matrix-vector multiplication  matrix-vector multiplication   The product of a matrix by a vector will be the linear combination of the columns of using the components of as weights. More specifically, if then .  If is an matrix, then must be an -dimensional vector, and the product will be an -dimensional vector.    The next activity explores some properties of matrix multiplication.   Matrix-vector multiplication     Find the matrix product .   Suppose that is the matrix . If is defined, what is the dimension of the vector and what is the dimension of ?   A vector whose entries are all zero is denoted by . If is a matrix, what is the product ?   Suppose that is the identity matrix and . Find the product and explain why is called the identity matrix.   Suppose we write the matrix in terms of its columns as . If the vector , what is the product ?    Suppose that . Is there a vector such that ?       We have   The dimension of must be the same as the number of columns of so is three-dimensional. The dimension of equals the number of rows of so is four-dimensional.  We have .  We have ; that is, multiplying a vector by produces the same vector.  The product .  If , then we have with corresponding augmented matrix This means that is the unique solution to the equation .         The dimension of must three, and the dimension of must be four.   .   .   .   is the unique solution.     Multiplication of a matrix and a vector is defined as a linear combination of the columns of . However, there is a shortcut for computing such a product. Let's look at our previous example and focus on the first row of the product. .  To find the first component of the product, we consider the first row of the matrix. We then multiply the first entry in that row by the first component of the vector, the second entry by the second component of the vector, and so on, and add the results. In this way, we see that the third component of the product would be obtained from the third row of the matrix by computing .  You are encouraged to evaluate the product of the previous activity using this shortcut and compare the result to what you found while completing that activity.    Sage can find the product of a matrix and vector using the * operator. For example,    Use Sage to evaluate the product from of the previous activity.    In Sage, define the matrix and vectors .   What do you find when you evaluate ?  What do you find when you evaluate and and compare your results?  What do you find when you evaluate and and compare your results?       We define A = matrix(3, 4, [1, 2, 0, -1, 2, 4, -3, -2, -1, -2, 6, 1]) v = vector([3, 1, -1, 1]) A*v   We define A = matrix(2, 3, [-2, 0, 3, 1, 4, 2]) zero = vector([0, 0]) v = vector([-2, 3]) w = vector([1, 2])    .   .         We define A = matrix(3, 4, [1, 2, 0, -1, 2, 4, -3, -2, -1, -2, 6, 1]) v = vector([3, 1, -1, 1]) A*v   We define A = matrix(2, 3, [-2, 0, 3, 1, 4, 2]) zero = vector([0, 0]) v = vector([-2, 3]) w = vector([1, 2])    .   .        This activity demonstrates several general properties satisfied by matrix multiplication that we record here.   Linearity of matrix multiplication   If is a matrix, and vectors of the appropriate dimensions, and a scalar, then    .    .    .        Matrix-vector multiplication and linear systems  So far, we have begun with a matrix and a vector and formed their product . We would now like to turn this around: beginning with a matrix and a vector , we will ask if we can find a vector such that . This will naturally lead back to linear systems.  To see the connection between the matrix equation and linear systems, let's write the matrix in terms of its columns and in terms of its components. .  We know that the matrix product forms a linear combination of the columns of . Therefore, the equation is merely a compact way of writing the equation for the weights : . We have seen this equation before: Remember that says that the solutions of this equation are the same as the solutions to the linear system whose augmented matrix is .  This gives us three different ways of looking at the same solution space.    If and , then the following statements are equivalent.   The vector satisfies the equation .   The vector is a linear combination of the columns of with weights : .   The components of form a solution to the linear system corresponding to the augmented matrix       When the matrix , we will frequently write and say that the matrix is augmented by the vector .  The equation gives a notationally compact way to write a linear system. Moreover, this notation will allow us to focus on important features of the system that determine its solution space.    We will describe the solution space of the equation   By , this equation may be equivalently expressed as , which is the linear system corresponding to the augmented matrix . The reduced row echelon form of the augmented matrix is which corresponds to the linear system The variable is free so we may write the solution space parametrically as   Since we originally asked to describe the solutions to the equation , we will express the solution in terms of the vector : As before, we call this a parametric description of the solution space.  This shows that the solutions may be written in the form , for appropriate vectors and . Geometrically, the solution space is a line in through moving parallel to .     The equation      Consider the linear system Identify the matrix and vector to express this system in the form .   If and are as below, write the linear system corresponding to the equation and describe its solution space, using a parametric description if appropriate:    Describe the solution space of the equation .   Suppose is an matrix. What can you guarantee about the solution space of the equation ?       and .  Form the augmented matrix so that   We have the augmented matrix Since this system is inconsistent, there are no solutions to the matrix equation.  We know that there is at least one solution, namely, .       and .     There are no solutions.  There is at least one solution, namely, .       Matrix-matrix products  In this section, we have developed some algebraic operations on matrices with the aim of simplifying our description of linear systems. We now introduce a final operation, the product of two matrices, that will become important when we study linear transformations in .   Matrix-matrix multiplication   Given matrices and , we form their product by first writing in terms of its columns and then defining       Given the matrices , we have .      It is important to note that we can only multiply matrices if the shapes of the matrices are compatible. More specifically, when constructing the product , the matrix multiplies the columns of . Therefore, the number of columns of must equal the number of rows of . When this condition is met, the number of rows of is the number of rows of , and the number of columns of is the number of columns of .      Consider the matrices .   Before computing, first explain why the shapes of and enable us to form the product . Then describe the shape of .   Compute the product .  Sage can multiply matrices using the * operator. Define the matrices and in the Sage cell below and check your work by computing .   Are we able to form the matrix product ? If so, use the Sage cell above to find . Is it generally true that ?  Suppose we form the three matrices. . Compare what happens when you compute and . State your finding as a general principle.   Compare the results of evaluating and and state your finding as a general principle.  When we are dealing with real numbers, we know if and , then . Define matrices and compute and . If , is it necessarily true that ?  Again, with real numbers, we know that if , then either or . Define and compute . If , is it necessarily true that either or ?       The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .  Define A = matrix(2, 3, [1, 3, 2, -3, 4, -1]) B = matrix(3, 2, [3, 0, 1, 2, -2, -1]) A*B   Yes, we can form the product because the number of columns of equals the number of rows of . This product will be , however, so it must be true that .  We find that .  We find that .  It is not generally true that if , as illustrated by this example.  It is not generally true that or if , as illustrated by this example.      The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .  Define A = matrix(2, 3, [1, 3, 2, -3, 4, -1]) B = matrix(3, 2, [3, 0, 1, 2, -2, -1]) A*B   It is not generally true that .  We find that .  We find that .  It is not generally true that if .  It is not generally true that or if .     This activity demonstrated some general properties about products of matrices, which mirror some properties about operations with real numbers.   Properties of Matrix-matrix Multiplication  If , , and are matrices such that the following operations are defined, it follows that  Associativity:   .  Distributivity:   .   .      At the same time, there are a few properties that hold for real numbers that do not hold for matrices.   Caution  The following properties hold for real numbers but not for matrices.  Commutativity:  It is not generally true that .   Cancellation:  It is not generally true that implies that .   Zero divisors:  It is not generally true that implies that either or .        Summary  In this section, we have found an especially simple way to express linear systems using matrix multiplication.   If is an matrix and an -dimensional vector, then is the linear combination of the columns of using the components of as weights. The vector is -dimensional.   The solution space to the equation is the same as the solution space to the linear system corresponding to the augmented matrix .  If is an matrix and is an matrix, we can form the product , which is an matrix whose columns are the products of and the columns of .       Consider the system of linear equations .  Find the matrix and vector that expresses this linear system in the form .  Give a description of the solution space to the equation .       and .  There is a unique solution .       and .  We have so there is a unique solution .      Suppose that is a matrix, and that is a vector. If is defined, what is the dimension of ? What is the dimension of ?    and .   If is defined, then and .    Suppose that is a matrix whose columns are and ; that is, .   What is the dimension of the vectors and ?   What is the product in terms of and ? What is the product ? What is the product ?   If we know that what is the matrix ?      Both and are three-dimensional.   and . Also, .        Both and are three-dimensional.   and . Also, .        Suppose that the matrix where and are shown in .      Two vectors and that form the columns of the matrix .     What is the shape of the matrix ?  On , indicate the vectors .    Find all vectors such that .   Find all vectors such that .       is a matrix.  We have       There is a unique vector .  There is a unique vector .      is a matrix.  We have       There is a unique vector .  There is a unique vector .     Suppose that .  Describe the solution space to the equation .  Find a matrix with no zero entries such that .         There are many possibilities. For instance,      We construct the augmented matrix which shows that   We form by choosing two vectors in the solution space to . For instance,      Consider the matrix .   Find the product where .   Give a description of the vectors such that .     Find the reduced row echelon form of and identify the pivot positions.   Can you find a vector such that is inconsistent?   For a general 3-dimensional vector , what can you say about the solution space of the equation ?      .   .  We see that   No  The solution space will form a line in , as in part b.      .  We consider the augmented matrix so that .  We see that   There is no vector such that is inconsistent because the augmented matrix cannot have a pivot position in the rightmost column.  The coefficient matrix will have exactly one column without a pivot position. Therefore, the solution space will form a line in , as in part b.     The operations that we perform in Gaussian elimination can be accomplished using matrix multiplication. This observation is the basis of an important technique that we will investigate in a subsequent chapter.  Let's consider the matrix .  Suppose that . Verify that is the matrix that results when the second row of is scaled by a factor of 7. What matrix would scale the third row by -3?  Suppose that . Verify that is the matrix that results from interchanging the first and second rows. What matrix would interchange the first and third rows?  Suppose that . Verify that is the matrix that results from multiplying the first row of by and adding it to the second row. What matrix would multiply the first row by 3 and add it to the third row?  When we performed Gaussian elimination, our first goal was to perform row operations that brought the matrix into a triangular form. For our matrix , find the row operations needed to find a row equivalent matrix in triangular form. By expressing these row operations in terms of matrix multiplication, find a matrix such that .       To scale the third row by , we would use the matrix .  To interchange the first and third rows, we would use .  To multiply the first row by 3 and add to the third row, we would use .   .     We compute that To scale the third row by , we would use the matrix .  We compute that To interchange the first and third rows, we would use .  We compute that To multiply the first row by 3 and add to the third row, we would use .  Keeping track of the operations we perform in Gaussian elimination, we define Then . So .     In this exercise, you will construct the inverse of a matrix, a subject that we will investigate more fully in the next chapter. Suppose that is the matrix: .  Find the vectors and such that the matrix satisfies .   In general, it is not true that . Check that it is true, however, for the specific and that appear in this problem.   Suppose that . What do you find when you evaluate ?  Suppose that we want to solve the equation . We know how to do this using Gaussian elimination; let's use our matrix to find a different way: . In other words, the solution to the equation is .  Consider the equation . Find the solution in two different ways, first using Gaussian elimination and then as , and verify that you have found the same result.           It is true, in this case, that .   .   .     We find and by solving the equations This gives   It is true, in this case, that .  We have seen that .  If , then is the solution to the equation .     Determine whether the following statements are true or false and provide a justification for your response.  If is defined, then the number of components of equals the number of rows of .  The solution space to the equation is equivalent to the solution space to the linear system whose augmented matrix is .  If a linear system of equations has 8 equations and 5 unknowns, then the shape of the matrix in the corresponding equation is .  If has a pivot position in every row, then every equation is consistent.  If is a matrix, then is inconsistent for some vector .     False  True  False  True  True     False, the number of components of equals the number of columns of .  True. This is a result of .  False. The shape of is .  True, because the augmented matrix cannot have a pivot position in the rightmost column.  True. Because there is not a pivot position in every row of , the augmented matrix will have a pivot position in the rightmost column for some vectors .     Suppose that is a matrix and that the equation has a unique solution for some vector .  What does this say about the pivot positions of the matrix ? Write the reduced row echelon form of .  Can you find another vector such that is inconsistent?  What can you say about the solution space to the equation ?  Suppose . Explain why every four-dimensional vector can be written as a linear combination of the vectors , , , and in exactly one way.       .  No   .  Every equation has exactly one solution.     Since the solution is unique, there must be a pivot position in every column of . This means that there are four pivot positions so there is a pivot position in every row. In other words, .  No, because has a pivot in every row.  There is only one solution, which is .  If is a four-dimensional vector, then has a unique solution since has a pivot position in every row and every column. This means that can be written as a linear combination of the columns of in exactly one way.     Define the matrix .  Describe the solution space to the homogeneous equation using a parametric description, if appropriate. What does this solution space represent geometrically?   Describe the solution space to the equation where . What does this solution space represent geometrically and how does it compare to the previous solution space?   We will now explain the relationship between the previous two solution spaces. Suppose that is a solution to the homogeneous equation; that is . Suppose also that is a solution to the equation ; that is, .  Use the Linearity Principle expressed in to explain why is a solution to the equation . You may do this by evaluating .  That is, if we find one solution to an equation , we may add any solution to the homogeneous equation to and still have a solution to the equation . In other words, the solution space to the equation is given by translating the solution space to the homogeneous equation by the vector .      .   .  Notice that .     We have the augmented matrix so that . The solution space is a line that passes through the origin.  Now we have so that . This is a line through parallel to . Notice that this line is parallel to the line in part a.  Notice that . This shows that if we add a vector from the solution space to to a vector from the solution space to , we obtain a vector in the solution space to .     Suppose that a city is starting a bicycle sharing program with bicycles at locations and . Bicycles that are rented at one location may be returned to either location at the end of the day. Over time, the city finds that 80% of bicycles rented at location are returned to with the other 20% returned to . Similarly, 50% of bicycles rented at location are returned to and 50% to .  To keep track of the bicycles, we form a vector where is the number of bicycles at location at the beginning of day and is the number of bicycles at . The information above tells us how to determine the distribution of bicycles the following day: Expressed in matrix-vector form, these expressions give where .   Let's check that this makes sense.  Suppose that there are 1000 bicycles at location and none at on day 1. This means we have . Find the number of bicycles at both locations on day 2 by evaluating .  Suppose that there are 1000 bicycles at location and none at on day 1. Form the vector and determine the number of bicycles at the two locations the next day by finding .     Suppose that one day there are 1050 bicycles at location and 450 at location . How many bicycles were there at each location the previous day?    Suppose that there are 500 bicycles at location and 500 at location on Monday. How many bicycles are there at the two locations on Tuesday? on Wednesday? on Thursday?     We see that   .   .     .   represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.     We see that   .   .    We solve the equation to obtain .  If represents the distribution of bicycles on Monday, then represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.     This problem is a continuation of the previous problem.  Let us define vectors . Show that .  Suppose that where and are scalars. Use the Linearity Principle expressed in to explain why .  Continuing in this way, explain why .   Suppose that there are initially 500 bicycles at location and 500 at location . Write the vector and find the scalars and such that .  Use the previous part of this problem to determine , and .  After a very long time, how are all the bicycles distributed?      and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so       and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so      "
},
{
  "id": "exploration-4",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exploration-4",
  "type": "Preview Activity",
  "number": "2.2.1",
  "title": "Matrix operations.",
  "body": " Matrix operations     Compute the scalar multiple .   Find the sum .   Suppose that and are two matrices. What do we need to know about their shapes before we can form the sum ?   matrix, identity  The matrix , which we call the identity matrix, is the matrix whose entries are zero except for the diagonal entries, all of which are 1. For instance, . If we can form the sum , what must be true about the matrix ?   Find the matrix where .            The shapes must be the same.  The shape of must be .               The shapes must be the same.  The shape of must be .       "
},
{
  "id": "example-5",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#example-5",
  "type": "Example",
  "number": "2.2.1",
  "title": "Matrix-vector multiplication.",
  "body": " Matrix-vector multiplication   Suppose we have the matrix and vector : . Their product will be defined to be the linear combination of the columns of using the components of as weights. This means that   Because has two columns, we need two weights to form a linear combination of those columns, which means that must have two components. In other words, the number of columns of must equal the dimension of the vector .  Similarly, the columns of are 3-dimensional so any linear combination of them is 3-dimensional as well. Therefore, will be 3-dimensional.  We then see that if is a matrix, must be a 2-dimensional vector and will be 3-dimensional.   "
},
{
  "id": "definition-7",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#definition-7",
  "type": "Definition",
  "number": "2.2.2",
  "title": "Matrix-vector multiplication.",
  "body": " Matrix-vector multiplication  matrix-vector multiplication   The product of a matrix by a vector will be the linear combination of the columns of using the components of as weights. More specifically, if then .  If is an matrix, then must be an -dimensional vector, and the product will be an -dimensional vector.   "
},
{
  "id": "activity-13",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#activity-13",
  "type": "Activity",
  "number": "2.2.2",
  "title": "Matrix-vector multiplication.",
  "body": " Matrix-vector multiplication     Find the matrix product .   Suppose that is the matrix . If is defined, what is the dimension of the vector and what is the dimension of ?   A vector whose entries are all zero is denoted by . If is a matrix, what is the product ?   Suppose that is the identity matrix and . Find the product and explain why is called the identity matrix.   Suppose we write the matrix in terms of its columns as . If the vector , what is the product ?    Suppose that . Is there a vector such that ?       We have   The dimension of must be the same as the number of columns of so is three-dimensional. The dimension of equals the number of rows of so is four-dimensional.  We have .  We have ; that is, multiplying a vector by produces the same vector.  The product .  If , then we have with corresponding augmented matrix This means that is the unique solution to the equation .         The dimension of must three, and the dimension of must be four.   .   .   .   is the unique solution.    "
},
{
  "id": "activity-14",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#activity-14",
  "type": "Activity",
  "number": "2.2.3",
  "title": "",
  "body": "  Sage can find the product of a matrix and vector using the * operator. For example,    Use Sage to evaluate the product from of the previous activity.    In Sage, define the matrix and vectors .   What do you find when you evaluate ?  What do you find when you evaluate and and compare your results?  What do you find when you evaluate and and compare your results?       We define A = matrix(3, 4, [1, 2, 0, -1, 2, 4, -3, -2, -1, -2, 6, 1]) v = vector([3, 1, -1, 1]) A*v   We define A = matrix(2, 3, [-2, 0, 3, 1, 4, 2]) zero = vector([0, 0]) v = vector([-2, 3]) w = vector([1, 2])    .   .         We define A = matrix(3, 4, [1, 2, 0, -1, 2, 4, -3, -2, -1, -2, 6, 1]) v = vector([3, 1, -1, 1]) A*v   We define A = matrix(2, 3, [-2, 0, 3, 1, 4, 2]) zero = vector([0, 0]) v = vector([-2, 3]) w = vector([1, 2])    .   .       "
},
{
  "id": "prop-matrix-mult-prop",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#prop-matrix-mult-prop",
  "type": "Proposition",
  "number": "2.2.3",
  "title": "Linearity of matrix multiplication.",
  "body": " Linearity of matrix multiplication   If is a matrix, and vectors of the appropriate dimensions, and a scalar, then    .    .    .     "
},
{
  "id": "prop-matrix-eq-solution",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#prop-matrix-eq-solution",
  "type": "Proposition",
  "number": "2.2.4",
  "title": "",
  "body": "  If and , then the following statements are equivalent.   The vector satisfies the equation .   The vector is a linear combination of the columns of with weights : .   The components of form a solution to the linear system corresponding to the augmented matrix      "
},
{
  "id": "example-6",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#example-6",
  "type": "Example",
  "number": "2.2.5",
  "title": "",
  "body": "  We will describe the solution space of the equation   By , this equation may be equivalently expressed as , which is the linear system corresponding to the augmented matrix . The reduced row echelon form of the augmented matrix is which corresponds to the linear system The variable is free so we may write the solution space parametrically as   Since we originally asked to describe the solutions to the equation , we will express the solution in terms of the vector : As before, we call this a parametric description of the solution space.  This shows that the solutions may be written in the form , for appropriate vectors and . Geometrically, the solution space is a line in through moving parallel to .   "
},
{
  "id": "activity-15",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#activity-15",
  "type": "Activity",
  "number": "2.2.4",
  "title": "The equation <span class=\"process-math\">\\(A\\xvec = \\bvec\\)<\/span>.",
  "body": " The equation      Consider the linear system Identify the matrix and vector to express this system in the form .   If and are as below, write the linear system corresponding to the equation and describe its solution space, using a parametric description if appropriate:    Describe the solution space of the equation .   Suppose is an matrix. What can you guarantee about the solution space of the equation ?       and .  Form the augmented matrix so that   We have the augmented matrix Since this system is inconsistent, there are no solutions to the matrix equation.  We know that there is at least one solution, namely, .       and .     There are no solutions.  There is at least one solution, namely, .    "
},
{
  "id": "definition-8",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#definition-8",
  "type": "Definition",
  "number": "2.2.6",
  "title": "Matrix-matrix multiplication.",
  "body": " Matrix-matrix multiplication   Given matrices and , we form their product by first writing in terms of its columns and then defining    "
},
{
  "id": "example-7",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#example-7",
  "type": "Example",
  "number": "2.2.7",
  "title": "",
  "body": "  Given the matrices , we have .   "
},
{
  "id": "observation-6",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#observation-6",
  "type": "Observation",
  "number": "2.2.8",
  "title": "",
  "body": "  It is important to note that we can only multiply matrices if the shapes of the matrices are compatible. More specifically, when constructing the product , the matrix multiplies the columns of . Therefore, the number of columns of must equal the number of rows of . When this condition is met, the number of rows of is the number of rows of , and the number of columns of is the number of columns of .   "
},
{
  "id": "activity-16",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#activity-16",
  "type": "Activity",
  "number": "2.2.5",
  "title": "",
  "body": "  Consider the matrices .   Before computing, first explain why the shapes of and enable us to form the product . Then describe the shape of .   Compute the product .  Sage can multiply matrices using the * operator. Define the matrices and in the Sage cell below and check your work by computing .   Are we able to form the matrix product ? If so, use the Sage cell above to find . Is it generally true that ?  Suppose we form the three matrices. . Compare what happens when you compute and . State your finding as a general principle.   Compare the results of evaluating and and state your finding as a general principle.  When we are dealing with real numbers, we know if and , then . Define matrices and compute and . If , is it necessarily true that ?  Again, with real numbers, we know that if , then either or . Define and compute . If , is it necessarily true that either or ?       The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .  Define A = matrix(2, 3, [1, 3, 2, -3, 4, -1]) B = matrix(3, 2, [3, 0, 1, 2, -2, -1]) A*B   Yes, we can form the product because the number of columns of equals the number of rows of . This product will be , however, so it must be true that .  We find that .  We find that .  It is not generally true that if , as illustrated by this example.  It is not generally true that or if , as illustrated by this example.      The product exists because the number of columns of equals the number of rows of . The dimensions of are .  We have .  Define A = matrix(2, 3, [1, 3, 2, -3, 4, -1]) B = matrix(3, 2, [3, 0, 1, 2, -2, -1]) A*B   It is not generally true that .  We find that .  We find that .  It is not generally true that if .  It is not generally true that or if .    "
},
{
  "id": "exercise-30",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-30",
  "type": "Exercise",
  "number": "2.2.6.1",
  "title": "",
  "body": " Consider the system of linear equations .  Find the matrix and vector that expresses this linear system in the form .  Give a description of the solution space to the equation .       and .  There is a unique solution .       and .  We have so there is a unique solution .    "
},
{
  "id": "exercise-31",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-31",
  "type": "Exercise",
  "number": "2.2.6.2",
  "title": "",
  "body": " Suppose that is a matrix, and that is a vector. If is defined, what is the dimension of ? What is the dimension of ?    and .   If is defined, then and .  "
},
{
  "id": "exercise-32",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-32",
  "type": "Exercise",
  "number": "2.2.6.3",
  "title": "",
  "body": " Suppose that is a matrix whose columns are and ; that is, .   What is the dimension of the vectors and ?   What is the product in terms of and ? What is the product ? What is the product ?   If we know that what is the matrix ?      Both and are three-dimensional.   and . Also, .        Both and are three-dimensional.   and . Also, .      "
},
{
  "id": "exercise-33",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-33",
  "type": "Exercise",
  "number": "2.2.6.4",
  "title": "",
  "body": " Suppose that the matrix where and are shown in .      Two vectors and that form the columns of the matrix .     What is the shape of the matrix ?  On , indicate the vectors .    Find all vectors such that .   Find all vectors such that .       is a matrix.  We have       There is a unique vector .  There is a unique vector .      is a matrix.  We have       There is a unique vector .  There is a unique vector .   "
},
{
  "id": "exercise-34",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-34",
  "type": "Exercise",
  "number": "2.2.6.5",
  "title": "",
  "body": " Suppose that .  Describe the solution space to the equation .  Find a matrix with no zero entries such that .         There are many possibilities. For instance,      We construct the augmented matrix which shows that   We form by choosing two vectors in the solution space to . For instance,    "
},
{
  "id": "exercise-35",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-35",
  "type": "Exercise",
  "number": "2.2.6.6",
  "title": "",
  "body": " Consider the matrix .   Find the product where .   Give a description of the vectors such that .     Find the reduced row echelon form of and identify the pivot positions.   Can you find a vector such that is inconsistent?   For a general 3-dimensional vector , what can you say about the solution space of the equation ?      .   .  We see that   No  The solution space will form a line in , as in part b.      .  We consider the augmented matrix so that .  We see that   There is no vector such that is inconsistent because the augmented matrix cannot have a pivot position in the rightmost column.  The coefficient matrix will have exactly one column without a pivot position. Therefore, the solution space will form a line in , as in part b.   "
},
{
  "id": "exercise-36",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-36",
  "type": "Exercise",
  "number": "2.2.6.7",
  "title": "",
  "body": " The operations that we perform in Gaussian elimination can be accomplished using matrix multiplication. This observation is the basis of an important technique that we will investigate in a subsequent chapter.  Let's consider the matrix .  Suppose that . Verify that is the matrix that results when the second row of is scaled by a factor of 7. What matrix would scale the third row by -3?  Suppose that . Verify that is the matrix that results from interchanging the first and second rows. What matrix would interchange the first and third rows?  Suppose that . Verify that is the matrix that results from multiplying the first row of by and adding it to the second row. What matrix would multiply the first row by 3 and add it to the third row?  When we performed Gaussian elimination, our first goal was to perform row operations that brought the matrix into a triangular form. For our matrix , find the row operations needed to find a row equivalent matrix in triangular form. By expressing these row operations in terms of matrix multiplication, find a matrix such that .       To scale the third row by , we would use the matrix .  To interchange the first and third rows, we would use .  To multiply the first row by 3 and add to the third row, we would use .   .     We compute that To scale the third row by , we would use the matrix .  We compute that To interchange the first and third rows, we would use .  We compute that To multiply the first row by 3 and add to the third row, we would use .  Keeping track of the operations we perform in Gaussian elimination, we define Then . So .   "
},
{
  "id": "exercise-37",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-37",
  "type": "Exercise",
  "number": "2.2.6.8",
  "title": "",
  "body": " In this exercise, you will construct the inverse of a matrix, a subject that we will investigate more fully in the next chapter. Suppose that is the matrix: .  Find the vectors and such that the matrix satisfies .   In general, it is not true that . Check that it is true, however, for the specific and that appear in this problem.   Suppose that . What do you find when you evaluate ?  Suppose that we want to solve the equation . We know how to do this using Gaussian elimination; let's use our matrix to find a different way: . In other words, the solution to the equation is .  Consider the equation . Find the solution in two different ways, first using Gaussian elimination and then as , and verify that you have found the same result.           It is true, in this case, that .   .   .     We find and by solving the equations This gives   It is true, in this case, that .  We have seen that .  If , then is the solution to the equation .   "
},
{
  "id": "exercise-38",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-38",
  "type": "Exercise",
  "number": "2.2.6.9",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If is defined, then the number of components of equals the number of rows of .  The solution space to the equation is equivalent to the solution space to the linear system whose augmented matrix is .  If a linear system of equations has 8 equations and 5 unknowns, then the shape of the matrix in the corresponding equation is .  If has a pivot position in every row, then every equation is consistent.  If is a matrix, then is inconsistent for some vector .     False  True  False  True  True     False, the number of components of equals the number of columns of .  True. This is a result of .  False. The shape of is .  True, because the augmented matrix cannot have a pivot position in the rightmost column.  True. Because there is not a pivot position in every row of , the augmented matrix will have a pivot position in the rightmost column for some vectors .   "
},
{
  "id": "exercise-39",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-39",
  "type": "Exercise",
  "number": "2.2.6.10",
  "title": "",
  "body": " Suppose that is a matrix and that the equation has a unique solution for some vector .  What does this say about the pivot positions of the matrix ? Write the reduced row echelon form of .  Can you find another vector such that is inconsistent?  What can you say about the solution space to the equation ?  Suppose . Explain why every four-dimensional vector can be written as a linear combination of the vectors , , , and in exactly one way.       .  No   .  Every equation has exactly one solution.     Since the solution is unique, there must be a pivot position in every column of . This means that there are four pivot positions so there is a pivot position in every row. In other words, .  No, because has a pivot in every row.  There is only one solution, which is .  If is a four-dimensional vector, then has a unique solution since has a pivot position in every row and every column. This means that can be written as a linear combination of the columns of in exactly one way.   "
},
{
  "id": "exercise-40",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-40",
  "type": "Exercise",
  "number": "2.2.6.11",
  "title": "",
  "body": " Define the matrix .  Describe the solution space to the homogeneous equation using a parametric description, if appropriate. What does this solution space represent geometrically?   Describe the solution space to the equation where . What does this solution space represent geometrically and how does it compare to the previous solution space?   We will now explain the relationship between the previous two solution spaces. Suppose that is a solution to the homogeneous equation; that is . Suppose also that is a solution to the equation ; that is, .  Use the Linearity Principle expressed in to explain why is a solution to the equation . You may do this by evaluating .  That is, if we find one solution to an equation , we may add any solution to the homogeneous equation to and still have a solution to the equation . In other words, the solution space to the equation is given by translating the solution space to the homogeneous equation by the vector .      .   .  Notice that .     We have the augmented matrix so that . The solution space is a line that passes through the origin.  Now we have so that . This is a line through parallel to . Notice that this line is parallel to the line in part a.  Notice that . This shows that if we add a vector from the solution space to to a vector from the solution space to , we obtain a vector in the solution space to .   "
},
{
  "id": "exercise-41",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-41",
  "type": "Exercise",
  "number": "2.2.6.12",
  "title": "",
  "body": " Suppose that a city is starting a bicycle sharing program with bicycles at locations and . Bicycles that are rented at one location may be returned to either location at the end of the day. Over time, the city finds that 80% of bicycles rented at location are returned to with the other 20% returned to . Similarly, 50% of bicycles rented at location are returned to and 50% to .  To keep track of the bicycles, we form a vector where is the number of bicycles at location at the beginning of day and is the number of bicycles at . The information above tells us how to determine the distribution of bicycles the following day: Expressed in matrix-vector form, these expressions give where .   Let's check that this makes sense.  Suppose that there are 1000 bicycles at location and none at on day 1. This means we have . Find the number of bicycles at both locations on day 2 by evaluating .  Suppose that there are 1000 bicycles at location and none at on day 1. Form the vector and determine the number of bicycles at the two locations the next day by finding .     Suppose that one day there are 1050 bicycles at location and 450 at location . How many bicycles were there at each location the previous day?    Suppose that there are 500 bicycles at location and 500 at location on Monday. How many bicycles are there at the two locations on Tuesday? on Wednesday? on Thursday?     We see that   .   .     .   represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.     We see that   .   .    We solve the equation to obtain .  If represents the distribution of bicycles on Monday, then represents the distribution of bicycles on Tuesday, represents the distribution of bicycles on Wednesday, and on Thursday.   "
},
{
  "id": "exercise-42",
  "level": "2",
  "url": "sec-matrices-lin-combs.html#exercise-42",
  "type": "Exercise",
  "number": "2.2.6.13",
  "title": "",
  "body": " This problem is a continuation of the previous problem.  Let us define vectors . Show that .  Suppose that where and are scalars. Use the Linearity Principle expressed in to explain why .  Continuing in this way, explain why .   Suppose that there are initially 500 bicycles at location and 500 at location . Write the vector and find the scalars and such that .  Use the previous part of this problem to determine , and .  After a very long time, how are all the bicycles distributed?      and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so       and .  We have    and so on.  We find that .  Then   After a very long time, the second term becomes increasingly close to zero and so    "
},
{
  "id": "sec-span",
  "level": "1",
  "url": "sec-span.html",
  "type": "Section",
  "number": "2.3",
  "title": "The span of a set of vectors",
  "body": " The span of a set of vectors   Matrix multiplication allows us to rewrite a linear system in the form . Besides being a more compact way of expressing a linear system, this form allows us to think about linear systems geometrically since matrix multiplication is defined in terms of linear combinations of vectors.  We now return to our two fundamental questions, rephrased here in terms of matrix multiplication.   Existence: Is there a solution to the equation ?    Uniqueness: If there is a solution to the equation , is it unique?  In this section, we focus on the existence question and see how it leads to the concept of the span of a set of vectors.   The existence of solutions      If the equation is inconsistent, what can we say about the pivot positions of the augmented matrix ?    Consider the matrix  . If , is the equation consistent? If so, find a solution.    If , is the equation consistent? If so, find a solution.    Identify the pivot positions of .   For our two choices of the vector , one equation has a solution and the other does not. What feature of the pivot positions of the matrix tells us to expect this?       We know there must be a pivot position in the rightmost column of the augmented matrix.  We construct the augmented matrix which shows that the system is consistent. The solution space is described parametrically as   Now the augmented matrix is showing that the equation is inconsistent.  There are two pivot positions in , as shown.   Since there is a row of that does not have a pivot position, it is possible to augment by a vector so that we obtain a pivot position in the rightmost column of the augmented matrix. In this case, we have an inconsistent system.       The span of a set of vectors  In the preview activity, we considered a matrix and found that the equation has a solution for some vectors in and has no solution for others. We will introduce a concept called span that describes the vectors for which there is a solution.  We can write an matrix in terms of its columns . Remember that says that the equation is consistent if and only if we can express as a linear combination of .   span  The span of a set of vectors is the set of all linear combinations that can be formed from the vectors.  Alternatively, if , then the span of the vectors consists of all vectors for which the equation is consistent.     Considering the set of vectors and , we see that the vector is one vector in the span of the vectors and because it is a linear combination of and .  To determine whether the vector is in the span of and , we form the matrix and consider the equation . We have which shows that the equation is inconsistent. Therefore, is one vector that is not in the span of and .      Let's look at two examples to develop some intuition for the concept of span.  First, we will consider the set of vectors .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?      We will now look at an example where .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?         For the first set of vectors, we find:  We can form the linear combinations:  When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes, we saw that there are at least two ways. For instance, when the weights are and . This means that is in the span of and .  No. No matter how we change the weights, the linear combination lies on the line through and . This means that is not in the span of and .  The span of and is the set of all vectors on the line through .  If the equation has a solution, must lie on the line defined by .    For the second set of vectors, we have:  We can form the linear combinations:   When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes. Using the diagram, we see that . This means that is in the span of and .  Yes. Using the diagram, we see that . This means that is in the span of and .  Every two-dimensional vector is in the span of and .  The equation has a solution for every .      This activity aims to convey the geometric meaning of span. Remember that we can think of a linear combination of the two vectors and as a recipe for walking in the plane . We first move a prescribed amount in the direction of and then a prescribed amount in the direction of . The span consists of all the places we can walk to.    Let's consider the vectors and as shown in .   The vectors and and some linear combinations they create.      The figure shows us that is a linear combination of and . Indeed, we can verify this algebraically by constructing the linear system whose corresponding augmented matrix has the reduced row echelon form Because this system is consistent, we know that is in the span of and .  In fact, we can say more. Notice that the coefficient matrix has a pivot position in every row. This means that for any other vector , the augmented matrix corresponding to the equation cannot have a pivot position in its rightmost column: Therefore, the equation is consistent for every two-dimensional vector , which tells us that every two-dimensional vector is in the span of and . In this case, we say that the span of and is .  The intuitive meaning is that we can walk to any point in the plane by moving an appropriate distance in the and directions.      Now let's consider the vectors and as shown in .   The vectors and and some linear combinations they create.      From the figure, we expect that is not a linear combination of and . Once again, we can verify this algebraically by constructing the linear system The augmented matrix has the reduced row echelon form from which we see that the system is inconsistent. Therefore, is not in the span of and .  We should expect this behavior from the coefficient matrix Because the second row of the coefficient matrix does not have a pivot position, it is possible for a linear system to have a pivot position in its rightmost column:   If we notice that , we see that any linear combination of and , is actually a scalar multiple of . Therefore, the span of and is the line defined by the vector . Intuitively, this means that we can only walk to points on this line using these two vectors.     We will denote the span of the set of vectors by .   In , we saw that . However, for the vectors in , we saw that is simply a line.    Pivot positions and span  A set of vectors naturally defines a matrix whose columns are the given vectors. As we've seen, a vector is in precisely when the linear system is consistent.  The previous examples point to the fact that the span is related to the pivot positions of . While and develop this idea more fully, we will now examine the possibilities in .    In this activity, we will look at the span of sets of vectors in .   Suppose . Give a geometric description of and a rough sketch of and its span in .   A three-dimensional coordinate system for sketching and its span.        Now consider the two vectors . Sketch the vectors below. Then give a geometric description of and a rough sketch of the span in .   A coordinate system for sketching , , and .       Let's now look at this situation algebraically by writing write . Determine the conditions on , , and so that is in by considering the linear system or . Explain how this relates to your sketch of .   Consider the vectors    Is the vector in ?    Is the vector in ?   Give a geometric description of .    Consider the vectors . Form the matrix and find its reduced row echelon form. What does this tell you about ?   If the span of a set of vectors is , what can you say about the pivot positions of the matrix ?  What is the smallest number of vectors such that ?       is the line defined by .   is the -plane.   For the linear system to be consistent, we need , which means that the third coordinate of the vector must be 0 for to be in . In other words, must lie in the -plane.   We consider the two cases.  We have the augmented matrix which shows that the system is inconsistent. Therefore, is not in .  We have the augmented matrix which shows that the system is consistent. Therefore, is in .  The span is the plane in defined by and .   We have the reduced row echelon form Since there is a pivot position in every row, this says that every equation is consistent. The is therefore .  There must be a pivot position in every row.  If a set of vectors spans , its corresponding matrix must have a pivot position in every row. Because there can be at most one pivot position in a column, there must be at least three columns. Therefore, the smallest number of vectors that span is three.     The types of sets that appear as the span of a set of vectors in are relatively simple.   First, with a single nonzero vector, all linear combinations are simply scalar multiples of that vector so that the span of this vector is a line, as shown in .   The span of a single nonzero vector is a line.      Notice that the matrix formed by this vector has one pivot position. For example, .    The span of two vectors in that do not lie on the same line will be a plane, as seen in .   The span of these two vectors in is a plane.      For example, the vectors lead to the matrix with two pivot positions.   Finally, a set of three vectors, such as may form a matrix having three pivot positions one in every row. When this happens, no matter how we augment this matrix, it is impossible to obtain a pivot position in the rightmost column: Therefore, any linear system is consistent, which tells us that .    To summarize, we looked at the pivot positions in a matrix whose columns are the three-dimensional vectors . We found that with   one pivot position, the span was a line.    two pivot positions, the span was a plane.   three pivot positions, the span was .  Though we will return to these ideas later, for now take note of the fact that the span of a set of vectors in is a relatively simple, familiar geometric object.  The reasoning that led us to conclude that the span of a set of vectors is when the associated matrix has a pivot position in every row applies more generally.    Suppose we have vectors in . Then if and only if the matrix has a pivot position in every row.    This tells us something important about the number of vectors needed to span . Suppose we have vectors that span . The proposition tells us that the matrix has a pivot position in every row, such as in this reduced row echelon matrix. Since a matrix can have at most one pivot position in a column, there must be at least as many columns as there are rows, which implies that . For instance, if we have a set of vectors that span , there must be at least 632 vectors in the set.    A set of vectors whose span is contains at least vectors.    We have thought about a linear combination of a set of vectors as the result of walking a certain distance in the direction of , followed by walking a certain distance in the direction of , and so on. If , this means that we can walk to every point in using the directions . Intuitively, this proposition is telling us that we need at least directions to have the flexibility needed to reach every point in .   Terminology  Because span is a concept that is connected to a set of vectors, we say, The span of the set of vectors is .... While it may be tempting to say, The span of the matrix is ..., we should instead say The span of the columns of the matrix is ....      Summary  We defined the span of a set of vectors and developed some intuition for this concept through a series of examples.   The span of a set of vectors is the set of linear combinations of the vectors. We denote the span by .   A vector is in if and only if the linear system is consistent.   If the matrix has a pivot position in every row, then the span of these vectors is ; that is,   Any set of vectors that spans must have at least vectors.      In this exercise, we will consider the span of some sets of two- and three-dimensional vectors.   Consider the vectors .  Is in ?  Give a geometric description of .    Consider the vectors .  Is the vector in ?  Is the vector in ?   Is the vector in ?  Give a geometric description of .         .  For the following vectors,   is in .   is in .   is not in .   is a plane in .      The equation is consistent so is in . Since has a pivot position in every row, is .  Let's consider the following vectors.  The equation is consistent so is in .  The vector is in because .  The equation is not consistent so is not in .   is the plane defined by and .      Provide a justification for your response to the following questions.  Suppose you have a set of vectors . Can you guarantee that is in ?  Suppose that is an matrix. Can you guarantee that the equation is consistent?   What is ?      Yes  Yes   consists only of the vector .     Yes, because .  Yes, is a solution to the equation .   consists only of the vector .     For both parts of this exercise, give a geometric description of sets of the vectors and include a sketch.   For which vectors in is the equation consistent?   For which vectors in is the equation consistent?       must be a scalar multiple of .   can be any vector in .      so must be on the line defined by .   so can be any vector in .     Consider the following matrices: . Do the columns of span ? Do the columns of span ?    The columns of do not span . The columns of do.    The reduced row echelon form of is so the columns of do not span .  The reduced row echelon form of is so the columns of span .     Determine whether the following statements are true or false and provide a justification for your response. Throughout, we will assume that the matrix has columns ; that is, .  If the equation is consistent, then is in .   The equation is consistent.  If , , , and are vectors in , then their span is .  If is a linear combination of , then is in .  If is an matrix, then the span of the columns of is a set of vectors in .      True   True   False   True   False     True. If is a solution to , then the components of are weights whose linear combination is .  True, because is a solution.  False. The span could be a smaller set.  True. This is the definition of the span.  False. The span is a set of vectors in .     This exercise asks you to construct some matrices whose columns span a given set.  Construct a matrix whose columns span .  Construct a matrix whose columns span a plane in .  Construct a matrix whose columns span a line in .     We will choose matrices in reduced row echelon form.   .   .   .    We will choose matrices in reduced row echelon form.   .   .   .     Provide a justification for your response to the following questions.  Suppose that we have vectors in , , whose span is . Can every vector in be written as a linear combination of ?   Suppose that we have vectors in , , whose span is . Can every vector in be written uniquely as a linear combination of ?   Do the vectors span ?  Suppose that span . What can you guarantee about the value of ?  Can 17 vectors in span ?     Yes  No  Yes     No     Yes. Because the span is , every vector in can be written as a linear combination of the vectors.  No. The matrix must have a column without a pivot position. Therefore, any equation has infinitely many solutions.  Yes, because has a pivot position in every row.  There must be at least 438 vectors so .  No, because we need at least 20 vectors to span .     The following observation will be helpful in this exercise. If we want to find a solution to the equation , we could first find a solution to the equation and then find a solution to the equation .  Suppose that is a matrix whose columns span and is a matrix. In this case, we can form the product .  What is the shape of the product ?  Can you guarantee that the columns of span ?  If you know additionally that the span of the columns of is , can you guarantee that the columns of span ?         No  Yes      is a matrix.  No. Since the columns of span , then the equation has a solution for every vector . However, we may not be able to solve the equation if the columns of do not span .  Yes. If we are given a vector in , we can find a vector such that since the columns of span . Since the columns of span , we can find a vector such that . Then , which means that is the span of the columns of .     Suppose that is a matrix and that, for some vector , the equation has a unique solution.  What can you say about the pivot positions of ?  What can you say about the span of the columns of ?  If is some other vector in , what can you conclude about the equation ?  What can you about the solution space to the equation ?     There is a pivot position in each row and each column.  The span is .  There is a unique solution.  There is a unique solution      Since the solution is unique, each column of must have a pivot position. Since there are also 12 rows, each row must have a pivot position as well. This means that the row reduced echelon form of is the identity matrix.  Since there is a pivot position in every row, the span of the columns of is .  There is a unique solution to the equation because has a pivot position in every row and every column.  There is a unique solution .     Suppose that .  Is a linear combination of and ? If so, find weights such that .  Show that a linear combination can be rewritten as a linear combination of and .  Explain why .      Yes  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .     Yes, because .  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .     As defined in this section, the span of a set of vectors is generated by taking all possible linear combinations of those vectors. This exercise will demonstrate the fact that the span can also be realized as the solution space to a linear system.  We will consider the vectors    Is every vector in in ? If not, describe the span.    To describe as the solution space of a linear system, we will write . If is in , then the linear system corresponding to the augmented matrix must be consistent. This means that a pivot cannot occur in the rightmost column. Perform row operations to put this augmented matrix into a triangular form. Now identify an equation in , , and that tells us when there is no pivot in the rightmost column. The solution space to this equation describes .   In this example, the matrix formed by the vectors has two pivot positions. Suppose we were to consider another example in which this matrix had had only one pivot position. How would this have changed the linear system describing ?      is the plane defined by and .   .  There would be two equations involving the variables , , and .     Finding the reduced row echelon form shows that is a linear combination of and . Therefore, is the plane defined by and .  We see that If the equation is consistent, we must have . The solution space is the plane defined by and .  In that case, there would be two equations involving the variables , , and , and the solution space would be a line.     "
},
{
  "id": "exploration-5",
  "level": "2",
  "url": "sec-span.html#exploration-5",
  "type": "Preview Activity",
  "number": "2.3.1",
  "title": "The existence of solutions.",
  "body": " The existence of solutions      If the equation is inconsistent, what can we say about the pivot positions of the augmented matrix ?    Consider the matrix  . If , is the equation consistent? If so, find a solution.    If , is the equation consistent? If so, find a solution.    Identify the pivot positions of .   For our two choices of the vector , one equation has a solution and the other does not. What feature of the pivot positions of the matrix tells us to expect this?       We know there must be a pivot position in the rightmost column of the augmented matrix.  We construct the augmented matrix which shows that the system is consistent. The solution space is described parametrically as   Now the augmented matrix is showing that the equation is inconsistent.  There are two pivot positions in , as shown.   Since there is a row of that does not have a pivot position, it is possible to augment by a vector so that we obtain a pivot position in the rightmost column of the augmented matrix. In this case, we have an inconsistent system.    "
},
{
  "id": "definition-9",
  "level": "2",
  "url": "sec-span.html#definition-9",
  "type": "Definition",
  "number": "2.3.1",
  "title": "",
  "body": " span  The span of a set of vectors is the set of all linear combinations that can be formed from the vectors.  Alternatively, if , then the span of the vectors consists of all vectors for which the equation is consistent.  "
},
{
  "id": "example-8",
  "level": "2",
  "url": "sec-span.html#example-8",
  "type": "Example",
  "number": "2.3.2",
  "title": "",
  "body": "  Considering the set of vectors and , we see that the vector is one vector in the span of the vectors and because it is a linear combination of and .  To determine whether the vector is in the span of and , we form the matrix and consider the equation . We have which shows that the equation is inconsistent. Therefore, is one vector that is not in the span of and .   "
},
{
  "id": "activity-intro-span",
  "level": "2",
  "url": "sec-span.html#activity-intro-span",
  "type": "Activity",
  "number": "2.3.2",
  "title": "",
  "body": "  Let's look at two examples to develop some intuition for the concept of span.  First, we will consider the set of vectors .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?      We will now look at an example where .   An interactive diagram for constructing linear combinations of the vectors and .      What vector is the linear combination of and with weights:    and ?    and ?    and ?      Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Can the vector be expressed as a linear combination of and ? Is the vector in the span of and ?   Describe the set of vectors in the span of and .   For what vectors does the equation have a solution?         For the first set of vectors, we find:  We can form the linear combinations:  When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes, we saw that there are at least two ways. For instance, when the weights are and . This means that is in the span of and .  No. No matter how we change the weights, the linear combination lies on the line through and . This means that is not in the span of and .  The span of and is the set of all vectors on the line through .  If the equation has a solution, must lie on the line defined by .    For the second set of vectors, we have:  We can form the linear combinations:   When and , the linear combination is .   When and , the linear combination is .   When and , the linear combination is .    Yes. Using the diagram, we see that . This means that is in the span of and .  Yes. Using the diagram, we see that . This means that is in the span of and .  Every two-dimensional vector is in the span of and .  The equation has a solution for every .     "
},
{
  "id": "example-span-linear-indep",
  "level": "2",
  "url": "sec-span.html#example-span-linear-indep",
  "type": "Example",
  "number": "2.3.5",
  "title": "",
  "body": "  Let's consider the vectors and as shown in .   The vectors and and some linear combinations they create.      The figure shows us that is a linear combination of and . Indeed, we can verify this algebraically by constructing the linear system whose corresponding augmented matrix has the reduced row echelon form Because this system is consistent, we know that is in the span of and .  In fact, we can say more. Notice that the coefficient matrix has a pivot position in every row. This means that for any other vector , the augmented matrix corresponding to the equation cannot have a pivot position in its rightmost column: Therefore, the equation is consistent for every two-dimensional vector , which tells us that every two-dimensional vector is in the span of and . In this case, we say that the span of and is .  The intuitive meaning is that we can walk to any point in the plane by moving an appropriate distance in the and directions.   "
},
{
  "id": "example-span-linear-dep",
  "level": "2",
  "url": "sec-span.html#example-span-linear-dep",
  "type": "Example",
  "number": "2.3.7",
  "title": "",
  "body": "  Now let's consider the vectors and as shown in .   The vectors and and some linear combinations they create.      From the figure, we expect that is not a linear combination of and . Once again, we can verify this algebraically by constructing the linear system The augmented matrix has the reduced row echelon form from which we see that the system is inconsistent. Therefore, is not in the span of and .  We should expect this behavior from the coefficient matrix Because the second row of the coefficient matrix does not have a pivot position, it is possible for a linear system to have a pivot position in its rightmost column:   If we notice that , we see that any linear combination of and , is actually a scalar multiple of . Therefore, the span of and is the line defined by the vector . Intuitively, this means that we can only walk to points on this line using these two vectors.   "
},
{
  "id": "convention-1",
  "level": "2",
  "url": "sec-span.html#convention-1",
  "type": "Notation",
  "number": "2.3.9",
  "title": "",
  "body": " We will denote the span of the set of vectors by .  "
},
{
  "id": "activity-span-r3",
  "level": "2",
  "url": "sec-span.html#activity-span-r3",
  "type": "Activity",
  "number": "2.3.3",
  "title": "",
  "body": "  In this activity, we will look at the span of sets of vectors in .   Suppose . Give a geometric description of and a rough sketch of and its span in .   A three-dimensional coordinate system for sketching and its span.        Now consider the two vectors . Sketch the vectors below. Then give a geometric description of and a rough sketch of the span in .   A coordinate system for sketching , , and .       Let's now look at this situation algebraically by writing write . Determine the conditions on , , and so that is in by considering the linear system or . Explain how this relates to your sketch of .   Consider the vectors    Is the vector in ?    Is the vector in ?   Give a geometric description of .    Consider the vectors . Form the matrix and find its reduced row echelon form. What does this tell you about ?   If the span of a set of vectors is , what can you say about the pivot positions of the matrix ?  What is the smallest number of vectors such that ?       is the line defined by .   is the -plane.   For the linear system to be consistent, we need , which means that the third coordinate of the vector must be 0 for to be in . In other words, must lie in the -plane.   We consider the two cases.  We have the augmented matrix which shows that the system is inconsistent. Therefore, is not in .  We have the augmented matrix which shows that the system is consistent. Therefore, is in .  The span is the plane in defined by and .   We have the reduced row echelon form Since there is a pivot position in every row, this says that every equation is consistent. The is therefore .  There must be a pivot position in every row.  If a set of vectors spans , its corresponding matrix must have a pivot position in every row. Because there can be at most one pivot position in a column, there must be at least three columns. Therefore, the smallest number of vectors that span is three.    "
},
{
  "id": "fig-span-line",
  "level": "2",
  "url": "sec-span.html#fig-span-line",
  "type": "Figure",
  "number": "2.3.12",
  "title": "",
  "body": " The span of a single nonzero vector is a line.     "
},
{
  "id": "fig-span-plane",
  "level": "2",
  "url": "sec-span.html#fig-span-plane",
  "type": "Figure",
  "number": "2.3.13",
  "title": "",
  "body": " The span of these two vectors in is a plane.     "
},
{
  "id": "prop-pivot-row",
  "level": "2",
  "url": "sec-span.html#prop-pivot-row",
  "type": "Proposition",
  "number": "2.3.14",
  "title": "",
  "body": "  Suppose we have vectors in . Then if and only if the matrix has a pivot position in every row.   "
},
{
  "id": "prop-span-bound",
  "level": "2",
  "url": "sec-span.html#prop-span-bound",
  "type": "Proposition",
  "number": "2.3.15",
  "title": "",
  "body": "  A set of vectors whose span is contains at least vectors.   "
},
{
  "id": "exercise-43",
  "level": "2",
  "url": "sec-span.html#exercise-43",
  "type": "Exercise",
  "number": "2.3.4.1",
  "title": "",
  "body": " In this exercise, we will consider the span of some sets of two- and three-dimensional vectors.   Consider the vectors .  Is in ?  Give a geometric description of .    Consider the vectors .  Is the vector in ?  Is the vector in ?   Is the vector in ?  Give a geometric description of .         .  For the following vectors,   is in .   is in .   is not in .   is a plane in .      The equation is consistent so is in . Since has a pivot position in every row, is .  Let's consider the following vectors.  The equation is consistent so is in .  The vector is in because .  The equation is not consistent so is not in .   is the plane defined by and .    "
},
{
  "id": "exercise-44",
  "level": "2",
  "url": "sec-span.html#exercise-44",
  "type": "Exercise",
  "number": "2.3.4.2",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose you have a set of vectors . Can you guarantee that is in ?  Suppose that is an matrix. Can you guarantee that the equation is consistent?   What is ?      Yes  Yes   consists only of the vector .     Yes, because .  Yes, is a solution to the equation .   consists only of the vector .   "
},
{
  "id": "exercise-45",
  "level": "2",
  "url": "sec-span.html#exercise-45",
  "type": "Exercise",
  "number": "2.3.4.3",
  "title": "",
  "body": " For both parts of this exercise, give a geometric description of sets of the vectors and include a sketch.   For which vectors in is the equation consistent?   For which vectors in is the equation consistent?       must be a scalar multiple of .   can be any vector in .      so must be on the line defined by .   so can be any vector in .   "
},
{
  "id": "exercise-46",
  "level": "2",
  "url": "sec-span.html#exercise-46",
  "type": "Exercise",
  "number": "2.3.4.4",
  "title": "",
  "body": " Consider the following matrices: . Do the columns of span ? Do the columns of span ?    The columns of do not span . The columns of do.    The reduced row echelon form of is so the columns of do not span .  The reduced row echelon form of is so the columns of span .   "
},
{
  "id": "exercise-47",
  "level": "2",
  "url": "sec-span.html#exercise-47",
  "type": "Exercise",
  "number": "2.3.4.5",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response. Throughout, we will assume that the matrix has columns ; that is, .  If the equation is consistent, then is in .   The equation is consistent.  If , , , and are vectors in , then their span is .  If is a linear combination of , then is in .  If is an matrix, then the span of the columns of is a set of vectors in .      True   True   False   True   False     True. If is a solution to , then the components of are weights whose linear combination is .  True, because is a solution.  False. The span could be a smaller set.  True. This is the definition of the span.  False. The span is a set of vectors in .   "
},
{
  "id": "exercise-48",
  "level": "2",
  "url": "sec-span.html#exercise-48",
  "type": "Exercise",
  "number": "2.3.4.6",
  "title": "",
  "body": " This exercise asks you to construct some matrices whose columns span a given set.  Construct a matrix whose columns span .  Construct a matrix whose columns span a plane in .  Construct a matrix whose columns span a line in .     We will choose matrices in reduced row echelon form.   .   .   .    We will choose matrices in reduced row echelon form.   .   .   .   "
},
{
  "id": "exercise-49",
  "level": "2",
  "url": "sec-span.html#exercise-49",
  "type": "Exercise",
  "number": "2.3.4.7",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose that we have vectors in , , whose span is . Can every vector in be written as a linear combination of ?   Suppose that we have vectors in , , whose span is . Can every vector in be written uniquely as a linear combination of ?   Do the vectors span ?  Suppose that span . What can you guarantee about the value of ?  Can 17 vectors in span ?     Yes  No  Yes     No     Yes. Because the span is , every vector in can be written as a linear combination of the vectors.  No. The matrix must have a column without a pivot position. Therefore, any equation has infinitely many solutions.  Yes, because has a pivot position in every row.  There must be at least 438 vectors so .  No, because we need at least 20 vectors to span .   "
},
{
  "id": "exercise-50",
  "level": "2",
  "url": "sec-span.html#exercise-50",
  "type": "Exercise",
  "number": "2.3.4.8",
  "title": "",
  "body": " The following observation will be helpful in this exercise. If we want to find a solution to the equation , we could first find a solution to the equation and then find a solution to the equation .  Suppose that is a matrix whose columns span and is a matrix. In this case, we can form the product .  What is the shape of the product ?  Can you guarantee that the columns of span ?  If you know additionally that the span of the columns of is , can you guarantee that the columns of span ?         No  Yes      is a matrix.  No. Since the columns of span , then the equation has a solution for every vector . However, we may not be able to solve the equation if the columns of do not span .  Yes. If we are given a vector in , we can find a vector such that since the columns of span . Since the columns of span , we can find a vector such that . Then , which means that is the span of the columns of .   "
},
{
  "id": "exercise-51",
  "level": "2",
  "url": "sec-span.html#exercise-51",
  "type": "Exercise",
  "number": "2.3.4.9",
  "title": "",
  "body": " Suppose that is a matrix and that, for some vector , the equation has a unique solution.  What can you say about the pivot positions of ?  What can you say about the span of the columns of ?  If is some other vector in , what can you conclude about the equation ?  What can you about the solution space to the equation ?     There is a pivot position in each row and each column.  The span is .  There is a unique solution.  There is a unique solution      Since the solution is unique, each column of must have a pivot position. Since there are also 12 rows, each row must have a pivot position as well. This means that the row reduced echelon form of is the identity matrix.  Since there is a pivot position in every row, the span of the columns of is .  There is a unique solution to the equation because has a pivot position in every row and every column.  There is a unique solution .   "
},
{
  "id": "exercise-52",
  "level": "2",
  "url": "sec-span.html#exercise-52",
  "type": "Exercise",
  "number": "2.3.4.10",
  "title": "",
  "body": " Suppose that .  Is a linear combination of and ? If so, find weights such that .  Show that a linear combination can be rewritten as a linear combination of and .  Explain why .      Yes  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .     Yes, because .  We have   As seen in the last part of this problem, every linear combination of , , and can be rewritten as a linear combination of and .   "
},
{
  "id": "exercise-53",
  "level": "2",
  "url": "sec-span.html#exercise-53",
  "type": "Exercise",
  "number": "2.3.4.11",
  "title": "",
  "body": " As defined in this section, the span of a set of vectors is generated by taking all possible linear combinations of those vectors. This exercise will demonstrate the fact that the span can also be realized as the solution space to a linear system.  We will consider the vectors    Is every vector in in ? If not, describe the span.    To describe as the solution space of a linear system, we will write . If is in , then the linear system corresponding to the augmented matrix must be consistent. This means that a pivot cannot occur in the rightmost column. Perform row operations to put this augmented matrix into a triangular form. Now identify an equation in , , and that tells us when there is no pivot in the rightmost column. The solution space to this equation describes .   In this example, the matrix formed by the vectors has two pivot positions. Suppose we were to consider another example in which this matrix had had only one pivot position. How would this have changed the linear system describing ?      is the plane defined by and .   .  There would be two equations involving the variables , , and .     Finding the reduced row echelon form shows that is a linear combination of and . Therefore, is the plane defined by and .  We see that If the equation is consistent, we must have . The solution space is the plane defined by and .  In that case, there would be two equations involving the variables , , and , and the solution space would be a line.   "
},
{
  "id": "sec-linear-dep",
  "level": "1",
  "url": "sec-linear-dep.html",
  "type": "Section",
  "number": "2.4",
  "title": "Linear independence",
  "body": " Linear independence   In the previous section, questions about the existence of solutions of a linear system led to the concept of the span of a set of vectors. In particular, the span of a set of vectors is the set of vectors for which a solution to the linear system exists.  In this section, we turn to the uniqueness of solutions of a linear system, the second of our two fundamental questions . This will lead us to the concept of linear independence.    Let's begin by looking at some sets of vectors in . As we saw in the previous section, the span of a set of vectors in will be either a line, a plane, or itself.   Consider the following vectors in : . Describe the span of these vectors, , as a line, a plane, or .   Now consider the set of vectors: . Describe the span of these vectors, , as a line, a plane, or .   Show that the vector is a linear combination of and by finding weights such that .  Explain why any linear combination of , , and , can be written as a linear combination of and .  Explain why .         We will construct the matrix whose columns are , , and : Because there is a pivot in every row, tells us that .    Similarly, As there are two pivot positions, we see that is a plane in .    We see that which tells us that .    We have     Any linear combination of , , and is itself a linear combination of and .         Linear dependence  We have seen examples where the span of a set of three vectors in is and other examples where the span of three vectors is a plane. We would like to understand the difference between these two situations.    Let's consider the set of three vectors in : Forming the associated matrix gives Because there is a pivot position in every row, tells us that .      Now let's consider the set of three vectors: Forming the associated matrix gives Since the last row does not have a pivot position, we know that the span of these vectors is not but is instead a plane.  In fact, we can say more if we shift our perspective slightly and view this as an augmented matrix: In this way, we see that , which enables us to rewrite any linear combination of these three vectors:   In other words, any linear combination of , , and may be written as a linear combination using only the vectors and . Since the span of a set of vectors is simply the set of their linear combinations, this shows that As a result, adding the vector to the set of vectors and does not change the span.    Before exploring this type of behavior more generally, let's think about it from a geometric point of view. Suppose that we begin with the two vectors and in . The span of these two vectors is a plane in , as seen on the left of .   The span of the vectors , , and .       Because the vector is not a linear combination of and , it provides a direction to move that is independent of and . Adding this third vector therefore forms a set whose span is , as seen on the right of .  Similarly, the span of the vectors and in is also a plane. However, the third vector is a linear combination of and , which means that it already lies in the plane formed by and , as seen in . Since we can already move in this direction using just and , adding to the set does not change the span. As a result, it remains a plane.   The span of the vectors , , and .       What distinguishes these two examples is whether one of the vectors is a linear combination of the others, an observation that leads to the following definition.   linearly independent  linearly dependent   A set of vectors is called linearly dependent if one of the vectors is a linear combination of the others. Otherwise, the set of vectors is called linearly independent .    For the sake of completeness, we say that a set of vectors containing only one nonzero vector is linearly independent.    How to recognize linear dependence    We would like to develop a means to detect when a set of vectors is linearly dependent. This activity will point the way.  Suppose we have five vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of the vectors as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  Suppose we have another set of three vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of these vectors , , as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  By looking at the pivot positions, how can you determine whether the columns of a matrix are linearly dependent or independent?  If one vector in a set is the zero vector , can the set of vectors be linearly independent?  Suppose a set of vectors in has twelve vectors. Is it possible for this set to be linearly independent?      Let's focus on the first three vectors and view the matrix as an augmented one: This shows that so it is possible to write one of the vectors as a linear combination of the others. Therefore, the set is linearly dependent.  Applying the same reasoning as in the previous part, we see that we cannot write any of the vectors as a linear combination of the others. Therefore, the set is linearly independent.  The columns of a matrix are linearly independent exactly when there is a pivot position in every column of the matrix.  No, because we can write the zero vector as a linear combination of the other vectors: .  No, because the matrix formed by the vectors would have 12 columns and only 10 rows. There can be at most 10 pivot positions so there are at least two columns without pivot positions.     By now, we should expect that the pivot positions play an important role in determining whether the columns of a matrix are linearly dependent. For instance, suppose we have four vectors and their associated matrix Since the third column does not contain a pivot position, let's just focus on the first three columns and view them as an augmented matrix: This says that which tells us that the set of vectors is linearly dependent. Moreover, we see that   More generally, the same reasoning implies that a set of vectors is linearly dependent if the associated matrix has a column without a pivot position. Indeed, as illustrated here, a vector corresponding to a column without a pivot position can be expressed as a linear combination of the vectors whose columns do contain pivot positions.  Suppose instead that the matrix associated to a set of vectors has a pivot position in every column. Viewing this as an augmented matrix again, we see that the linear system is inconsistent since there is a pivot in the rightmost column, which means that cannot be expressed as a linear combination of the other vectors. Similarly, cannot be expressed as a linear combination of and . In fact, none of the vectors can be written as a linear combination of the others so this set of vectors is linearly independent.  The following proposition summarizes these findings.    The columns of a matrix are linearly independent if and only if every column contains a pivot position.    This condition imposes a constraint on how many vectors we can have in a linearly independent set. Here is an example of the reduced row echelon form of a matrix whose columns form a set of three linearly independent vectors in : Notice that there are at least as many rows as columns, which must be the case if every column is to have a pivot position.  More generally, if is a linearly independent set of vectors in , the associated matrix must have a pivot position in every column. Since every row contains at most one pivot position, the number of columns can be no greater than the number of rows. This means that the number of vectors in a linearly independent set can be no greater than the number of dimensions.    A linearly independent set of vectors in contains at most vectors.    This says, for instance, that any linearly independent set of vectors in can contain no more three vectors. We usually imagine three independent directions, such as up\/down, front\/back, left\/right, in our three-dimensional world. This proposition tells us that there can be no more independent directions.  The proposition above says that a set of vectors in that is linear independent has at most vectors. By comparison, says that a set of vectors whose span is has at least vectors.    Homogeneous equations  If is a matrix, we call the equation a homogeneous equation. As we'll see, the uniqueness of solutions to this equation reflects on the linear independence of the columns of .   Linear independence and homogeneous equations    Explain why the homogeneous equation is consistent no matter the matrix .  Consider the matrix whose columns we denote by , , and . Describe the solution space of the homogeneous equation using a parametric description, if appropriate.   Find a nonzero solution to the homogeneous equation and use it to find weights , , and such that .  Use the equation you found in the previous part to write one of the vectors as a linear combination of the others.  Are the vectors , , and linearly dependent or independent?      The vector is always a solution.  We have From the reduced row echelon form, we see that is a free variable and that we have The solution space is then written parametrically as   If we set , then we have the solution , which says that   We may rewrite this expression as , showing that is a linear combination of and .   The vectors , , and are linearly dependent, and we know this in two ways. We have seen how to express one vector as a linear combination of the others. Also, we have seen that the associated matrix has a column without a pivot position.      This activity shows how the solution space of the homogeneous equation indicates whether the columns of are linearly dependent or independent. First, we know that the equation always has at least one solution, the vector . Any other solution is a nonzero solution.    Let's consider the vectors and their associated matrix .  The homogeneous equation has the associated augmented matrix Therefore, has a column without a pivot position, which tells us that the vectors , , and are linearly dependent. However, we can also see this fact in another way.  The reduced row echelon matrix tells us that the homogeneous equation has a free variable so that there must be infinitely many solutions. In particular, we have so the solutions have the form   If we choose , then we obtain the nonzero solution to the homogeneous equation , which implies that In other words,   Because is a linear combination of and , we know that this set of vectors is linearly dependent.    As this example demonstrates, there are many ways we can view the question of linear independence, some of which are recorded in the following proposition.    For a matrix , the following statements are equivalent:   The columns of are linearly dependent.   One of the vectors in the set is a linear combination of the others.   The matrix has a column without a pivot position.   The homogeneous equation has infinitely many solutions and hence a nonzero solution.   There are weights , not all of which are zero, such that .        Summary  This section developed the concept of linear dependence of a set of vectors. More specifically, we saw that:  A set of vectors is linearly dependent if one of the vectors is a linear combination of the others.  A set of vectors is linearly independent if and only if the vectors form a matrix that has a pivot position in every column.  A set of linearly independent vectors in contains no more than vectors.  The columns of the matrix are linearly dependent if the homogeneous equation has a nonzero solution.  A set of vectors is linearly dependent if there are weights , not all of which are zero, such that .   At the beginning of the section, we said that this concept addressed the second of our two fundamental questions concerning the uniqueness of solutions to a linear system. It is worth comparing the results of this section with those of the previous one so that the parallels between them become clear.  As usual, we will write a matrix as a collection of vectors,    Span and Linear Independence      Span  Linear independence     A vector is in the span of a set of vectors if it is a linear combination of those vectors.    A set of vectors is linearly dependent if one of the vectors is a linear combination of the others.      A vector is in the span of if there exists a solution to .    The vectors are linearly independent if is the unique solution to .      The columns of an matrix span if the matrix has a pivot position in every row.    The columns of a matrix are linearly independent if the matrix has a pivot position in every column.      A set of vectors that span has at least vectors.    A set of linearly independent vectors in has at most vectors.         Consider the set of vectors .   Explain why this set of vectors is linearly dependent.   Write one of the vectors as a linear combination of the others.    Find weights , , , and , not all of which are zero, such that .   Suppose . Find a nonzero solution to the homogenous equation .     Four vectors in must always be linearly dependent.   .   , , , and         We view the vectors as the columns of a matrix so that Since there is a column without a pivot position, the vectors must be linearly dependent. We also expect this because four vectors in must always be linearly dependent.  If we view the first three columns as an augmented matrix, we have We see that .  From the solution to the previous part, we have , which says that , , , and is an appropriate choice of weights.  From the solution to the previous part, we see that is a solution of the homogeneous equation.     Consider the vectors .   Are these vectors linearly independent or linearly dependent?    Describe the .   Suppose that is a vector in . Explain why we can guarantee that may be written as a linear combination of , , and .   Suppose that is a vector in . In how many ways can be written as a linear combination of , , and ?     Linearly independent.   .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way.     Viewing the vectors as the columns of a matrix, we have Since there is a pivot position in every column, the vectors are linearly independent.  Since the matrix has a pivot position in every row, we see that .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way. Because the matrix has a pivot position in every column, there cannot be a free variable.     Respond to the following questions and provide a justification for your responses.   If the vectors and form a linearly dependent set, must one vector be a scalar multiple of the other?  Suppose that is a linearly independent set of vectors. What can you say about the linear independence or dependence of a subset of these vectors?  Suppose is a linearly independent set of vectors that form the columns of a matrix . If the equation is inconsistent, what can you say about the linear independence or dependence of the set of vectors ?     Yes  Any subset is linearly independent.  They form a linearly independent set.     Yes. If and one of the weights is not zero, we can rearrange this expression to solve for one of the vectors as a scalar multiple of the other. For instance, if , then .  Any subset of the vectors forms a linearly independent set. Because the original set is linearly independent, none of the vectors can be written as a linear combination of the others. This is still true when we look at a smaller set of the vectors.  The set of vectors will also be linearly independent. Since is inconsistent, we know that cannot be written as a linear combination of the other vectors. We also know that each of the vectors cannot be written as a linear combination of the others because is a linearly independent set.     Determine whether the following statements are true or false and provide a justification for your response.  If are linearly dependent, then one vector is a scalar multiple of one of the others.  If are vectors in , then the set of vectors is linearly dependent.  If are vectors in , then the set of vectors is linearly independent.  Suppose we have a set of vectors and that is a scalar multiple of . Then the set is linearly dependent.  Suppose that are linearly independent and form the columns of a matrix . If is consistent, then there is exactly one solution.     False  True  False  True  True     False. We only know that one vector can be written as a linear combination of the others.  True. If we put the vectors into a matrix, there are more columns than rows. Therefore, there must be a column without a pivot position so the vectors form a linearly dependent set.  False. They could form a linearly independent set, but we cannot guarantee it. We would have to look at the location of the pivot positions in the associated matrix.  True. In this case, can be written as a linear combination of the other vectors so the set is linearly dependent.  True. Since the vectors are linearly independent, has a pivot position in every column. Therefore, there is not a free variable in the description of the solution space to the equation . Therefore, the solution is unique.     Suppose we have a set of vectors in that satisfy the relationship: and suppose that is the matrix .  Find a nonzero solution to the equation .  Explain why the matrix has a column without a pivot position.  Write one of the vectors as a linear combination of the others.  Explain why the set of vectors is linearly dependent.        There are infinitely many solutions to the homogeneous equation .   .  One vector can be written as a linear combination of the others.     The vector is a solution.  There is a nonzero solution to the homogeneous equation . Since there is also the zero solution , there must be infinitely many solutions. This can only happen when there is a column of that does not have a pivot position.   .  Because one of the vectors can be written as a linear combination of the others, the set of vectors is linearly dependent.     Suppose that is a set of vectors in that form the columns of a matrix .  Suppose that the vectors span . What can you say about the number of vectors in this set?  Suppose instead that the vectors are linearly independent. What can you say about the number of vectors in this set?  Suppose that the vectors are both linearly independent and span . What can you say about the number of vectors in the set?  Assume that the vectors are both linearly independent and span . Given a vector in , what can you say about the solution space to the equation ?     There must be at least 27 vectors.  There must be at most 27 vectors.  There are exactly 27 vectors.  There is exactly one solution.     In this case, the matrix must have a pivot position in evey row. There must be at least 27 vectors for this to be possible.  In this case, the matrix must have a pivot position in every column. There must be at most 27 vectors for this to be possible.  There must be exactly 27 vectors.  There is exactly one solution to the equation because the matrix has a pivot position in every row and every column.     Given below are some descriptions of sets of vectors that form the columns of a matrix . For each description, give a possible reduced row echelon form for or indicate why there is no set of vectors satisfying the description by stating why the required reduced row echelon matrix cannot exist.  A set of 4 linearly independent vectors in .  A set of 4 linearly independent vectors in .  A set of 3 vectors whose span is .  A set of 5 linearly independent vectors in .  A set of 5 vectors whose span is .      .   .  This is not possible.  This is not possible.   .     There must be a pivot position in every column: .  There must be a pivot position in every row. .  This is not possible. The matrix has dimensions so there cannot be a pivot position in every row.  This is not possible. The matrix has dimensions so there cannot be a pivot position in every column.  There must be a pivot position in every row. .     When we explored matrix multiplication in , we saw that some properties that are true for real numbers are not true for matrices. This exercise will investigate that in some more depth.  Suppose that and are two matrices and that . If , what can you say about the linear independence of the columns of ?  Suppose that we have matrices , and such that . We have seen that we cannot generally conclude that . If we assume additionally that is a matrix whose columns are linearly independent, explain why . You may wish to begin by rewriting the equation as .      They are linearly dependent.  The columns of and are all equal.     Since , there is a column of , which we'll call , that is not zero. Since , we know that , which means that is a nonzero solution to the homogenous equation . Therefore, the columns of are linearly dependent.   Since the columns of are linearly independent, the only solution to the homogeneous equation is . Since that , every column of satisfies . Therefore, every column of is the zero vector, which implies that .     Suppose that is an unknown parameter and consider the set of vectors .  For what values of is the set of vectors linearly dependent?  For what values of does the set of vectors span ?      .   .     We construct the matrix If the vectors are linearly dependent, we cannot have a pivot position in the third column. This means that .  If the vectors span , there must be a pivot in the third row, which means that .     Given a set of linearly dependent vectors, we can eliminate some of the vectors to create a smaller, linearly independent set of vectors.  Suppose that is a linear combination of the vectors and . Explain why .  Consider the vectors . Write one of the vectors as a linear combination of the others. Find a set of three vectors whose span is the same as .  Are the three vectors you are left with linearly independent? If not, express one of the vectors as a linear combination of the others and find a set of two vectors whose span is the same as .  Give a geometric description of in as we did in .      Any linear combination of , , and can be rewritten as a linear combination of and .   .   .  It is a plane in .     If is a linear combination of and , then we can write . Then any linear combination of can be rewritten as   We have This shows that and so .  The remaining three vectors are linearly dependent because . We therefore have .  The vectors and are linearly independent so their span is a plane in .     "
},
{
  "id": "exploration-6",
  "level": "2",
  "url": "sec-linear-dep.html#exploration-6",
  "type": "Preview Activity",
  "number": "2.4.1",
  "title": "",
  "body": "  Let's begin by looking at some sets of vectors in . As we saw in the previous section, the span of a set of vectors in will be either a line, a plane, or itself.   Consider the following vectors in : . Describe the span of these vectors, , as a line, a plane, or .   Now consider the set of vectors: . Describe the span of these vectors, , as a line, a plane, or .   Show that the vector is a linear combination of and by finding weights such that .  Explain why any linear combination of , , and , can be written as a linear combination of and .  Explain why .         We will construct the matrix whose columns are , , and : Because there is a pivot in every row, tells us that .    Similarly, As there are two pivot positions, we see that is a plane in .    We see that which tells us that .    We have     Any linear combination of , , and is itself a linear combination of and .      "
},
{
  "id": "example-span-r3",
  "level": "2",
  "url": "sec-linear-dep.html#example-span-r3",
  "type": "Example",
  "number": "2.4.1",
  "title": "",
  "body": "  Let's consider the set of three vectors in : Forming the associated matrix gives Because there is a pivot position in every row, tells us that .   "
},
{
  "id": "example-span-plane",
  "level": "2",
  "url": "sec-linear-dep.html#example-span-plane",
  "type": "Example",
  "number": "2.4.2",
  "title": "",
  "body": "  Now let's consider the set of three vectors: Forming the associated matrix gives Since the last row does not have a pivot position, we know that the span of these vectors is not but is instead a plane.  In fact, we can say more if we shift our perspective slightly and view this as an augmented matrix: In this way, we see that , which enables us to rewrite any linear combination of these three vectors:   In other words, any linear combination of , , and may be written as a linear combination using only the vectors and . Since the span of a set of vectors is simply the set of their linear combinations, this shows that As a result, adding the vector to the set of vectors and does not change the span.   "
},
{
  "id": "figure-span-r3",
  "level": "2",
  "url": "sec-linear-dep.html#figure-span-r3",
  "type": "Figure",
  "number": "2.4.3",
  "title": "",
  "body": " The span of the vectors , , and .      "
},
{
  "id": "figure-span-plane",
  "level": "2",
  "url": "sec-linear-dep.html#figure-span-plane",
  "type": "Figure",
  "number": "2.4.4",
  "title": "",
  "body": " The span of the vectors , , and .      "
},
{
  "id": "definition-10",
  "level": "2",
  "url": "sec-linear-dep.html#definition-10",
  "type": "Definition",
  "number": "2.4.5",
  "title": "",
  "body": " linearly independent  linearly dependent   A set of vectors is called linearly dependent if one of the vectors is a linear combination of the others. Otherwise, the set of vectors is called linearly independent .   "
},
{
  "id": "activity-19",
  "level": "2",
  "url": "sec-linear-dep.html#activity-19",
  "type": "Activity",
  "number": "2.4.2",
  "title": "",
  "body": "  We would like to develop a means to detect when a set of vectors is linearly dependent. This activity will point the way.  Suppose we have five vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of the vectors as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  Suppose we have another set of three vectors in that form the columns of a matrix having reduced row echelon form . Is it possible to write one of these vectors , , as a linear combination of the others? If so, show explicitly how one vector appears as a linear combination of some of the other vectors. Is this set of vectors linearly dependent or independent?  By looking at the pivot positions, how can you determine whether the columns of a matrix are linearly dependent or independent?  If one vector in a set is the zero vector , can the set of vectors be linearly independent?  Suppose a set of vectors in has twelve vectors. Is it possible for this set to be linearly independent?      Let's focus on the first three vectors and view the matrix as an augmented one: This shows that so it is possible to write one of the vectors as a linear combination of the others. Therefore, the set is linearly dependent.  Applying the same reasoning as in the previous part, we see that we cannot write any of the vectors as a linear combination of the others. Therefore, the set is linearly independent.  The columns of a matrix are linearly independent exactly when there is a pivot position in every column of the matrix.  No, because we can write the zero vector as a linear combination of the other vectors: .  No, because the matrix formed by the vectors would have 12 columns and only 10 rows. There can be at most 10 pivot positions so there are at least two columns without pivot positions.    "
},
{
  "id": "proposition-9",
  "level": "2",
  "url": "sec-linear-dep.html#proposition-9",
  "type": "Proposition",
  "number": "2.4.6",
  "title": "",
  "body": "  The columns of a matrix are linearly independent if and only if every column contains a pivot position.   "
},
{
  "id": "prop-linear-indep-bound",
  "level": "2",
  "url": "sec-linear-dep.html#prop-linear-indep-bound",
  "type": "Proposition",
  "number": "2.4.7",
  "title": "",
  "body": "  A linearly independent set of vectors in contains at most vectors.   "
},
{
  "id": "activity-20",
  "level": "2",
  "url": "sec-linear-dep.html#activity-20",
  "type": "Activity",
  "number": "2.4.3",
  "title": "Linear independence and homogeneous equations.",
  "body": " Linear independence and homogeneous equations    Explain why the homogeneous equation is consistent no matter the matrix .  Consider the matrix whose columns we denote by , , and . Describe the solution space of the homogeneous equation using a parametric description, if appropriate.   Find a nonzero solution to the homogeneous equation and use it to find weights , , and such that .  Use the equation you found in the previous part to write one of the vectors as a linear combination of the others.  Are the vectors , , and linearly dependent or independent?      The vector is always a solution.  We have From the reduced row echelon form, we see that is a free variable and that we have The solution space is then written parametrically as   If we set , then we have the solution , which says that   We may rewrite this expression as , showing that is a linear combination of and .   The vectors , , and are linearly dependent, and we know this in two ways. We have seen how to express one vector as a linear combination of the others. Also, we have seen that the associated matrix has a column without a pivot position.     "
},
{
  "id": "example-13",
  "level": "2",
  "url": "sec-linear-dep.html#example-13",
  "type": "Example",
  "number": "2.4.8",
  "title": "",
  "body": "  Let's consider the vectors and their associated matrix .  The homogeneous equation has the associated augmented matrix Therefore, has a column without a pivot position, which tells us that the vectors , , and are linearly dependent. However, we can also see this fact in another way.  The reduced row echelon matrix tells us that the homogeneous equation has a free variable so that there must be infinitely many solutions. In particular, we have so the solutions have the form   If we choose , then we obtain the nonzero solution to the homogeneous equation , which implies that In other words,   Because is a linear combination of and , we know that this set of vectors is linearly dependent.   "
},
{
  "id": "proposition-11",
  "level": "2",
  "url": "sec-linear-dep.html#proposition-11",
  "type": "Proposition",
  "number": "2.4.9",
  "title": "",
  "body": "  For a matrix , the following statements are equivalent:   The columns of are linearly dependent.   One of the vectors in the set is a linear combination of the others.   The matrix has a column without a pivot position.   The homogeneous equation has infinitely many solutions and hence a nonzero solution.   There are weights , not all of which are zero, such that .     "
},
{
  "id": "table-1",
  "level": "2",
  "url": "sec-linear-dep.html#table-1",
  "type": "Table",
  "number": "2.4.10",
  "title": "Span and Linear Independence",
  "body": " Span and Linear Independence      Span  Linear independence     A vector is in the span of a set of vectors if it is a linear combination of those vectors.    A set of vectors is linearly dependent if one of the vectors is a linear combination of the others.      A vector is in the span of if there exists a solution to .    The vectors are linearly independent if is the unique solution to .      The columns of an matrix span if the matrix has a pivot position in every row.    The columns of a matrix are linearly independent if the matrix has a pivot position in every column.      A set of vectors that span has at least vectors.    A set of linearly independent vectors in has at most vectors.     "
},
{
  "id": "exercise-54",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-54",
  "type": "Exercise",
  "number": "2.4.5.1",
  "title": "",
  "body": " Consider the set of vectors .   Explain why this set of vectors is linearly dependent.   Write one of the vectors as a linear combination of the others.    Find weights , , , and , not all of which are zero, such that .   Suppose . Find a nonzero solution to the homogenous equation .     Four vectors in must always be linearly dependent.   .   , , , and         We view the vectors as the columns of a matrix so that Since there is a column without a pivot position, the vectors must be linearly dependent. We also expect this because four vectors in must always be linearly dependent.  If we view the first three columns as an augmented matrix, we have We see that .  From the solution to the previous part, we have , which says that , , , and is an appropriate choice of weights.  From the solution to the previous part, we see that is a solution of the homogeneous equation.   "
},
{
  "id": "exercise-55",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-55",
  "type": "Exercise",
  "number": "2.4.5.2",
  "title": "",
  "body": " Consider the vectors .   Are these vectors linearly independent or linearly dependent?    Describe the .   Suppose that is a vector in . Explain why we can guarantee that may be written as a linear combination of , , and .   Suppose that is a vector in . In how many ways can be written as a linear combination of , , and ?     Linearly independent.   .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way.     Viewing the vectors as the columns of a matrix, we have Since there is a pivot position in every column, the vectors are linearly independent.  Since the matrix has a pivot position in every row, we see that .  Because , every vector in can be written as a linear combination of , , and .  In exactly one way. Because the matrix has a pivot position in every column, there cannot be a free variable.   "
},
{
  "id": "exercise-56",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-56",
  "type": "Exercise",
  "number": "2.4.5.3",
  "title": "",
  "body": " Respond to the following questions and provide a justification for your responses.   If the vectors and form a linearly dependent set, must one vector be a scalar multiple of the other?  Suppose that is a linearly independent set of vectors. What can you say about the linear independence or dependence of a subset of these vectors?  Suppose is a linearly independent set of vectors that form the columns of a matrix . If the equation is inconsistent, what can you say about the linear independence or dependence of the set of vectors ?     Yes  Any subset is linearly independent.  They form a linearly independent set.     Yes. If and one of the weights is not zero, we can rearrange this expression to solve for one of the vectors as a scalar multiple of the other. For instance, if , then .  Any subset of the vectors forms a linearly independent set. Because the original set is linearly independent, none of the vectors can be written as a linear combination of the others. This is still true when we look at a smaller set of the vectors.  The set of vectors will also be linearly independent. Since is inconsistent, we know that cannot be written as a linear combination of the other vectors. We also know that each of the vectors cannot be written as a linear combination of the others because is a linearly independent set.   "
},
{
  "id": "exercise-57",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-57",
  "type": "Exercise",
  "number": "2.4.5.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If are linearly dependent, then one vector is a scalar multiple of one of the others.  If are vectors in , then the set of vectors is linearly dependent.  If are vectors in , then the set of vectors is linearly independent.  Suppose we have a set of vectors and that is a scalar multiple of . Then the set is linearly dependent.  Suppose that are linearly independent and form the columns of a matrix . If is consistent, then there is exactly one solution.     False  True  False  True  True     False. We only know that one vector can be written as a linear combination of the others.  True. If we put the vectors into a matrix, there are more columns than rows. Therefore, there must be a column without a pivot position so the vectors form a linearly dependent set.  False. They could form a linearly independent set, but we cannot guarantee it. We would have to look at the location of the pivot positions in the associated matrix.  True. In this case, can be written as a linear combination of the other vectors so the set is linearly dependent.  True. Since the vectors are linearly independent, has a pivot position in every column. Therefore, there is not a free variable in the description of the solution space to the equation . Therefore, the solution is unique.   "
},
{
  "id": "exercise-58",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-58",
  "type": "Exercise",
  "number": "2.4.5.5",
  "title": "",
  "body": " Suppose we have a set of vectors in that satisfy the relationship: and suppose that is the matrix .  Find a nonzero solution to the equation .  Explain why the matrix has a column without a pivot position.  Write one of the vectors as a linear combination of the others.  Explain why the set of vectors is linearly dependent.        There are infinitely many solutions to the homogeneous equation .   .  One vector can be written as a linear combination of the others.     The vector is a solution.  There is a nonzero solution to the homogeneous equation . Since there is also the zero solution , there must be infinitely many solutions. This can only happen when there is a column of that does not have a pivot position.   .  Because one of the vectors can be written as a linear combination of the others, the set of vectors is linearly dependent.   "
},
{
  "id": "exercise-59",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-59",
  "type": "Exercise",
  "number": "2.4.5.6",
  "title": "",
  "body": " Suppose that is a set of vectors in that form the columns of a matrix .  Suppose that the vectors span . What can you say about the number of vectors in this set?  Suppose instead that the vectors are linearly independent. What can you say about the number of vectors in this set?  Suppose that the vectors are both linearly independent and span . What can you say about the number of vectors in the set?  Assume that the vectors are both linearly independent and span . Given a vector in , what can you say about the solution space to the equation ?     There must be at least 27 vectors.  There must be at most 27 vectors.  There are exactly 27 vectors.  There is exactly one solution.     In this case, the matrix must have a pivot position in evey row. There must be at least 27 vectors for this to be possible.  In this case, the matrix must have a pivot position in every column. There must be at most 27 vectors for this to be possible.  There must be exactly 27 vectors.  There is exactly one solution to the equation because the matrix has a pivot position in every row and every column.   "
},
{
  "id": "exercise-60",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-60",
  "type": "Exercise",
  "number": "2.4.5.7",
  "title": "",
  "body": " Given below are some descriptions of sets of vectors that form the columns of a matrix . For each description, give a possible reduced row echelon form for or indicate why there is no set of vectors satisfying the description by stating why the required reduced row echelon matrix cannot exist.  A set of 4 linearly independent vectors in .  A set of 4 linearly independent vectors in .  A set of 3 vectors whose span is .  A set of 5 linearly independent vectors in .  A set of 5 vectors whose span is .      .   .  This is not possible.  This is not possible.   .     There must be a pivot position in every column: .  There must be a pivot position in every row. .  This is not possible. The matrix has dimensions so there cannot be a pivot position in every row.  This is not possible. The matrix has dimensions so there cannot be a pivot position in every column.  There must be a pivot position in every row. .   "
},
{
  "id": "exercise-61",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-61",
  "type": "Exercise",
  "number": "2.4.5.8",
  "title": "",
  "body": " When we explored matrix multiplication in , we saw that some properties that are true for real numbers are not true for matrices. This exercise will investigate that in some more depth.  Suppose that and are two matrices and that . If , what can you say about the linear independence of the columns of ?  Suppose that we have matrices , and such that . We have seen that we cannot generally conclude that . If we assume additionally that is a matrix whose columns are linearly independent, explain why . You may wish to begin by rewriting the equation as .      They are linearly dependent.  The columns of and are all equal.     Since , there is a column of , which we'll call , that is not zero. Since , we know that , which means that is a nonzero solution to the homogenous equation . Therefore, the columns of are linearly dependent.   Since the columns of are linearly independent, the only solution to the homogeneous equation is . Since that , every column of satisfies . Therefore, every column of is the zero vector, which implies that .   "
},
{
  "id": "exercise-62",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-62",
  "type": "Exercise",
  "number": "2.4.5.9",
  "title": "",
  "body": " Suppose that is an unknown parameter and consider the set of vectors .  For what values of is the set of vectors linearly dependent?  For what values of does the set of vectors span ?      .   .     We construct the matrix If the vectors are linearly dependent, we cannot have a pivot position in the third column. This means that .  If the vectors span , there must be a pivot in the third row, which means that .   "
},
{
  "id": "exercise-63",
  "level": "2",
  "url": "sec-linear-dep.html#exercise-63",
  "type": "Exercise",
  "number": "2.4.5.10",
  "title": "",
  "body": " Given a set of linearly dependent vectors, we can eliminate some of the vectors to create a smaller, linearly independent set of vectors.  Suppose that is a linear combination of the vectors and . Explain why .  Consider the vectors . Write one of the vectors as a linear combination of the others. Find a set of three vectors whose span is the same as .  Are the three vectors you are left with linearly independent? If not, express one of the vectors as a linear combination of the others and find a set of two vectors whose span is the same as .  Give a geometric description of in as we did in .      Any linear combination of , , and can be rewritten as a linear combination of and .   .   .  It is a plane in .     If is a linear combination of and , then we can write . Then any linear combination of can be rewritten as   We have This shows that and so .  The remaining three vectors are linearly dependent because . We therefore have .  The vectors and are linearly independent so their span is a plane in .   "
},
{
  "id": "sec-linear-trans",
  "level": "1",
  "url": "sec-linear-trans.html",
  "type": "Section",
  "number": "2.5",
  "title": "Matrix transformations",
  "body": " Matrix transformations   The past few sections introduced us to matrix-vector multiplication as a means of thinking geometrically about the solutions to a linear system. In particular, we rewrote a linear system as a matrix equation and developed the concepts of span and linear independence in response to our two fundamental questions.   In this section, we will explore how matrix-vector multiplication defines certain types of functions, which we call matrix transformations , similar to those encountered in previous algebra courses. In particular, we will develop some algebraic tools for thinking about matrix transformations and look at some motivating examples. In the next section, we will see how matrix transformations describe important geometric operations and how they are used in computer animation.    We will begin by considering a more familiar situation; namely, the function , which takes a real number as an input and produces its square as its output.  What is the value of ?  Can we solve the equation ? If so, is the solution unique?  Can we solve the equation ? If so, is the solution unique?  Sketch a graph of the function in      Graph the function above.    We will now consider functions having the form . Draw a graph of the function on the left in .      Graphs of the function and .    Draw a graph of the function on the right of .  Remember that composing two functions means we use the output from one function as the input into the other; that is, . What function results from composing ?      We find .  If , then so there is not a unique solution.  There are no solutions to the equation .       The graph is shown on the left below.      The graph is shown on the right above.  Composing the functions, we find that . We see that the composition is a new linear function whose slope is obtained by multiplying the slopes of and .       Matrix transformations  In the preview activity, we considered familiar linear functions of a single variable, such as . We construct a function like this by choosing a number ; when given an input , the output is formed by multiplying by .  In this section, we will consider functions whose inputs are vectors and whose outputs are vectors defined through matrix-vector multiplication. That is, if is a matrix and is a vector, the function forms the product as its output. Such a function is called a matrix transformation.    matrix transformation  The matrix transformation associated to the matrix is the function that assigns to the vector the vector ; that is, .      The matrix defines a matrix transformation in the following way:   Notice that the input to is a two-dimensional vector and the output is a three-dimensional vector . As a shorthand, we will write to indicate that the inputs are two-dimensional vectors and the outputs are three-dimensional vectors.      Suppose we have a function that has the form We may write This shows that is a matrix transformation associated to the matrix       In this activity, we will look at some examples of matrix transformations.  To begin, suppose that is the matrix with associated matrix transformation .  What is ?  What is ?  What is ?  Is there a vector such that ?  Write as a two-dimensional vector.      Suppose that where .  What is the dimension of the vectors that are inputs for ?  What is the dimension of the vectors that are outputs?  If we describe this transformation as , what are the values of and and how do they relate to the shape of ?  Describe the vectors for which .   If is the matrix , what is in terms of the vectors and ? What about ?  Suppose that is a matrix and that . If , what is the matrix ?      If , then   .   .   .   .  We seek a vector such that . We can solve this equation to find the unique solution .    Now if the matrix has dimensions .   must be a four-dimensional vector.   must be a three-dimensional vector.  For this matrix, we have . In general, if is an matrix, .  If we solve the homogeneous equation , we find that .     , the first column of the matrix. Similarly, gives the second column of the matrix.  The matrix is      Let's discuss a few of the issues that appear in this activity. First, notice that the shape of the matrix and the dimension of the input vector must be compatible if the product is to be defined. In particular, if is an matrix, needs to be an -dimensional vector, and the resulting product will be an -dimensional vector. For the associated matrix transformation, we therefore write meaning takes vectors in as inputs and produces vectors in as outputs. For instance, if , then .  Second, we can often reconstruct the matrix if we only know some output values from its associated linear transformation by remembering that matrix-vector multiplication constructs linear combinations. For instance, if is an matrix , then . That is, we can find the first column of by evaluating . Similarly, the second column of is found by evaluating .  More generally, we will write the columns of the identity matrix as so that . This means that the column of is found by evaluating . We record this fact in the following proposition.    If is a matrix transformation given by , then the matrix has columns ; that is, .      Let's look at some examples and apply these observations.   To begin, suppose that is the matrix transformation that takes a two-dimensional vector as an input and outputs , the two-dimensional vector obtained by rotating counterclockwise by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by counterclockwise into the vectors on the right.       We will see in the next section that many geometric operations like this one can be performed by matrix transformations.   If we write , what are the values of and , and what is the shape of the associated matrix ?    Determine the matrix by applying .    If as shown on the left in , use your matrix to determine and verify that it agrees with that shown on the right of .    If , determine the vector obtained by rotating counterclockwise by .       Suppose that we work for a company that makes baked goods, including cakes, doughnuts, and eclairs. The company operates two bakeries, Bakery 1 and Bakery 2. In one hour of operation,  Bakery 1 produces 10 cakes, 50 doughnuts, and 30 eclairs.  Bakery 2 produces 20 cakes, 30 doughnuts, and 30 eclairs.  If Bakery 1 operates for hours and Bakery 2 for hours, we will use the vector to describe the operation of the two bakeries.  We would like to describe a matrix transformation where describes the number of hours the bakeries operate and describes the total number of cakes, doughnuts, and eclairs produced. That is, where is the number of cakes, is the number of doughnuts, and is the number of eclairs produced.   If , what are the values of and , and what is the shape of the associated matrix ?    We can determine the matrix using . For instance, will describe the number of cakes, doughnuts, and eclairs produced when Bakery 1 operates for one hour and Bakery 2 sits idle. What is this vector?    In the same way, determine . What is the matrix ?    If Bakery 1 operates for 120 hours and Bakery 2 for 180 hours, what is the total number of cakes, doughnuts, and eclairs produced?     Suppose that in one period of time, the company produces 5060 cakes, 14310 doughnuts, and 10470 eclairs. How long did each bakery operate?    Suppose that the company receives an order for a certain number of cakes, doughnuts, and eclairs. Can you guarantee that you can fill the order without having leftovers?                Since both the inputs and the outputs of are two-dimensional, it follows that and that is a matrix.    Since we have .    Multiplying , which agrees with the vector shown in the figure.     .          The shape of matrix is , and .     .     .     .    We solve the equation to obtain     No, you cannot guarantee this because the two columns of cannot span . If we view an order received as a three-dimensional vector , then a solution to the equation tells us how long to operate the two bakeries to produce this order. However, since is a matrix, it must have a row without a pivot position, which means that the equation will be inconsistent for some vectors .          In these examples, we glided over an important point: how do we know these functions can be expressed as matrix transformations? We will take up this question in detail in the next section and not worry about it for now.    Composing matrix transformations  It sometimes happens that we want to combine matrix transformations by performing one and then another. In the last activity, for instance, we considered the matrix transformation where is the result of rotating the two-dimensional vector by . Now suppose we are interested in rotating that vector twice; that is, we take a vector , rotate it by to obtain , and then rotate the result by again to obtain .  This process is called function composition and likely appeared in an earlier algebra course. For instance, if and , the composition of these functions obtained by first performing and then performing is denoted by   Composing matrix transformations is similar. Suppose that we have two matrix transformations, and . Their associated matrices will be denoted by and so that and . If we apply to a vector to obtain and then apply to the result, we have Notice that this implies that the composition is itself a matrix transformation and that the associated matrix is the product .   If and are matrix transformations with associated matrices and respectively, then the composition is also a matrix transformation whose associated matrix is the product .   Notice that the matrix transformations must be compatible if they are to be composed. In particular, the vector , an -dimensional vector, must be a suitable input vector for , which means that the inputs to must be -dimensional. In fact, this is the same condition we need to form the product of their associated matrices, namely, that the number of columns of is the same as the number of rows of .    We will explore the composition of matrix transformations by revisiting the matrix transformations from .   Let's begin with the matrix transformation that rotates a two-dimensional vector by to produce . We saw in the earlier activity that the associated matrix is . Suppose that we compose this matrix transformation with itself to obtain , which is the result of rotating by twice.   What is the matrix associated to the composition ?    What is the result of rotating twice?    Suppose that is the matrix transformation that rotates vectors by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by into the vectors on the right.       Use to find the matrix associated to and explain why it is the same matrix associated to .    Write the two-dimensional vector . How might this vector be expressed in terms of scalar multiplication and why does this make sense geometrically?       In the previous activity, we imagined a company that operates two bakeries. We found the matrix transformation where describes the number of cakes, doughnuts, and eclairs when Bakery1 runs for hours and Bakery 2 runs for hours. The associated matrix is .  Suppose now that  Each cake requires 4 cups of flour and and 2 cups of sugar.  Each doughnut requires 1 cup of flour and 1 cup of sugar.  Each eclair requires 1 cup of flour and 2 cups of sugar.  We will describe a matrix transformation where is a two-dimensional vector describing the number of cups of flour and sugar required to make cakes, doughnuts, and eclairs.   Use to write the matrix associated to the transformation .    If we make 1200 cakes, 2850 doughnuts, and 2250 eclairs, how many cups of flour and sugar are required?     Suppose that Bakery 1 operates for 75 hours and Bakery 2 operates for 53 hours. How many cakes, doughnuts, and eclairs are produced? How many cups of flour and sugar are required?    What is the meaning of the composition and what is its associated matrix?    In a certain time interval, both bakeries use a total of 5800 cups of flour and 5980 cups of sugar. How long have the two bakeries been operating?                The matrix is .     .    The matrix associated to is also since rotating by twice is the same as rotating once by .     , which makes sense because multiplying a vector by simply changes its direction.           .     .     and .     takes as input a vector that records the number of hours both bakeries operate and outputs a vector that tells us the total number of cups of flour and sugar used. The associated matrix is .    We want to find the vector for which . Solving this equation gives .            Discrete Dynamical Systems  In , we will give considerable attention to a specific type of matrix transformation, which is illustrated in the next activity.    Suppose we run a company that has two warehouses, which we will call and , and a fleet of 1000 delivery trucks. Every morning, a delivery truck goes out from one of the warehouses and returns in the evening to one of the warehouses. It is observed that  70% of the trucks that leave return to . The other 30% return to .  50% of the trucks that leave return to and 50% return to .   The distribution of trucks is represented by the vector when there are trucks at location and trucks at . If describes the distribution of trucks in the morning, then the matrix transformation will describe the distribution in the evening.   Suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? Using our vector representation, what is ?  So that we can find the matrix associated to , what does this tell us about ?  In the same way, suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? What is the result and what is ?  Find the matrix such that .  Suppose that there are 100 trucks at and 900 at in the morning. How many are there at the two locations in the evening?  Suppose that there are 550 trucks at and 450 at in the evening. How many trucks were there at the two locations that morning?  Suppose that all of the trucks are at location on Monday morning.  How many trucks are at each location Monday evening?  How many trucks are at each location Tuesday evening?  How many trucks are at each location Wednesday evening?   Suppose that is the matrix transformation that transforms the distribution of trucks one morning into the distribution of trucks in the morning one week (seven days) later. What is the matrix that defines the transformation ?      If 1000 trucks begin at , that evening we find that 70% of them are at with the remaining 30% at . Therefore, . Since , we see that .  In the same way, we see that so that .  The columns of are and so that .  Evaluate .  We solve to find .  We denote the distribution of trucks Monday morning by .  Monday evening, we have .  Tuesday evening, we have .  Wednesday evening, we have .    The matrix is .     As we will see later, this type of situation occurs frequently. We have a vector that describes the state of some system; in this case, describes the distribution of trucks between the two locations at a particular time. Then there is a matrix transformation that describes the state at some later time. We call the state vector and the transition function, as it describes the transition of the state vector from one time to the next. state vector  transition function   Beginning with an initial state , we would like to know how the state evolves over time. For instance, and so on.   discrete dynamical system We call this situation where the state of a system evolves from one time to the next according to the rule a discrete dynamical system . In , we will develop a theory that enables us to make long-term predictions about the evolution of the state vector.    Summary  This section introduced matrix transformations, functions that are defined by matrix-vector multiplication, such as for some matrix .  If is an matrix, then .  The columns of the matrix are given by evaluating the transformation on the vectors ; that is, .  The composition of matrix transformations corresponds to matrix multiplication.  A discrete dynamical system consists of a state vector along with a transition function that describes how the state vector evolves from one time to the next. Powers of the matrix determine the long-term behavior of the state vector.       Suppose that is the matrix transformation defined by the matrix and is the matrix transformation defined by where .  If , what are the values of and ? What values of and are appropriate for the transformation ?  Evaluate .  Evaluate .  Evaluate .  Find the matrix that defines the matrix transformation .      and .   .   .   .   .     Since is a matrix, we have . Since is a matrix, we have .   .   .   .   .     This problem concerns the identification of matrix transformations, about which more will be said in the next section.  Check that the following function is a matrix transformation by finding a matrix such that . .   Explain why is not a matrix transformation.       .  Because the first component has the term .     The matrix .  Because the first component has the term , there is no matrix such that .     Suppose that the matrix defines the matrix transformation .  Describe the vectors that satisfy .  Describe the vectors that satisfy .  Describe the vectors that satisfy .     .  There are no such vectors.   .     We have . The reduced row echelon form of is . Therefore, the solutions have the form .  We see that Since this is an inconsistent system, there are no vectors satisfying the equation.  In the same way, we have This shows that .     Suppose is a matrix transformation with where , , and are as shown in .      The vectors .     Sketch the vector .  What is the vector ?  Find all the vectors such that .      .   .   .      .   .  The matrix that defines is This shows that the solutions to are .     In and , we wrote matrix transformations in terms of the components of . This exercise makes use of that form.   Let's return to the example in concerning the company that operates two bakeries. We used a matrix transformation with input , which recorded the amount of time the two bakeries operated, and output , the number of cakes, doughnuts, and eclairs produced. The associated matrix is .   If , write the output as a three-dimensional vector in terms of and .    If Bakery 1 operates for hours and Bakery 2 for hours, how many cakes are produced?    Explain how you may have discovered this expression by considering the rates at which the two locations make cakes.       Suppose that a bicycle sharing program has two locations and . Bicycles are rented from some location in the morning and returned to a location in the evening. Suppose that   60% of bicycles that begin at in the morning are returned to in the evening while the other 40% are returned to .    30% of bicycles that begin at are returned to and the other 70% are returned to .      If is the number of bicycles at location and the number at in the morning, write an expression for the number of bicycles at in the evening.    Write an expression for the number of bicycles at in the evening.    Write an expression for , the vector that describs the distribution of bicycles in the evening.    Use this expression to identify the matrix associated to the matrix transformation .               We have .     .    Bakery 1 makes 10 cakes per hour and Bakery 2 makes 20.           .          .     .               We have     The number of cakes produced is .    This is consistent with the given information because Bakery 1 produces cakes at the rate of 10 cakes per hour so the number of cakes it produces is . In the same way, the number of cakes produced by Bakery 2 is giving a total of .          Of the bicycles that begin at , 60% of them end up at . Therefore is the number of bicycles that begin at and end at . Similarly, 70% of the bicycles that begin at end up at so this number is . Therefore, the total number of bicycles that end up at is     In the same way, the number of bicycles that end up at is     This gives the matrix     This expression leads to .          Determine whether the following statements are true or false and provide a justification for your response.  A matrix transformation is defined by where is a matrix.  If is a matrix transformation, then there are infinitely many vectors such that .  If is a matrix transformation, then it is possible that every equation has a solution for every vector .  If is a matrix transformation, then the equation always has a solution.     False  True  False  True     False. The dimensions of are .  True. The dimensions of are so there must be a column without a pivot position.  False. The dimensions of are so there must be a row without a pivot position.  True. The vector is always a solution.     Suppose that a company has three plants, called Plants 1, 2, and 3, that produce milk and yogurt . For every hour of operation,  Plant produces 20 units of milk and 15 units of yogurt.  Plant produces 30 units of milk and 5 units of yogurt.  Plant produces 0 units of milk and 40 units of yogurt.    Suppose that , , and record the amounts of time that the three plants are operated and that and record the amount of milk and yogurt produced. If we write and , find the matrix that defines the matrix transformation .  Furthermore, suppose that producing each unit of  milk requires 5 units of electricity and 8 units of labor.  yogurt requires 6 units of electricity and 10 units of labor.  If we write the vector to record the required amounts of electricity and labor , find the matrix that defines the matrix transformation .  If describes the amounts of time that the three plants are operated, how much milk and yogurt is produced? How much electricity and labor are required?  Find the matrix that describes the matrix transformation that gives the required amounts of electricity and labor when the each plants is operated an amount of time given by the vector .      .   .   and .   .     The first column of the matrix is . This shows us that the matrix is .  In the same way, the matrix is .   and .   .     Suppose that is a matrix transformation and that .  Find the vector .  Find the matrix that defines .  Find the vector .      .   .   .     We first find weights and such that by constructing the augmented matrix This shows that . Therefore,   In the same way, we find that This gives the matrix .  Then .     Suppose that two species and interact with one another and that we measure their populations every month. We record their populations in a state vector , where and are the populations of and , respectively. We observe that there is a matrix such that the matrix transformation is the transition function describing how the state vector evolves from month to month. We also observe that, at the beginning of July, the populations are described by the state vector .   What will the populations be at the beginning of August?  What were the populations at the beginning of June?  What will the populations be at the beginning of December?  What will the populations be at the beginning of July in the following year?       .   .   .   .    The initial state is the vector .  At the beginning of August, we have .  At the beginning of June, we have . Solving this equation gives .  At the beginning of December, we have .  At the beginning of July in the following year, we have .     Students in a school are sometimes absent due to an illness. Suppose that  95% of the students who attend school will attend school the next day.  50% of the students who are absent one day will be absent the next day.  We will record the number of present students and the number of absent students in a state vector and note that that state vector evolves from one day to the next according to the transition function . On Tuesday, the state vector is .   Suppose we initially have 1000 students who are present and none absent. Find .  Suppose we initially have 1000 students who are absent and none present. Find .  Use the results of parts a and b to find the matrix that defines the matrix transformation .  If on Tuesday, how are the students distributed on Wednesday?  How many students were present on Monday?  How many students are present on the following Tuesday?  What happens to the number of students who are present after a very long time?            .  There are 1665 students present and 135 absent.  1778  1636  It stabilizes around 1636 students present.      so that .   so that .  The matrix .   meaning that 1665 students are present and 135 are absent.  We want to solve the equation , which gives . There were 1778 students present on Monday.  The following Tuesday, the state will be described by the vector . This means that there were 1636 students present the following Tuesday.  If we compute for some large powers , we find that there are about 1636 students present every day after a very long time.     "
},
{
  "id": "exploration-7",
  "level": "2",
  "url": "sec-linear-trans.html#exploration-7",
  "type": "Preview Activity",
  "number": "2.5.1",
  "title": "",
  "body": "  We will begin by considering a more familiar situation; namely, the function , which takes a real number as an input and produces its square as its output.  What is the value of ?  Can we solve the equation ? If so, is the solution unique?  Can we solve the equation ? If so, is the solution unique?  Sketch a graph of the function in      Graph the function above.    We will now consider functions having the form . Draw a graph of the function on the left in .      Graphs of the function and .    Draw a graph of the function on the right of .  Remember that composing two functions means we use the output from one function as the input into the other; that is, . What function results from composing ?      We find .  If , then so there is not a unique solution.  There are no solutions to the equation .       The graph is shown on the left below.      The graph is shown on the right above.  Composing the functions, we find that . We see that the composition is a new linear function whose slope is obtained by multiplying the slopes of and .    "
},
{
  "id": "definition-11",
  "level": "2",
  "url": "sec-linear-trans.html#definition-11",
  "type": "Definition",
  "number": "2.5.3",
  "title": "",
  "body": "  matrix transformation  The matrix transformation associated to the matrix is the function that assigns to the vector the vector ; that is, .   "
},
{
  "id": "example-matrix-to-mt",
  "level": "2",
  "url": "sec-linear-trans.html#example-matrix-to-mt",
  "type": "Example",
  "number": "2.5.4",
  "title": "",
  "body": "  The matrix defines a matrix transformation in the following way:   Notice that the input to is a two-dimensional vector and the output is a three-dimensional vector . As a shorthand, we will write to indicate that the inputs are two-dimensional vectors and the outputs are three-dimensional vectors.   "
},
{
  "id": "example-mt-to-matrix",
  "level": "2",
  "url": "sec-linear-trans.html#example-mt-to-matrix",
  "type": "Example",
  "number": "2.5.5",
  "title": "",
  "body": "  Suppose we have a function that has the form We may write This shows that is a matrix transformation associated to the matrix    "
},
{
  "id": "activity-21",
  "level": "2",
  "url": "sec-linear-trans.html#activity-21",
  "type": "Activity",
  "number": "2.5.2",
  "title": "",
  "body": "  In this activity, we will look at some examples of matrix transformations.  To begin, suppose that is the matrix with associated matrix transformation .  What is ?  What is ?  What is ?  Is there a vector such that ?  Write as a two-dimensional vector.      Suppose that where .  What is the dimension of the vectors that are inputs for ?  What is the dimension of the vectors that are outputs?  If we describe this transformation as , what are the values of and and how do they relate to the shape of ?  Describe the vectors for which .   If is the matrix , what is in terms of the vectors and ? What about ?  Suppose that is a matrix and that . If , what is the matrix ?      If , then   .   .   .   .  We seek a vector such that . We can solve this equation to find the unique solution .    Now if the matrix has dimensions .   must be a four-dimensional vector.   must be a three-dimensional vector.  For this matrix, we have . In general, if is an matrix, .  If we solve the homogeneous equation , we find that .     , the first column of the matrix. Similarly, gives the second column of the matrix.  The matrix is     "
},
{
  "id": "prop-linear-trans-columns",
  "level": "2",
  "url": "sec-linear-trans.html#prop-linear-trans-columns",
  "type": "Proposition",
  "number": "2.5.6",
  "title": "",
  "body": "  If is a matrix transformation given by , then the matrix has columns ; that is, .   "
},
{
  "id": "activity-mt-intro",
  "level": "2",
  "url": "sec-linear-trans.html#activity-mt-intro",
  "type": "Activity",
  "number": "2.5.3",
  "title": "",
  "body": "  Let's look at some examples and apply these observations.   To begin, suppose that is the matrix transformation that takes a two-dimensional vector as an input and outputs , the two-dimensional vector obtained by rotating counterclockwise by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by counterclockwise into the vectors on the right.       We will see in the next section that many geometric operations like this one can be performed by matrix transformations.   If we write , what are the values of and , and what is the shape of the associated matrix ?    Determine the matrix by applying .    If as shown on the left in , use your matrix to determine and verify that it agrees with that shown on the right of .    If , determine the vector obtained by rotating counterclockwise by .       Suppose that we work for a company that makes baked goods, including cakes, doughnuts, and eclairs. The company operates two bakeries, Bakery 1 and Bakery 2. In one hour of operation,  Bakery 1 produces 10 cakes, 50 doughnuts, and 30 eclairs.  Bakery 2 produces 20 cakes, 30 doughnuts, and 30 eclairs.  If Bakery 1 operates for hours and Bakery 2 for hours, we will use the vector to describe the operation of the two bakeries.  We would like to describe a matrix transformation where describes the number of hours the bakeries operate and describes the total number of cakes, doughnuts, and eclairs produced. That is, where is the number of cakes, is the number of doughnuts, and is the number of eclairs produced.   If , what are the values of and , and what is the shape of the associated matrix ?    We can determine the matrix using . For instance, will describe the number of cakes, doughnuts, and eclairs produced when Bakery 1 operates for one hour and Bakery 2 sits idle. What is this vector?    In the same way, determine . What is the matrix ?    If Bakery 1 operates for 120 hours and Bakery 2 for 180 hours, what is the total number of cakes, doughnuts, and eclairs produced?     Suppose that in one period of time, the company produces 5060 cakes, 14310 doughnuts, and 10470 eclairs. How long did each bakery operate?    Suppose that the company receives an order for a certain number of cakes, doughnuts, and eclairs. Can you guarantee that you can fill the order without having leftovers?                Since both the inputs and the outputs of are two-dimensional, it follows that and that is a matrix.    Since we have .    Multiplying , which agrees with the vector shown in the figure.     .          The shape of matrix is , and .     .     .     .    We solve the equation to obtain     No, you cannot guarantee this because the two columns of cannot span . If we view an order received as a three-dimensional vector , then a solution to the equation tells us how long to operate the two bakeries to produce this order. However, since is a matrix, it must have a row without a pivot position, which means that the equation will be inconsistent for some vectors .         "
},
{
  "id": "proposition-13",
  "level": "2",
  "url": "sec-linear-trans.html#proposition-13",
  "type": "Proposition",
  "number": "2.5.8",
  "title": "",
  "body": " If and are matrix transformations with associated matrices and respectively, then the composition is also a matrix transformation whose associated matrix is the product .  "
},
{
  "id": "activity-23",
  "level": "2",
  "url": "sec-linear-trans.html#activity-23",
  "type": "Activity",
  "number": "2.5.4",
  "title": "",
  "body": "  We will explore the composition of matrix transformations by revisiting the matrix transformations from .   Let's begin with the matrix transformation that rotates a two-dimensional vector by to produce . We saw in the earlier activity that the associated matrix is . Suppose that we compose this matrix transformation with itself to obtain , which is the result of rotating by twice.   What is the matrix associated to the composition ?    What is the result of rotating twice?    Suppose that is the matrix transformation that rotates vectors by , as shown in .   The matrix transformation takes two-dimensional vectors on the left and rotates them by into the vectors on the right.       Use to find the matrix associated to and explain why it is the same matrix associated to .    Write the two-dimensional vector . How might this vector be expressed in terms of scalar multiplication and why does this make sense geometrically?       In the previous activity, we imagined a company that operates two bakeries. We found the matrix transformation where describes the number of cakes, doughnuts, and eclairs when Bakery1 runs for hours and Bakery 2 runs for hours. The associated matrix is .  Suppose now that  Each cake requires 4 cups of flour and and 2 cups of sugar.  Each doughnut requires 1 cup of flour and 1 cup of sugar.  Each eclair requires 1 cup of flour and 2 cups of sugar.  We will describe a matrix transformation where is a two-dimensional vector describing the number of cups of flour and sugar required to make cakes, doughnuts, and eclairs.   Use to write the matrix associated to the transformation .    If we make 1200 cakes, 2850 doughnuts, and 2250 eclairs, how many cups of flour and sugar are required?     Suppose that Bakery 1 operates for 75 hours and Bakery 2 operates for 53 hours. How many cakes, doughnuts, and eclairs are produced? How many cups of flour and sugar are required?    What is the meaning of the composition and what is its associated matrix?    In a certain time interval, both bakeries use a total of 5800 cups of flour and 5980 cups of sugar. How long have the two bakeries been operating?                The matrix is .     .    The matrix associated to is also since rotating by twice is the same as rotating once by .     , which makes sense because multiplying a vector by simply changes its direction.           .     .     and .     takes as input a vector that records the number of hours both bakeries operate and outputs a vector that tells us the total number of cups of flour and sugar used. The associated matrix is .    We want to find the vector for which . Solving this equation gives .         "
},
{
  "id": "activity-24",
  "level": "2",
  "url": "sec-linear-trans.html#activity-24",
  "type": "Activity",
  "number": "2.5.5",
  "title": "",
  "body": "  Suppose we run a company that has two warehouses, which we will call and , and a fleet of 1000 delivery trucks. Every morning, a delivery truck goes out from one of the warehouses and returns in the evening to one of the warehouses. It is observed that  70% of the trucks that leave return to . The other 30% return to .  50% of the trucks that leave return to and 50% return to .   The distribution of trucks is represented by the vector when there are trucks at location and trucks at . If describes the distribution of trucks in the morning, then the matrix transformation will describe the distribution in the evening.   Suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? Using our vector representation, what is ?  So that we can find the matrix associated to , what does this tell us about ?  In the same way, suppose that all 1000 trucks begin the day at location and none at . How many trucks are at each location that evening? What is the result and what is ?  Find the matrix such that .  Suppose that there are 100 trucks at and 900 at in the morning. How many are there at the two locations in the evening?  Suppose that there are 550 trucks at and 450 at in the evening. How many trucks were there at the two locations that morning?  Suppose that all of the trucks are at location on Monday morning.  How many trucks are at each location Monday evening?  How many trucks are at each location Tuesday evening?  How many trucks are at each location Wednesday evening?   Suppose that is the matrix transformation that transforms the distribution of trucks one morning into the distribution of trucks in the morning one week (seven days) later. What is the matrix that defines the transformation ?      If 1000 trucks begin at , that evening we find that 70% of them are at with the remaining 30% at . Therefore, . Since , we see that .  In the same way, we see that so that .  The columns of are and so that .  Evaluate .  We solve to find .  We denote the distribution of trucks Monday morning by .  Monday evening, we have .  Tuesday evening, we have .  Wednesday evening, we have .    The matrix is .    "
},
{
  "id": "exercise-64",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-64",
  "type": "Exercise",
  "number": "2.5.5.1",
  "title": "",
  "body": " Suppose that is the matrix transformation defined by the matrix and is the matrix transformation defined by where .  If , what are the values of and ? What values of and are appropriate for the transformation ?  Evaluate .  Evaluate .  Evaluate .  Find the matrix that defines the matrix transformation .      and .   .   .   .   .     Since is a matrix, we have . Since is a matrix, we have .   .   .   .   .   "
},
{
  "id": "exercise-65",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-65",
  "type": "Exercise",
  "number": "2.5.5.2",
  "title": "",
  "body": " This problem concerns the identification of matrix transformations, about which more will be said in the next section.  Check that the following function is a matrix transformation by finding a matrix such that . .   Explain why is not a matrix transformation.       .  Because the first component has the term .     The matrix .  Because the first component has the term , there is no matrix such that .   "
},
{
  "id": "exercise-66",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-66",
  "type": "Exercise",
  "number": "2.5.5.3",
  "title": "",
  "body": " Suppose that the matrix defines the matrix transformation .  Describe the vectors that satisfy .  Describe the vectors that satisfy .  Describe the vectors that satisfy .     .  There are no such vectors.   .     We have . The reduced row echelon form of is . Therefore, the solutions have the form .  We see that Since this is an inconsistent system, there are no vectors satisfying the equation.  In the same way, we have This shows that .   "
},
{
  "id": "exercise-67",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-67",
  "type": "Exercise",
  "number": "2.5.5.4",
  "title": "",
  "body": " Suppose is a matrix transformation with where , , and are as shown in .      The vectors .     Sketch the vector .  What is the vector ?  Find all the vectors such that .      .   .   .      .   .  The matrix that defines is This shows that the solutions to are .   "
},
{
  "id": "exercise-68",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-68",
  "type": "Exercise",
  "number": "2.5.5.5",
  "title": "",
  "body": " In and , we wrote matrix transformations in terms of the components of . This exercise makes use of that form.   Let's return to the example in concerning the company that operates two bakeries. We used a matrix transformation with input , which recorded the amount of time the two bakeries operated, and output , the number of cakes, doughnuts, and eclairs produced. The associated matrix is .   If , write the output as a three-dimensional vector in terms of and .    If Bakery 1 operates for hours and Bakery 2 for hours, how many cakes are produced?    Explain how you may have discovered this expression by considering the rates at which the two locations make cakes.       Suppose that a bicycle sharing program has two locations and . Bicycles are rented from some location in the morning and returned to a location in the evening. Suppose that   60% of bicycles that begin at in the morning are returned to in the evening while the other 40% are returned to .    30% of bicycles that begin at are returned to and the other 70% are returned to .      If is the number of bicycles at location and the number at in the morning, write an expression for the number of bicycles at in the evening.    Write an expression for the number of bicycles at in the evening.    Write an expression for , the vector that describs the distribution of bicycles in the evening.    Use this expression to identify the matrix associated to the matrix transformation .               We have .     .    Bakery 1 makes 10 cakes per hour and Bakery 2 makes 20.           .          .     .               We have     The number of cakes produced is .    This is consistent with the given information because Bakery 1 produces cakes at the rate of 10 cakes per hour so the number of cakes it produces is . In the same way, the number of cakes produced by Bakery 2 is giving a total of .          Of the bicycles that begin at , 60% of them end up at . Therefore is the number of bicycles that begin at and end at . Similarly, 70% of the bicycles that begin at end up at so this number is . Therefore, the total number of bicycles that end up at is     In the same way, the number of bicycles that end up at is     This gives the matrix     This expression leads to .        "
},
{
  "id": "exercise-69",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-69",
  "type": "Exercise",
  "number": "2.5.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  A matrix transformation is defined by where is a matrix.  If is a matrix transformation, then there are infinitely many vectors such that .  If is a matrix transformation, then it is possible that every equation has a solution for every vector .  If is a matrix transformation, then the equation always has a solution.     False  True  False  True     False. The dimensions of are .  True. The dimensions of are so there must be a column without a pivot position.  False. The dimensions of are so there must be a row without a pivot position.  True. The vector is always a solution.   "
},
{
  "id": "exercise-70",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-70",
  "type": "Exercise",
  "number": "2.5.5.7",
  "title": "",
  "body": " Suppose that a company has three plants, called Plants 1, 2, and 3, that produce milk and yogurt . For every hour of operation,  Plant produces 20 units of milk and 15 units of yogurt.  Plant produces 30 units of milk and 5 units of yogurt.  Plant produces 0 units of milk and 40 units of yogurt.    Suppose that , , and record the amounts of time that the three plants are operated and that and record the amount of milk and yogurt produced. If we write and , find the matrix that defines the matrix transformation .  Furthermore, suppose that producing each unit of  milk requires 5 units of electricity and 8 units of labor.  yogurt requires 6 units of electricity and 10 units of labor.  If we write the vector to record the required amounts of electricity and labor , find the matrix that defines the matrix transformation .  If describes the amounts of time that the three plants are operated, how much milk and yogurt is produced? How much electricity and labor are required?  Find the matrix that describes the matrix transformation that gives the required amounts of electricity and labor when the each plants is operated an amount of time given by the vector .      .   .   and .   .     The first column of the matrix is . This shows us that the matrix is .  In the same way, the matrix is .   and .   .   "
},
{
  "id": "exercise-71",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-71",
  "type": "Exercise",
  "number": "2.5.5.8",
  "title": "",
  "body": " Suppose that is a matrix transformation and that .  Find the vector .  Find the matrix that defines .  Find the vector .      .   .   .     We first find weights and such that by constructing the augmented matrix This shows that . Therefore,   In the same way, we find that This gives the matrix .  Then .   "
},
{
  "id": "exercise-72",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-72",
  "type": "Exercise",
  "number": "2.5.5.9",
  "title": "",
  "body": " Suppose that two species and interact with one another and that we measure their populations every month. We record their populations in a state vector , where and are the populations of and , respectively. We observe that there is a matrix such that the matrix transformation is the transition function describing how the state vector evolves from month to month. We also observe that, at the beginning of July, the populations are described by the state vector .   What will the populations be at the beginning of August?  What were the populations at the beginning of June?  What will the populations be at the beginning of December?  What will the populations be at the beginning of July in the following year?       .   .   .   .    The initial state is the vector .  At the beginning of August, we have .  At the beginning of June, we have . Solving this equation gives .  At the beginning of December, we have .  At the beginning of July in the following year, we have .   "
},
{
  "id": "exercise-73",
  "level": "2",
  "url": "sec-linear-trans.html#exercise-73",
  "type": "Exercise",
  "number": "2.5.5.10",
  "title": "",
  "body": " Students in a school are sometimes absent due to an illness. Suppose that  95% of the students who attend school will attend school the next day.  50% of the students who are absent one day will be absent the next day.  We will record the number of present students and the number of absent students in a state vector and note that that state vector evolves from one day to the next according to the transition function . On Tuesday, the state vector is .   Suppose we initially have 1000 students who are present and none absent. Find .  Suppose we initially have 1000 students who are absent and none present. Find .  Use the results of parts a and b to find the matrix that defines the matrix transformation .  If on Tuesday, how are the students distributed on Wednesday?  How many students were present on Monday?  How many students are present on the following Tuesday?  What happens to the number of students who are present after a very long time?            .  There are 1665 students present and 135 absent.  1778  1636  It stabilizes around 1636 students present.      so that .   so that .  The matrix .   meaning that 1665 students are present and 135 are absent.  We want to solve the equation , which gives . There were 1778 students present on Monday.  The following Tuesday, the state will be described by the vector . This means that there were 1636 students present the following Tuesday.  If we compute for some large powers , we find that there are about 1636 students present every day after a very long time.   "
},
{
  "id": "sec-transforms-geom",
  "level": "1",
  "url": "sec-transforms-geom.html",
  "type": "Section",
  "number": "2.6",
  "title": "The geometry of matrix transformations",
  "body": " The geometry of matrix transformations   Matrix transformations, which we explored in the last section, allow us to describe certain functions . In this section, we will demonstrate how matrix transformations provide a convenient way to describe geometric operations, such as rotations, reflections, and scalings. We will then explore how matrix transformations are used in computer animation.    We will describe the matrix transformation that reflects 2-dimensional vectors across the horizontal axis. For instance, illustrates how a vector is reflected onto the vector .      A vector and its reflection across the horizontal axis.    If , what is the vector ? Sketch the vectors and .  More generally, if , what is ?  Find the vectors and .  Use your results to write the matrix so that . Then verify that agrees with what you found in part b.  Describe the transformation that results from composing with itself; that is, what is the transformation ? Explain how matrix multiplication can be used to justify your response.       .   .   and .  We have the matrix . It follows that as expected.  If we reflect a vector twice in the horizontal axis, we obtain the original vector. The matrix for the transformation is simply .       The geometry of matrix transformations  We have now seen how a few geometric operations, such as rotations and reflections, can be described using matrix transformations. The following activity shows, more generally, that matrix transformations can perform a variety of important geometric operations.   Using matrix transformations to describe geometric operations    The matrix transformation transforms features shown on the left into features shown on the right.    For the following matrices , use the diagram to study the effect of the corresponding matrix transformation . For each transformation, describe the geometric effect the transformation has on the plane.   .   .   .   .   .   .   .   .       This transformation stretches by a factor of 2 in the horizontal direction.  This transformation stretches by a factor of 2 uniformly in all directions.  This is a clockwise rotation.  This transformation is called a shear ; it pushes vectors horizontally an amount equal to the vertical component.  This transformation reflects vectors in the vertical axis.  This transformation is called a projection ; it produces the shadow of the vector on the horizontal axis.  This transformation is called the identity ; it causes no change.  This transformation pushes vectors onto the line defined by the vector .     The previous activity presented some examples showing that matrix transformations can perform interesting geometric operations, such as rotations, scalings, and reflections. Before we go any further, we should explain why it is possible to represent these operations by matrix transformations. In fact, we ask more generally: what types of functions are represented as matrix transformations?  The linearity of matrix-vector multiplication provides the key to answering this question. Remember that if is a matrix, and vectors, and a scalar, then . This means that a matrix transformation satisfies the corresponding linearity property:  Linearity of Matrix Transformations       It turns out that, if satisfies these two linearity properties, then we can find a matrix such that . In fact, tells us how to form ; we simply write . We will now check that using the linearity of : .  The result is the following proposition.    The function is a matrix transformation where for some matrix if and only if . In this case, is the matrix whose columns are ; that is, .    Said simply, this proposition means says that if have a function and can verify the two linearity properties stated in the proposition, then we know that is a matrix transformation. Let's see how this works in practice.    We will consider the function that rotates a vector by in the counterclockwise direction to obtain as seen in .      The function rotates a vector counterclockwise by .   We first need to know that can be represented by a matrix transformation, which means, by , that we need to verify the linearity properties:   The next two figures illustrate why these properties hold. For instance, shows the relationship between and when is a scalar. In particular, scaling a vector and then rotating it is the same as rotating and then scaling it, which means that .      We see that the vector is a scalar multiple to so that .   Similarly, shows the relationship between , , and . Remember that the sum of two vectors is represented by the diagonal of the parallelogram defined by the two vectors. The rotation has the effect of rotating the parallelogram defined by and into the parallelogram defined by and , explaining why .      We see that the vector is the sum of and so that .   Having verified these two properties, we now know that the function that rotates vectors by is a matrix transformation. We may therefore write it as where is the matrix . The columns of this matrix, and , are shown on the right of .      The matrix transformation rotates and by .   Notice that forms an isosceles right triangle, as shown in . Since the length of is 1, the length of , the hypotenuse of the triangle, is also 1, and by Pythagoras' theorem, the lengths of its legs are .      The vector has length 1 and is the hypotenuse of a right isosceles triangle.   This leads to . In the same way, we find that so that the matrix is . You may wish to check this using the interactive diagram in the previous activity using the approximation .    In this example, we found that , a function describing a rotation in the plane, was in fact a matrix transformation by checking that The same kind of thinking applies more generally to show that rotations, reflections, and scalings are matrix transformations. Similarly, we could revisit the functions in and verify that they are matrix transformations.    In this activity, we seek to describe various matrix transformations by finding the matrix that gives the desired transformation. All of the transformations that we study here have the form .  Find the matrix of the transformation that has no effect on vectors; that is, .  Find the matrix of the transformation that reflects vectors in across the line .  What is the result of composing the reflection you found in the previous part with itself; that is, what is the effect of reflecting across the line and then reflecting across this line again? Provide a geometric explanation for your result as well as an algebraic one obtained by multiplying matrices.  Find the matrix that rotates vectors counterclockwise in the plane by .  Compare the result of rotating by and then reflecting in the line to the result of first reflecting in and then rotating .  Find the matrix that results from composing a rotation with itself four times; that is, if is the matrix transformation that rotates vectors by , find the matrix for . Explain why your result makes sense geometrically.  Explain why the matrix that rotates vectors counterclockwise by an angle is .     We use the fact that the columns of the requested matrices have the form .   .   .  The composition of this reflection with itself is described by multiplying the matrix by itself. This produces the matrix , which we just saw is the matrix for the identity transformation. This means that reflecting a vector in the line twice produces the original vector.   .  If we first rotate and then reflect, we obtain the matrix transformation defined by which is the matrix for reflecting in the horizontal axis.  If we first reflect and then rotate, we obtain the matrix which is the matrix for reflecting in the vertical axis.  Composing four times corresponds to raising the matrix to the fourth power, which gives us the identity matrix .  If we consider the effect of rotating the vector by an angle , we obtain the vector .       Matrix transformations and computer animation  Linear algebra plays a significant role in computer animation. We will now illustrate how matrix transformations and some of the ideas we have developed in this section are used by computer animators to create the illusion of motion in their characters.   shows a test character used by Pixar animators. On the left is the original definition of the character; on the right, we see that the character has been moved into a different pose. To make it appear that the character is moving, animators create a sequence of frames in which the character's pose is modified slightly from one frame to the next often using matrix transformations.       Computer animators define a character and create motion by drawing it in a sequence of poses. copyright Disney\/Pixar     Of course, realistic characters will be drawn in three-dimensions. To keep things a little more simple, however, we will look at this two-dimensional character and devise matrix transformations that move them into different poses.    Of course, the first thing we may wish to do is simply move them to a different position in the plane, such as that shown in . Motions like this are called translations .      Translating our character to a new position in the plane.   This presents a problem because a matrix transformation has the property that . This means that a matrix transformation cannot move the origin of the coordinate plane. To address this restriction, animators use homogeneous coordinates , which are formed by placing the two-dimensional coordinate plane inside as the plane , as shown in .      Include the two-dimensional coordinate plane in as the plane so that we can translate the character.   As a result, rather than describing points in the plane as vectors , we describe them as three-dimensional vectors . As we see in the next activity, this allows us to translate our character in the plane.    In this activity, we will use homogeneous coordinates and matrix transformations to move our character into a variety of poses.   Since we regard our character as living in , we will consider matrix transformations defined by matrices . Verify that such a matrix transformation transforms points in the plane into points in the same plane; that is, verify that . Express the coordinates of the resulting point and in terms of the coordinates of the original point and .   An interactive diagram that allows us to move the character using homogeneous coordinates.     Find the matrix transformation that translates our character to a new position in the plane, as shown in        Translating to a new position.    As originally drawn, our character is waving with one of their hands. In one of the movie's scenes, we would like them to wave with their other hand, as shown in . Find the matrix transformation that moves them into this pose.       Waving with the other hand.    Later, our character performs a cartwheel by moving through the sequence of poses shown in . Find the matrix transformations that create these poses.             Performing a cartwheel.    Next, we would like to find the transformations that zoom in on our character's face, as shown in . To do this, you should think about composing matrix transformations. This can be accomplished in the diagram by using the Compose button, which makes the current pose, displayed on the right, the new beginning pose, displayed on the left. What is the matrix transformation that moves the character from the original pose, shown in the upper left, to the final pose, shown in the lower right?             Zooming in on our characters' face.    We would also like to create our character's shadow, shown in the sequence of poses in . Find the sequence of matrix transformations that achieves this. In particular, find the matrix transformation that takes our character from their original pose to their shadow in the lower right.             Casting a shadow.    Write a final scene to the movie and describe how to construct a sequence of matrix transformations that create your scene.        which shows that   Notice that the entries and are responsible for the translation. Therefore, we need the matrix transformation defined by .  We would like to reflect in the vertical axis so we use the matrix .  The character is successively rotated by using the matrix .  We first translate the character down two units using the matrix . Then we zoom in by uniformly stretching by a factor of using the matrix . The net effect is the transformation described by the matrix   The shadow is first created using the shear . Then the vertical scale is compressed using the matrix .       Summary  This section explored how geometric operations are performed by matrix transformations.  A function is a matrix transformation if and only if these properties are satisfied:   Geometric operations, such as rotations, reflections, and scalings, can be represented as matrix transformations.  Composing geometric operations corresponds to matrix multiplication.  Computer animators use homogeneous coordinates and matrix transformations to create the illusion of motion.       For each of the following geometric operations in the plane, find a matrix that defines the matrix transformation performing the operation.  Rotates vectors by .  Reflects vectors across the vertical axis.  Reflects vectors across the line .  Rotates vectors counterclockwise by .  First rotates vectors counterclockwise by and then reflects in the line .      .   .   .   .   .    We create the following matrices as .   .   .   .   .   .     This exercise investigates the composition of reflections in the plane.  Find the result of first reflecting across the line and then . What familiar operation is the cumulative effect of this composition?  What happens if you compose the operations in the opposite order; that is, what happens if you first reflect across and then ? What familiar operation results?  What familiar geometric operation results if you first reflect across the line and then ?  What familiar geometric operation results if you first rotate by and then reflect across the line ?   It is a general fact that the composition of two reflections results in a rotation through twice the angle from the first line of reflection to the second. We will investigate this more generally in     This is the same as a counterclockwise rotation.  This is the same as a clockwise rotation.  This is the same as a rotation.  This is the same as a reflection in the horizontal axis.      . This is the same as a counterclockwise rotation.   . This is the same as a clockwise rotation.   . This is the same as a rotation.   . This is the same as a reflection in the horizontal axis.     Shown below in are the vectors , , and in .      The vectors , , and in .     Imagine that the thumb of your right hand points in the direction of . A positive rotation about the axis corresponds to a rotation in the direction in which your fingers point. Find the matrix definining the matrix transformation that rotates vectors by around the -axis.  In the same way, find the matrix that rotates vectors by around the -axis.  Find the matrix that rotates vectors by around the -axis.  What is the cumulative effect of rotating by about the -axis, followed by a rotation about the -axis, followed by a rotation about the -axis.         .   .   .   .    We construct the matrices as .   .   .   .   , which is a rotation about the -axis.     If a matrix transformation performs a geometric operation, we would like to find a matrix transformation that undoes that operation.  Suppose that is the matrix transformation that rotates vectors by . Find a matrix transformation that undoes the rotation; that is, takes back into so that . Think geometrically about what the transformation should be and then verify it algebraically.  We say that is the inverse of and we will write it as .  Verify algebraically that the reflection across the line is its own inverse; that is, .  The matrix transformation defined by the matrix is called a shear . Find the inverse of .  Describe the geometric effect of the matrix transformation defined by and then find its inverse.       .  The square of the matrix is the identity.   .   .     The transformation should rotate vectors by . The matrix is therefore .  To check , we compute .  The matrix defining the reflection is . Multiplying this by itself gives the identity.  We should apply a shear in the opposite direction: .  We will undo the stretch in the horizontal and vertical directions with the matrix .     We have seen that the matrix performs a rotation through an angle about the origin. Suppose instead that we would like to rotate by about the point . Using homogeneous coordinates, we will develop a matrix that performs this operation.  Our strategy is to  begin with a vector whose tail is at the point ,  translate the vector so that its tail is at the origin,  rotate by , and  translate the vector so that its tail is back at .  This is shown in .      A sequence of matrix transformations that, when read right to left and top to bottom, rotate a vector about the point .   Remember that, when working with homogeneous coordinates, we consider matrices of the form .  The first operation is a translation by . Find the matrix that performs this translation.  The second operation is a rotation about the origin. Find the matrix that performs this rotation.  The third operation is a translation by . Find the matrix that performs this translation.  Use these matrices to find the matrix that performs a rotation about .  Use your matrix to determine where the point ends up if rotated by about the .      .   .   .     The point is rotated to .      .   .   .     If we call this matrix , we compute . Therefore, the point is rotated to .     Consider the matrix transformation that assigns to a vector the closest vector on horizontal axis as illustrated in . This transformation is called the projection onto the horizontal axis. You may imagine as the shadow cast by from a flashlight far up on the positive -axis.      Projection onto the -axis.    Find the matrix that defines this matrix transformation .  Find the matrix that defines projection on the vertical axis.  What is the result of composing the projection onto the horizontal axis with the projection onto the vertical axis?  Find the matrix that defines projection onto the line .       .   .   .   .      .   .   . The result of composing the two projections sends every vector to .   .     This exericse concerns the matrix transformations defined by matrices of the form . Let's begin by looking at two special types of these matrices.  First, consider the matrix where and so that . Describe the geometric effect of this matrix. More generally, suppose we have , where is a positive number. What is the geometric effect of on vectors in the plane?  Suppose now that and so that . What is the geometric effect of on vectors in the plane? More generally, suppose we have . What is the geometric effect of on vectors in the plane?  In general, the composition of matrix transformation depends on the order in which we compose them. For these transformations, however, it is not the case. Check this by verifying that .  Let's now look at the general case where . We will draw the vector in the plane and express it using polar coordinates and as shown in .      A vector may be expressed in polar coordinates.   We then have . Show that the matrix .  Using this description, describe the geometric effect on vectors in the plane of the matrix transformation defined by .  Suppose we have a matrix transformation defined by a matrix and another transformation defined by where . Describe the geometric effect of the composition in terms of the , , , and .  The matrices of this form give a model for the complex numbers and will play an important role in .    It stretches vectors by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .     The matrix stretches vectors by . It has the same effect as scalar multiplication by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .     We saw earlier that the rotation in the plane through an angle is given by the matrix: . We would like to find a similar expression for the matrix that represents the reflection across , the line passing through the origin and making an angle of with the positive -axis, as shown in .      The reflection across the line .    To do this, notice that this reflection can be obtained by composing three separate transformations as shown in . Beginning with the vector , we apply the transformation to rotate by and obtain . Next, we apply , a reflection in the horizontal axis, followed by , a rotation by . We see that is the same as the reflection of in the original line .      Reflection in the line as a composition of three transformations.   Using this decomposition, show that the reflection in the line is described by the matrix . You will need to remember the trigonometric identities: .   Now that we have a matrix that describes the reflection in the line , show that the composition of the reflection in the horizontal axis followed by the reflection in is a counterclockwise rotation by an angle . We saw some examples of this earlier in .     We have   Compute that      We have   Compute that      "
},
{
  "id": "exploration-8",
  "level": "2",
  "url": "sec-transforms-geom.html#exploration-8",
  "type": "Preview Activity",
  "number": "2.6.1",
  "title": "",
  "body": "  We will describe the matrix transformation that reflects 2-dimensional vectors across the horizontal axis. For instance, illustrates how a vector is reflected onto the vector .      A vector and its reflection across the horizontal axis.    If , what is the vector ? Sketch the vectors and .  More generally, if , what is ?  Find the vectors and .  Use your results to write the matrix so that . Then verify that agrees with what you found in part b.  Describe the transformation that results from composing with itself; that is, what is the transformation ? Explain how matrix multiplication can be used to justify your response.       .   .   and .  We have the matrix . It follows that as expected.  If we reflect a vector twice in the horizontal axis, we obtain the original vector. The matrix for the transformation is simply .    "
},
{
  "id": "activity-linear-trans-geom",
  "level": "2",
  "url": "sec-transforms-geom.html#activity-linear-trans-geom",
  "type": "Activity",
  "number": "2.6.2",
  "title": "Using matrix transformations to describe geometric operations.",
  "body": " Using matrix transformations to describe geometric operations    The matrix transformation transforms features shown on the left into features shown on the right.    For the following matrices , use the diagram to study the effect of the corresponding matrix transformation . For each transformation, describe the geometric effect the transformation has on the plane.   .   .   .   .   .   .   .   .       This transformation stretches by a factor of 2 in the horizontal direction.  This transformation stretches by a factor of 2 uniformly in all directions.  This is a clockwise rotation.  This transformation is called a shear ; it pushes vectors horizontally an amount equal to the vertical component.  This transformation reflects vectors in the vertical axis.  This transformation is called a projection ; it produces the shadow of the vector on the horizontal axis.  This transformation is called the identity ; it causes no change.  This transformation pushes vectors onto the line defined by the vector .    "
},
{
  "id": "prop-linear-trans-characterization",
  "level": "2",
  "url": "sec-transforms-geom.html#prop-linear-trans-characterization",
  "type": "Proposition",
  "number": "2.6.3",
  "title": "",
  "body": "  The function is a matrix transformation where for some matrix if and only if . In this case, is the matrix whose columns are ; that is, .   "
},
{
  "id": "example-16",
  "level": "2",
  "url": "sec-transforms-geom.html#example-16",
  "type": "Example",
  "number": "2.6.4",
  "title": "",
  "body": "  We will consider the function that rotates a vector by in the counterclockwise direction to obtain as seen in .      The function rotates a vector counterclockwise by .   We first need to know that can be represented by a matrix transformation, which means, by , that we need to verify the linearity properties:   The next two figures illustrate why these properties hold. For instance, shows the relationship between and when is a scalar. In particular, scaling a vector and then rotating it is the same as rotating and then scaling it, which means that .      We see that the vector is a scalar multiple to so that .   Similarly, shows the relationship between , , and . Remember that the sum of two vectors is represented by the diagonal of the parallelogram defined by the two vectors. The rotation has the effect of rotating the parallelogram defined by and into the parallelogram defined by and , explaining why .      We see that the vector is the sum of and so that .   Having verified these two properties, we now know that the function that rotates vectors by is a matrix transformation. We may therefore write it as where is the matrix . The columns of this matrix, and , are shown on the right of .      The matrix transformation rotates and by .   Notice that forms an isosceles right triangle, as shown in . Since the length of is 1, the length of , the hypotenuse of the triangle, is also 1, and by Pythagoras' theorem, the lengths of its legs are .      The vector has length 1 and is the hypotenuse of a right isosceles triangle.   This leads to . In the same way, we find that so that the matrix is . You may wish to check this using the interactive diagram in the previous activity using the approximation .   "
},
{
  "id": "activity-26",
  "level": "2",
  "url": "sec-transforms-geom.html#activity-26",
  "type": "Activity",
  "number": "2.6.3",
  "title": "",
  "body": "  In this activity, we seek to describe various matrix transformations by finding the matrix that gives the desired transformation. All of the transformations that we study here have the form .  Find the matrix of the transformation that has no effect on vectors; that is, .  Find the matrix of the transformation that reflects vectors in across the line .  What is the result of composing the reflection you found in the previous part with itself; that is, what is the effect of reflecting across the line and then reflecting across this line again? Provide a geometric explanation for your result as well as an algebraic one obtained by multiplying matrices.  Find the matrix that rotates vectors counterclockwise in the plane by .  Compare the result of rotating by and then reflecting in the line to the result of first reflecting in and then rotating .  Find the matrix that results from composing a rotation with itself four times; that is, if is the matrix transformation that rotates vectors by , find the matrix for . Explain why your result makes sense geometrically.  Explain why the matrix that rotates vectors counterclockwise by an angle is .     We use the fact that the columns of the requested matrices have the form .   .   .  The composition of this reflection with itself is described by multiplying the matrix by itself. This produces the matrix , which we just saw is the matrix for the identity transformation. This means that reflecting a vector in the line twice produces the original vector.   .  If we first rotate and then reflect, we obtain the matrix transformation defined by which is the matrix for reflecting in the horizontal axis.  If we first reflect and then rotate, we obtain the matrix which is the matrix for reflecting in the vertical axis.  Composing four times corresponds to raising the matrix to the fourth power, which gives us the identity matrix .  If we consider the effect of rotating the vector by an angle , we obtain the vector .    "
},
{
  "id": "fig-blob-man",
  "level": "2",
  "url": "sec-transforms-geom.html#fig-blob-man",
  "type": "Figure",
  "number": "2.6.10",
  "title": "",
  "body": "     Computer animators define a character and create motion by drawing it in a sequence of poses. copyright Disney\/Pixar  "
},
{
  "id": "fig-animate-translate",
  "level": "2",
  "url": "sec-transforms-geom.html#fig-animate-translate",
  "type": "Figure",
  "number": "2.6.11",
  "title": "",
  "body": "    Translating our character to a new position in the plane.  "
},
{
  "id": "fig-animate-homogeneous",
  "level": "2",
  "url": "sec-transforms-geom.html#fig-animate-homogeneous",
  "type": "Figure",
  "number": "2.6.12",
  "title": "",
  "body": "    Include the two-dimensional coordinate plane in as the plane so that we can translate the character.  "
},
{
  "id": "activity-27",
  "level": "2",
  "url": "sec-transforms-geom.html#activity-27",
  "type": "Activity",
  "number": "2.6.4",
  "title": "",
  "body": "  In this activity, we will use homogeneous coordinates and matrix transformations to move our character into a variety of poses.   Since we regard our character as living in , we will consider matrix transformations defined by matrices . Verify that such a matrix transformation transforms points in the plane into points in the same plane; that is, verify that . Express the coordinates of the resulting point and in terms of the coordinates of the original point and .   An interactive diagram that allows us to move the character using homogeneous coordinates.     Find the matrix transformation that translates our character to a new position in the plane, as shown in        Translating to a new position.    As originally drawn, our character is waving with one of their hands. In one of the movie's scenes, we would like them to wave with their other hand, as shown in . Find the matrix transformation that moves them into this pose.       Waving with the other hand.    Later, our character performs a cartwheel by moving through the sequence of poses shown in . Find the matrix transformations that create these poses.             Performing a cartwheel.    Next, we would like to find the transformations that zoom in on our character's face, as shown in . To do this, you should think about composing matrix transformations. This can be accomplished in the diagram by using the Compose button, which makes the current pose, displayed on the right, the new beginning pose, displayed on the left. What is the matrix transformation that moves the character from the original pose, shown in the upper left, to the final pose, shown in the lower right?             Zooming in on our characters' face.    We would also like to create our character's shadow, shown in the sequence of poses in . Find the sequence of matrix transformations that achieves this. In particular, find the matrix transformation that takes our character from their original pose to their shadow in the lower right.             Casting a shadow.    Write a final scene to the movie and describe how to construct a sequence of matrix transformations that create your scene.        which shows that   Notice that the entries and are responsible for the translation. Therefore, we need the matrix transformation defined by .  We would like to reflect in the vertical axis so we use the matrix .  The character is successively rotated by using the matrix .  We first translate the character down two units using the matrix . Then we zoom in by uniformly stretching by a factor of using the matrix . The net effect is the transformation described by the matrix   The shadow is first created using the shear . Then the vertical scale is compressed using the matrix .    "
},
{
  "id": "exercise-74",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-74",
  "type": "Exercise",
  "number": "2.6.4.1",
  "title": "",
  "body": " For each of the following geometric operations in the plane, find a matrix that defines the matrix transformation performing the operation.  Rotates vectors by .  Reflects vectors across the vertical axis.  Reflects vectors across the line .  Rotates vectors counterclockwise by .  First rotates vectors counterclockwise by and then reflects in the line .      .   .   .   .   .    We create the following matrices as .   .   .   .   .   .   "
},
{
  "id": "ex-compose-reflections",
  "level": "2",
  "url": "sec-transforms-geom.html#ex-compose-reflections",
  "type": "Exercise",
  "number": "2.6.4.2",
  "title": "",
  "body": " This exercise investigates the composition of reflections in the plane.  Find the result of first reflecting across the line and then . What familiar operation is the cumulative effect of this composition?  What happens if you compose the operations in the opposite order; that is, what happens if you first reflect across and then ? What familiar operation results?  What familiar geometric operation results if you first reflect across the line and then ?  What familiar geometric operation results if you first rotate by and then reflect across the line ?   It is a general fact that the composition of two reflections results in a rotation through twice the angle from the first line of reflection to the second. We will investigate this more generally in     This is the same as a counterclockwise rotation.  This is the same as a clockwise rotation.  This is the same as a rotation.  This is the same as a reflection in the horizontal axis.      . This is the same as a counterclockwise rotation.   . This is the same as a clockwise rotation.   . This is the same as a rotation.   . This is the same as a reflection in the horizontal axis.   "
},
{
  "id": "exercise-76",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-76",
  "type": "Exercise",
  "number": "2.6.4.3",
  "title": "",
  "body": " Shown below in are the vectors , , and in .      The vectors , , and in .     Imagine that the thumb of your right hand points in the direction of . A positive rotation about the axis corresponds to a rotation in the direction in which your fingers point. Find the matrix definining the matrix transformation that rotates vectors by around the -axis.  In the same way, find the matrix that rotates vectors by around the -axis.  Find the matrix that rotates vectors by around the -axis.  What is the cumulative effect of rotating by about the -axis, followed by a rotation about the -axis, followed by a rotation about the -axis.         .   .   .   .    We construct the matrices as .   .   .   .   , which is a rotation about the -axis.   "
},
{
  "id": "exercise-77",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-77",
  "type": "Exercise",
  "number": "2.6.4.4",
  "title": "",
  "body": " If a matrix transformation performs a geometric operation, we would like to find a matrix transformation that undoes that operation.  Suppose that is the matrix transformation that rotates vectors by . Find a matrix transformation that undoes the rotation; that is, takes back into so that . Think geometrically about what the transformation should be and then verify it algebraically.  We say that is the inverse of and we will write it as .  Verify algebraically that the reflection across the line is its own inverse; that is, .  The matrix transformation defined by the matrix is called a shear . Find the inverse of .  Describe the geometric effect of the matrix transformation defined by and then find its inverse.       .  The square of the matrix is the identity.   .   .     The transformation should rotate vectors by . The matrix is therefore .  To check , we compute .  The matrix defining the reflection is . Multiplying this by itself gives the identity.  We should apply a shear in the opposite direction: .  We will undo the stretch in the horizontal and vertical directions with the matrix .   "
},
{
  "id": "exercise-78",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-78",
  "type": "Exercise",
  "number": "2.6.4.5",
  "title": "",
  "body": " We have seen that the matrix performs a rotation through an angle about the origin. Suppose instead that we would like to rotate by about the point . Using homogeneous coordinates, we will develop a matrix that performs this operation.  Our strategy is to  begin with a vector whose tail is at the point ,  translate the vector so that its tail is at the origin,  rotate by , and  translate the vector so that its tail is back at .  This is shown in .      A sequence of matrix transformations that, when read right to left and top to bottom, rotate a vector about the point .   Remember that, when working with homogeneous coordinates, we consider matrices of the form .  The first operation is a translation by . Find the matrix that performs this translation.  The second operation is a rotation about the origin. Find the matrix that performs this rotation.  The third operation is a translation by . Find the matrix that performs this translation.  Use these matrices to find the matrix that performs a rotation about .  Use your matrix to determine where the point ends up if rotated by about the .      .   .   .     The point is rotated to .      .   .   .     If we call this matrix , we compute . Therefore, the point is rotated to .   "
},
{
  "id": "exercise-79",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-79",
  "type": "Exercise",
  "number": "2.6.4.6",
  "title": "",
  "body": " Consider the matrix transformation that assigns to a vector the closest vector on horizontal axis as illustrated in . This transformation is called the projection onto the horizontal axis. You may imagine as the shadow cast by from a flashlight far up on the positive -axis.      Projection onto the -axis.    Find the matrix that defines this matrix transformation .  Find the matrix that defines projection on the vertical axis.  What is the result of composing the projection onto the horizontal axis with the projection onto the vertical axis?  Find the matrix that defines projection onto the line .       .   .   .   .      .   .   . The result of composing the two projections sends every vector to .   .   "
},
{
  "id": "exercise-80",
  "level": "2",
  "url": "sec-transforms-geom.html#exercise-80",
  "type": "Exercise",
  "number": "2.6.4.7",
  "title": "",
  "body": " This exericse concerns the matrix transformations defined by matrices of the form . Let's begin by looking at two special types of these matrices.  First, consider the matrix where and so that . Describe the geometric effect of this matrix. More generally, suppose we have , where is a positive number. What is the geometric effect of on vectors in the plane?  Suppose now that and so that . What is the geometric effect of on vectors in the plane? More generally, suppose we have . What is the geometric effect of on vectors in the plane?  In general, the composition of matrix transformation depends on the order in which we compose them. For these transformations, however, it is not the case. Check this by verifying that .  Let's now look at the general case where . We will draw the vector in the plane and express it using polar coordinates and as shown in .      A vector may be expressed in polar coordinates.   We then have . Show that the matrix .  Using this description, describe the geometric effect on vectors in the plane of the matrix transformation defined by .  Suppose we have a matrix transformation defined by a matrix and another transformation defined by where . Describe the geometric effect of the composition in terms of the , , , and .  The matrices of this form give a model for the complex numbers and will play an important role in .    It stretches vectors by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .     The matrix stretches vectors by . It has the same effect as scalar multiplication by .  This matrix rotates vectors by an angle .  No matter which order we multiply the matrices, we find that their product is   This follows from the previous part.  If we use polar coordinates to write , multiplying a vector by will stretch the vector by and rotate it by .  If then has the effect of stretching a vector by and rotating it by .   "
},
{
  "id": "ex-reflection-compose-general",
  "level": "2",
  "url": "sec-transforms-geom.html#ex-reflection-compose-general",
  "type": "Exercise",
  "number": "2.6.4.8",
  "title": "",
  "body": " We saw earlier that the rotation in the plane through an angle is given by the matrix: . We would like to find a similar expression for the matrix that represents the reflection across , the line passing through the origin and making an angle of with the positive -axis, as shown in .      The reflection across the line .    To do this, notice that this reflection can be obtained by composing three separate transformations as shown in . Beginning with the vector , we apply the transformation to rotate by and obtain . Next, we apply , a reflection in the horizontal axis, followed by , a rotation by . We see that is the same as the reflection of in the original line .      Reflection in the line as a composition of three transformations.   Using this decomposition, show that the reflection in the line is described by the matrix . You will need to remember the trigonometric identities: .   Now that we have a matrix that describes the reflection in the line , show that the composition of the reflection in the horizontal axis followed by the reflection in is a counterclockwise rotation by an angle . We saw some examples of this earlier in .     We have   Compute that      We have   Compute that    "
},
{
  "id": "sec-matrix-inverse",
  "level": "1",
  "url": "sec-matrix-inverse.html",
  "type": "Section",
  "number": "3.1",
  "title": "Invertibility",
  "body": " Invertibility   Up to this point, we have used the Gaussian elimination algorithm to find solutions to linear systems. We now investigate another way to find solutions to the equation when the matrix has the same number of rows and columns. To get started, let's look at some familiar examples.      Explain how you would solve the equation using multiplication rather than division.  Find the matrix that rotates vectors counterclockwise by .  Find the matrix that rotates vectors clockwise by .  What do you expect the product to be? Explain the reasoning behind your expectation and then compute to verify it.  Solve the equation using Gaussian elimination.   Explain why your solution may also be found by computing .       Dividing by is the same as multiplying by , the multiplicative inverse of . We have   As we have seen a few times, the matrix is   Here, the matrix is   We should expect that since the effect of rotating by clockwise followed by rotating counterclockwise is to leave a vector unchanged. We can verify this by performing the matrix multiplication.  We have so the solution is .  The equation is asking us to find the vector that becomes after being rotated by . If we rotate by in the opposite direction, it will have this property. That is, if , then        Invertible matrices  The preview activity began with a familiar type of equation, , and asked for a strategy to solve it. One possible response is to divide both sides by 3. Instead, let's rephrase this as multiplying by , the multiplicative inverse of 3.  Now that we are interested in solving equations of the form , we might try to find a similar approach. Is there a matrix that plays the role of the multiplicative inverse of ? Of course, the real number does not have a multiplicative inverse so we probably shouldn't expect every matrix to have a multiplicative inverse. We will see, however, that many do.    invertible  matrix, inverse  An matrix is called invertible if there is a matrix such that , where is the identity matrix. The matrix is called the inverse of and denoted .     matrix, square Notice that we only define invertibility for matrices that have the same number of rows and columns in which case we say that the matrix is square .    Suppose that is the matrix that rotates two-dimensional vectors counterclockwise by and that rotates vectors by . We have We can check that which shows that is invertible and that .  Notice that if we multiply the matrices in the opposite order, we find that , which says that is also invertible and that . In other words, and are inverses of each other.      This activity demonstrates a procedure for finding the inverse of a matrix .   Suppose that . To find an inverse , we write its columns as and require that In other words, we can find the columns of by solving the equations Solve these equations to find and . Then write the matrix and verify that . This is enough for us to conclude that is the inverse of .     Find the product and explain why we now know that is invertible and .     What happens when you try to find the inverse of ?    We now develop a condition that must be satisfied by an invertible matrix. Suppose that is an invertible matrix with inverse and suppose that is any -dimensional vector. Since , we have This says that the equation is consistent and that is a solution.  Since we know that is consistent for any vector , what does this say about the span of the columns of ?    Since is a square matrix, what does this say about the pivot positions of ? What is the reduced row echelon form of ?    In this activity, we have studied the matrices Find the reduced row echelon form of each and explain how those forms enable us to conclude that one matrix is invertible and the other is not.        Solving the two equations for and gives . We can verify that, as we expect, .  We find that , which is the condition that tells us that is invertible.  Seeking the first column of , we see that the equation is not consistent. This means that is not invertible.  Since the equation is consistent for every , we know that the span of the columns of is .  Because the span of the columns of is , there is a pivot position in every row. Since is square, there is also a pivot position in every column. This means that the reduced row echelon form of must be the identity matrix .  We see that which shows that is invertible and is not.       We can reformulate this procedure for finding the inverse of a matrix. For the sake of convenience, suppose that is a invertible matrix with inverse . Rather than solving the equations separately, we can solve them at the same time by augmenting by both vectors and and finding the reduced row echelon form.  For example, if , we form This shows that the matrix is the inverse of .  In other words, beginning with , we augment by the identify and find the reduced row echelon form to determine :     In fact, this reformulation will always work. Suppose that is an invertible matrix with inverse . Suppose furthermore that is any -dimensional vector and consider the equation . We know that is a solution because     If is an invertible matrix with inverse , then any equation is consistent and is a solution. In other words, the solution to is .    Notice that this is similar to saying that the solution to is , as we saw in the preview activity.  Now since is consistent for every vector , the columns of must span so there is a pivot position in every row. Since is also square, this means that the reduced row echelon form of is the identity matrix.    The matrix is invertible if and only if the reduced row echelon form of is the identity matrix: . In addition, we can find the inverse by augmenting by the identity and finding the reduced row echelon form:     You may have noticed that says that the solution to the equation is . Indeed, we know that this equation has a unique solution because has a pivot position in every column.  It is important to remember that the product of two matrices depends on the order in which they are multiplied. That is, if and are matrices, then it sometimes happens that . However, something fortunate happens when we consider invertibility. It turns out that if is an matrix and that , then it is also true that . We have verified this in a few examples so far, and explains why it always happens. This leads to the following proposition.    If is a invertible matrix with inverse , then , which tells us that is invertible with inverse . In other words,       Solving equations with an inverse  If is an invertible matrix, then shows us how to use to solve equations involving . In particular, the solution to is .    We'll begin by considering the square matrix    Describe the solution space to the equation by augmenting and finding the reduced row echelon form.     Using , explain why is invertible and find its inverse.    Now use the inverse to solve the equation and verify that your result agrees with what you found in part a.    If you have defined a matrix B in Sage, you can find it's inverse as B.inverse() or B^-1 . Use Sage to find the inverse of the matrix and use it to solve the equation .     If and are the two matrices defined in this activity, find their product and verify that it is invertible.    Compute the products and . Which one agrees with ?    Explain your finding by considering the product and using associativity to regroup the products so that the middle two terms are multiplied first.        Constructing the augmented matrix, we see that which says that there is a unique solution .    Our work in part a shows that from which we conclude that is invertible. To find the inverse, which says that     We see that .    Sage tells us that .    Sage helps us see that , which tells us that is invertible.   We find that .   We see that       The next proposition summarizes much of what we have found about invertible matrices.   Properties of invertible matrices     An matrix is invertible if and only if .  If is invertible, then the solution to the equation is given by .  We can find by finding the reduced row echelon form of ; namely, .  If and are two invertible matrices, then their product is also invertible and .      There is a simple formula for finding the inverse of a matrix: , which can be easily checked. The condition that be invertible is, in this case, reduced to the condition that . We will understand this condition better once we have explored determinants in . There is a similar formula for the inverse of a matrix, but there is not a good reason to write it here.    Triangular matrices and Gaussian elimination  With some of the ideas we've developed, we can recast the Gaussian elimination algorithm in terms of matrix multiplication and invertibility. This will be especially helpful later when we consider determinants and LU factorizations. Triangular matrices will play an important role.    lower triangular matrix  upper triangular matrix  We say that a matrix is lower triangular if all its entries above the diagonal are zero. Similarly, is upper triangular if all the entries below the diagonal are zero.    For example, the matrix below is a lower triangular matrix while is an upper triangular one. .  We can develop a simple test to determine whether an lower triangular matrix is invertible. Let's use Gaussian elimination to find the reduced row echelon form of the lower triangular matrix Because the entries on the diagonal are nonzero, we find a pivot position in every row, which tells us that the matrix is invertible.  If, however, there is a zero entry on the diagonal, the matrix cannot be invertible. Considering the matrix below, we see that having a zero on the diagonal leads to a row without a pivot position.     An triangular matrix is invertible if and only if the entries on the diagonal are all nonzero.     Gaussian elimination and matrix multiplication   This activity explores how the row operations of scaling, interchange, and replacement can be performed using matrix multiplication.  As an example, we consider the matrix and apply a replacement operation that multiplies the first row by and adds it to the second row. Rather than performing this operation in the usual way, we construct a new matrix by applying the desired replacement operation to the identity matrix. To illustrate, we begin with the identity matrix and form a new matrix by multiplying the first row by and adding it to the second row to obtain   Show that the product is the result of applying the replacement operation to .   Explain why is invertible and find its inverse .  Describe the relationship between and and use the connection to replacement operations to explain why it holds.  Other row operations can be performed using a similar procedure. For instance, suppose we want to scale the second row of by . Find a matrix so that is the same as that obtained from the scaling operation. Why is invertible and what is ?   Finally, suppose we want to interchange the first and third rows of . Find a matrix , usually called a permutation matrix that performs this operation. What is ?  The original matrix is seen to be row equivalent to the upper triangular matrix by performing three replacement operations on : Find the matrices , , and that perform these row replacement operations so that .  Explain why the matrix product is invertible and use this fact to write . What is the matrix that you find? Why do you think we denote it by ?        Performing the matrix multiplication, we find that   We know that is invertible because it is a lower triangular matrix whose diagonal entries are all 1. We find that , which can be verified.  But we can see this in another way as well. The replacement operation is reversible; that is, multiplying the first row by and adding it to the second row can be undone by multiplying the first row by and adding it to the second row.  We find that This makes sense because scaling a row by can be undone by scaling the same row by .  We find that Moreover, because we can undo the interchange operation by repeating it.  Continuing with the Gaussian elimination algorithm, we have , as above, we then have .  Each of the matrices , , and is invertible so their product will be as well. Since , we have . Moreover, gives . Notice that this matrix is lower triangular so we call it .      matrix, elementary The following are examples of matrices, known as elementary matrices , that perform the row operations on a matrix having three rows.  Replacement  Multiplying the second row by 3 and adding it to the third row is performed by We often use to describe these matrices because they are lower triangular.  Scaling  Multiplying the third row by 2 is performed by   Interchange  Interchanging the first two rows is performed by       Suppose we have For the forward substitution phase of Gaussian elimination, we perform a sequence of three replacement operations. The first replacement operation multiplies the first row by and adds the result to the second row. We can perform this operation by multiplying by the lower triangular matrix where   The next two replacement operations are performed by the matrices so that   Notice that the inverse of has the simple form: . This says that if we want to undo the operation of multiplying the first row by and adding to the second row, we should multiply the first row by and add it to the second row. That is the effect of .  Notice that we now have , which gives where is the lower triangular matrix This way of writing as the product of a lower and an upper triangular matrix is known as an factorization of , and its usefulness will be explored in .      Summary  In this section, we found conditions guaranteeing that a matrix has an inverse. When these conditions hold, we also found an algorithm for finding the inverse.  A square matrix is invertible if there is a matrix , known as the inverse of , such that . We usually write .  The matrix is invertible if and only if it is row equivalent to , the identity matrix.  If a matrix is invertible, we can use Gaussian elimination to find its inverse: .  If a matrix is invertible, then the solution to the equation is .  The row operations of replacement, scaling, and interchange can be performed by multiplying by elementary matrices.       Consider the matrix .   Explain why has an inverse.  Find the inverse of by augmenting by the identity to form .  Use your inverse to solve the equation .             .     We see that , the identity matrix, which implies that has an inverse.  We have which says that   We compute that .     In this exercise, we will consider matrices as defining matrix transformations.  Write the matrix that performs a rotation. What geometric operation undoes this rotation? Find the matrix that perform this operation and verify that it is .  Write the matrix that performs a rotation. Verify that so that , and explain geometrically why this is the case.  Find three more matrices that satisfy .      and .   .         . To undo the rotation, we will perform a clockwise rotation, which is defined by the matrix .   . We see that because rotating by is its own inverse.  We can do this by constructing matrices that define reflections, such as      Inverses for certain types of matrices can be found in a relatively straightforward fashion.   The matrix is called diagonal since the only nonzero entries are on the diagonal of the matrix.     Find by augmenting by the identity and finding its reduced row echelon form.    Under what conditions is a diagonal matrix invertible?    Explain why the inverse of a diagonal matrix is also diagonal and explain the relationship between the diagonal entries in and .       Consider the lower triangular matrix .   Find by augmenting by the identity and finding its reduced row echelon form.    Explain why the inverse of a lower triangular matrix is also lower triangular.                .    When the entries on the diagonal are all nonzero.    Consider the steps performed in row reducing augmented by .           .    Consider the steps in row reducing augmented by .                .    When the entries on the diagonal are all nonzero.    Because the process of row reducing augmented by can be performed using only scalings. Then the diagonal entries of and are reciprocals of one another.           .     is always lower triangular because the only row operations needed to row reduce augmented by are replacements in which a multiple of one row is added to a row underneath it.          Our definition of an invertible matrix requires that be a square matrix. Let's examine what happens when is not square. For instance, suppose that .  Verify that . In this case, we say that is a left inverse of .   If has a left inverse , we can still use it to find solutions to linear equations. If we know there is a solution to the equation , we can multiply both sides of the equation by to find .  Suppose you know there is a solution to the equation . Use the left inverse to find and verify that it is a solution.  Now consider the matrix and verify that is also a left inverse of . This shows that the matrix may have more than one left inverse.       .      .     We compute that .  We find , which is indeed a solution to the equation .  In the same way, we compute .     If a matrix is invertible, there is a sequence of row operations that transforms into the identity matrix . We have seen that every row operation can be performed by matrix multiplication. If the step in the Gaussian elimination process is performed by multiplying by , then we have , which means that . For each of the following matrices, find a sequence of row operations that transforms the matrix to the identity . Write the matrices that perform the steps and use them to find .   .   .   .      .   .   .     If then so that .  We use three replacement operations Therefore, so .  We use a scaling followed by replacement operations: This gives so that .     Suppose that is an matrix.  Suppose that is invertible with inverse . This means that . Explain why must be invertible with inverse .  Suppose that is invertible with inverse . Explain why is invertible. What is in terms of and ?     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .     Determine whether the following statements are true or false and explain your reasoning.  If is invertible, then the columns of are linearly independent.  If is a square matrix whose diagonal entries are all nonzero, then is invertible.  If is an invertible matrix, then span of the columns of is .  If is invertible, then there is a nonzero solution to the homogeneous equation .  If is an matrix and the equation has a solution for every vector , then is invertible.      True  False  True  False  True     True. If is invertible, then it has a pivot position in every column, which implies that the columns are linearly independent.  False. We only know this if is a triangular matrix. For instance, the matrix is not invertible.  True. If is invertible, then it has a pivot position in every row, which implies that the columns span .  False. Since there is a pivot position in every column, the homogeneous equation has only the zero solution .  True. In this case, the columns of span so there must be a pivot position in every row. Because is a square matrix, it must be row equivalent to the identity matrix .     Provide a justification for your response to the following questions.  Suppose that is a square matrix with two identical columns. Can be invertible?  Suppose that is a square matrix with two identical rows. Can be invertible?  Suppose that is an invertible matrix and that . Can you conclude that ?  Suppose that is an invertible matrix. What can you say about the span of the columns of ?  Suppose that is an invertible matrix and that is row equivalent to . Can you guarantee that is invertible?      No  No  Yes  The span is .  Yes     No. If has two identical columns, then the columns are not linearly independent. This means there is a column without a pivot position and so is not row equivalent to the identity matrix .  No. If we perform a replacement operation that multiplies one of the equal rows by and adds it to the other equal row, we obtain a matrix having a row whose entries are all zero. This says that the reduced row echelon form has such a row as well. Therefore, there is a row that does not contain a pivot position so the reduced row echelon form cannot be the identity .  Yes. If we multiply both sides of by on the left, then we see that .  The inverse is also invertible since is its inverse. Therefore, the span of the columns of is .  Yes. Since is row equivalent to the identity matrix , the matrix is as well. Therefore, is invertible.     Suppose that we start with the matrix , perform the following sequence of row operations:  Multiply row 1 by -2 and add to row 2.  Multiply row 1 by 4 and add to row 3.  Scale row 2 by .  Multiply row 2 by -1 and add to row 3,  and arrive at the upper triangular matrix    Write the matrices , , , and that perform the four row operations.  Find the matrix .  We then have . Now that we have the matrix , find the original matrix .      We have    .   .     We have   We have .  We write .     We say that two square matrices and are similar if there is an invertible matrix such that .   If and are similar, explain why and are similar as well. In particular, if , explain why .    If and are similar and is invertible, explain why is also invertible.    If and are similar and both are invertible, explain why and are similar.    If is similar to and is similar to , explain why is similar to . To begin, you may wish to assume that and .          .     .     .     .          .     .     .     .       Suppose that and are two matrices and that is invertible. We would like to explain why both and are invertible.   We first explain why is invertible.   Since is invertible, explain why any solution to the homogeneous equation is .    Use this fact to explain why any solution to must be .    Explain why must be invertible.       Now we explain why is invertible.   Since is invertible, explain why the equation is consistent for every vector .    Using the fact that is consistent for every , explain why every equation is consistent.    Explain why must be invertible.                .    If , then .    We now know that .          The span of the columns of is .    A solution to produces a solution to .    The span of the columns of is .               Since is invertible, , which says that there is pivot position in every column. Therefore, is the only solution to the equation .    If is a solution to the equation , then , which says that .    Since the only solution to the homogeneous equation is the zero solution, we know that so is invertible.          Since is invertible, the span of the columns of is , which says that every equation is consistent.    If is the solution to the equation , then satisfies , which means that is consistent.    We now know that the span of the columns of is , which tells us that is invertible.          We defined an matrix to be invertible if there is a matrix such that . In this exercise, we will explain why it is also true that , which is the statement of . This means that, if , then .  Suppose that is an -dimensional vector. Since , explain why and use this to explain why the only vector for which is .   Explain why this implies that must be invertible. We will call the inverse so that .  Beginning with , explain why and why this tells us that .      If , then .   is invertible because .  Multiply on the left by and the right by and regroup the matrix multiplications.     If , then .  Since the only solution to the homogeneous equation is , we know that the columns of are linearly independent and so has a pivot position in every column. Since is a square matrix, this tells us that so that is invertible.  If we multiply on the left by and the right by , then we have       "
},
{
  "id": "exploration-9",
  "level": "2",
  "url": "sec-matrix-inverse.html#exploration-9",
  "type": "Preview Activity",
  "number": "3.1.1",
  "title": "",
  "body": "    Explain how you would solve the equation using multiplication rather than division.  Find the matrix that rotates vectors counterclockwise by .  Find the matrix that rotates vectors clockwise by .  What do you expect the product to be? Explain the reasoning behind your expectation and then compute to verify it.  Solve the equation using Gaussian elimination.   Explain why your solution may also be found by computing .       Dividing by is the same as multiplying by , the multiplicative inverse of . We have   As we have seen a few times, the matrix is   Here, the matrix is   We should expect that since the effect of rotating by clockwise followed by rotating counterclockwise is to leave a vector unchanged. We can verify this by performing the matrix multiplication.  We have so the solution is .  The equation is asking us to find the vector that becomes after being rotated by . If we rotate by in the opposite direction, it will have this property. That is, if , then     "
},
{
  "id": "definition-12",
  "level": "2",
  "url": "sec-matrix-inverse.html#definition-12",
  "type": "Definition",
  "number": "3.1.1",
  "title": "",
  "body": "  invertible  matrix, inverse  An matrix is called invertible if there is a matrix such that , where is the identity matrix. The matrix is called the inverse of and denoted .   "
},
{
  "id": "example-17",
  "level": "2",
  "url": "sec-matrix-inverse.html#example-17",
  "type": "Example",
  "number": "3.1.2",
  "title": "",
  "body": "  Suppose that is the matrix that rotates two-dimensional vectors counterclockwise by and that rotates vectors by . We have We can check that which shows that is invertible and that .  Notice that if we multiply the matrices in the opposite order, we find that , which says that is also invertible and that . In other words, and are inverses of each other.   "
},
{
  "id": "activity-28",
  "level": "2",
  "url": "sec-matrix-inverse.html#activity-28",
  "type": "Activity",
  "number": "3.1.2",
  "title": "",
  "body": "  This activity demonstrates a procedure for finding the inverse of a matrix .   Suppose that . To find an inverse , we write its columns as and require that In other words, we can find the columns of by solving the equations Solve these equations to find and . Then write the matrix and verify that . This is enough for us to conclude that is the inverse of .     Find the product and explain why we now know that is invertible and .     What happens when you try to find the inverse of ?    We now develop a condition that must be satisfied by an invertible matrix. Suppose that is an invertible matrix with inverse and suppose that is any -dimensional vector. Since , we have This says that the equation is consistent and that is a solution.  Since we know that is consistent for any vector , what does this say about the span of the columns of ?    Since is a square matrix, what does this say about the pivot positions of ? What is the reduced row echelon form of ?    In this activity, we have studied the matrices Find the reduced row echelon form of each and explain how those forms enable us to conclude that one matrix is invertible and the other is not.        Solving the two equations for and gives . We can verify that, as we expect, .  We find that , which is the condition that tells us that is invertible.  Seeking the first column of , we see that the equation is not consistent. This means that is not invertible.  Since the equation is consistent for every , we know that the span of the columns of is .  Because the span of the columns of is , there is a pivot position in every row. Since is square, there is also a pivot position in every column. This means that the reduced row echelon form of must be the identity matrix .  We see that which shows that is invertible and is not.    "
},
{
  "id": "example-inverse-augment-I",
  "level": "2",
  "url": "sec-matrix-inverse.html#example-inverse-augment-I",
  "type": "Example",
  "number": "3.1.3",
  "title": "",
  "body": "  We can reformulate this procedure for finding the inverse of a matrix. For the sake of convenience, suppose that is a invertible matrix with inverse . Rather than solving the equations separately, we can solve them at the same time by augmenting by both vectors and and finding the reduced row echelon form.  For example, if , we form This shows that the matrix is the inverse of .  In other words, beginning with , we augment by the identify and find the reduced row echelon form to determine :    "
},
{
  "id": "proposition-inverse-solve",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-inverse-solve",
  "type": "Proposition",
  "number": "3.1.4",
  "title": "",
  "body": "  If is an invertible matrix with inverse , then any equation is consistent and is a solution. In other words, the solution to is .   "
},
{
  "id": "proposition-invertible-rref",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-invertible-rref",
  "type": "Proposition",
  "number": "3.1.5",
  "title": "",
  "body": "  The matrix is invertible if and only if the reduced row echelon form of is the identity matrix: . In addition, we can find the inverse by augmenting by the identity and finding the reduced row echelon form:    "
},
{
  "id": "proposition-inverse-inverse",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-inverse-inverse",
  "type": "Proposition",
  "number": "3.1.6",
  "title": "",
  "body": "  If is a invertible matrix with inverse , then , which tells us that is invertible with inverse . In other words,    "
},
{
  "id": "activity-29",
  "level": "2",
  "url": "sec-matrix-inverse.html#activity-29",
  "type": "Activity",
  "number": "3.1.3",
  "title": "",
  "body": "  We'll begin by considering the square matrix    Describe the solution space to the equation by augmenting and finding the reduced row echelon form.     Using , explain why is invertible and find its inverse.    Now use the inverse to solve the equation and verify that your result agrees with what you found in part a.    If you have defined a matrix B in Sage, you can find it's inverse as B.inverse() or B^-1 . Use Sage to find the inverse of the matrix and use it to solve the equation .     If and are the two matrices defined in this activity, find their product and verify that it is invertible.    Compute the products and . Which one agrees with ?    Explain your finding by considering the product and using associativity to regroup the products so that the middle two terms are multiplied first.        Constructing the augmented matrix, we see that which says that there is a unique solution .    Our work in part a shows that from which we conclude that is invertible. To find the inverse, which says that     We see that .    Sage tells us that .    Sage helps us see that , which tells us that is invertible.   We find that .   We see that      "
},
{
  "id": "proposition-invertible-properties",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-invertible-properties",
  "type": "Proposition",
  "number": "3.1.7",
  "title": "Properties of invertible matrices.",
  "body": " Properties of invertible matrices     An matrix is invertible if and only if .  If is invertible, then the solution to the equation is given by .  We can find by finding the reduced row echelon form of ; namely, .  If and are two invertible matrices, then their product is also invertible and .     "
},
{
  "id": "definition-13",
  "level": "2",
  "url": "sec-matrix-inverse.html#definition-13",
  "type": "Definition",
  "number": "3.1.8",
  "title": "",
  "body": "  lower triangular matrix  upper triangular matrix  We say that a matrix is lower triangular if all its entries above the diagonal are zero. Similarly, is upper triangular if all the entries below the diagonal are zero.   "
},
{
  "id": "proposition-triangular-invertibility",
  "level": "2",
  "url": "sec-matrix-inverse.html#proposition-triangular-invertibility",
  "type": "Proposition",
  "number": "3.1.9",
  "title": "",
  "body": "  An triangular matrix is invertible if and only if the entries on the diagonal are all nonzero.   "
},
{
  "id": "activity-30",
  "level": "2",
  "url": "sec-matrix-inverse.html#activity-30",
  "type": "Activity",
  "number": "3.1.4",
  "title": "Gaussian elimination and matrix multiplication.",
  "body": " Gaussian elimination and matrix multiplication   This activity explores how the row operations of scaling, interchange, and replacement can be performed using matrix multiplication.  As an example, we consider the matrix and apply a replacement operation that multiplies the first row by and adds it to the second row. Rather than performing this operation in the usual way, we construct a new matrix by applying the desired replacement operation to the identity matrix. To illustrate, we begin with the identity matrix and form a new matrix by multiplying the first row by and adding it to the second row to obtain   Show that the product is the result of applying the replacement operation to .   Explain why is invertible and find its inverse .  Describe the relationship between and and use the connection to replacement operations to explain why it holds.  Other row operations can be performed using a similar procedure. For instance, suppose we want to scale the second row of by . Find a matrix so that is the same as that obtained from the scaling operation. Why is invertible and what is ?   Finally, suppose we want to interchange the first and third rows of . Find a matrix , usually called a permutation matrix that performs this operation. What is ?  The original matrix is seen to be row equivalent to the upper triangular matrix by performing three replacement operations on : Find the matrices , , and that perform these row replacement operations so that .  Explain why the matrix product is invertible and use this fact to write . What is the matrix that you find? Why do you think we denote it by ?        Performing the matrix multiplication, we find that   We know that is invertible because it is a lower triangular matrix whose diagonal entries are all 1. We find that , which can be verified.  But we can see this in another way as well. The replacement operation is reversible; that is, multiplying the first row by and adding it to the second row can be undone by multiplying the first row by and adding it to the second row.  We find that This makes sense because scaling a row by can be undone by scaling the same row by .  We find that Moreover, because we can undo the interchange operation by repeating it.  Continuing with the Gaussian elimination algorithm, we have , as above, we then have .  Each of the matrices , , and is invertible so their product will be as well. Since , we have . Moreover, gives . Notice that this matrix is lower triangular so we call it .    "
},
{
  "id": "example-19",
  "level": "2",
  "url": "sec-matrix-inverse.html#example-19",
  "type": "Example",
  "number": "3.1.10",
  "title": "",
  "body": "  Suppose we have For the forward substitution phase of Gaussian elimination, we perform a sequence of three replacement operations. The first replacement operation multiplies the first row by and adds the result to the second row. We can perform this operation by multiplying by the lower triangular matrix where   The next two replacement operations are performed by the matrices so that   Notice that the inverse of has the simple form: . This says that if we want to undo the operation of multiplying the first row by and adding to the second row, we should multiply the first row by and add it to the second row. That is the effect of .  Notice that we now have , which gives where is the lower triangular matrix This way of writing as the product of a lower and an upper triangular matrix is known as an factorization of , and its usefulness will be explored in .   "
},
{
  "id": "exercise-82",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-82",
  "type": "Exercise",
  "number": "3.1.5.1",
  "title": "",
  "body": " Consider the matrix .   Explain why has an inverse.  Find the inverse of by augmenting by the identity to form .  Use your inverse to solve the equation .             .     We see that , the identity matrix, which implies that has an inverse.  We have which says that   We compute that .   "
},
{
  "id": "exercise-83",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-83",
  "type": "Exercise",
  "number": "3.1.5.2",
  "title": "",
  "body": " In this exercise, we will consider matrices as defining matrix transformations.  Write the matrix that performs a rotation. What geometric operation undoes this rotation? Find the matrix that perform this operation and verify that it is .  Write the matrix that performs a rotation. Verify that so that , and explain geometrically why this is the case.  Find three more matrices that satisfy .      and .   .         . To undo the rotation, we will perform a clockwise rotation, which is defined by the matrix .   . We see that because rotating by is its own inverse.  We can do this by constructing matrices that define reflections, such as    "
},
{
  "id": "exercise-84",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-84",
  "type": "Exercise",
  "number": "3.1.5.3",
  "title": "",
  "body": " Inverses for certain types of matrices can be found in a relatively straightforward fashion.   The matrix is called diagonal since the only nonzero entries are on the diagonal of the matrix.     Find by augmenting by the identity and finding its reduced row echelon form.    Under what conditions is a diagonal matrix invertible?    Explain why the inverse of a diagonal matrix is also diagonal and explain the relationship between the diagonal entries in and .       Consider the lower triangular matrix .   Find by augmenting by the identity and finding its reduced row echelon form.    Explain why the inverse of a lower triangular matrix is also lower triangular.                .    When the entries on the diagonal are all nonzero.    Consider the steps performed in row reducing augmented by .           .    Consider the steps in row reducing augmented by .                .    When the entries on the diagonal are all nonzero.    Because the process of row reducing augmented by can be performed using only scalings. Then the diagonal entries of and are reciprocals of one another.           .     is always lower triangular because the only row operations needed to row reduce augmented by are replacements in which a multiple of one row is added to a row underneath it.        "
},
{
  "id": "exercise-85",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-85",
  "type": "Exercise",
  "number": "3.1.5.4",
  "title": "",
  "body": " Our definition of an invertible matrix requires that be a square matrix. Let's examine what happens when is not square. For instance, suppose that .  Verify that . In this case, we say that is a left inverse of .   If has a left inverse , we can still use it to find solutions to linear equations. If we know there is a solution to the equation , we can multiply both sides of the equation by to find .  Suppose you know there is a solution to the equation . Use the left inverse to find and verify that it is a solution.  Now consider the matrix and verify that is also a left inverse of . This shows that the matrix may have more than one left inverse.       .      .     We compute that .  We find , which is indeed a solution to the equation .  In the same way, we compute .   "
},
{
  "id": "exercise-86",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-86",
  "type": "Exercise",
  "number": "3.1.5.5",
  "title": "",
  "body": " If a matrix is invertible, there is a sequence of row operations that transforms into the identity matrix . We have seen that every row operation can be performed by matrix multiplication. If the step in the Gaussian elimination process is performed by multiplying by , then we have , which means that . For each of the following matrices, find a sequence of row operations that transforms the matrix to the identity . Write the matrices that perform the steps and use them to find .   .   .   .      .   .   .     If then so that .  We use three replacement operations Therefore, so .  We use a scaling followed by replacement operations: This gives so that .   "
},
{
  "id": "exercise-87",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-87",
  "type": "Exercise",
  "number": "3.1.5.6",
  "title": "",
  "body": " Suppose that is an matrix.  Suppose that is invertible with inverse . This means that . Explain why must be invertible with inverse .  Suppose that is invertible with inverse . Explain why is invertible. What is in terms of and ?     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .     If has inverse , then . This means that so is invertible with inverse .  In the same way, we have , which shows that .   "
},
{
  "id": "exercise-88",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-88",
  "type": "Exercise",
  "number": "3.1.5.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.  If is invertible, then the columns of are linearly independent.  If is a square matrix whose diagonal entries are all nonzero, then is invertible.  If is an invertible matrix, then span of the columns of is .  If is invertible, then there is a nonzero solution to the homogeneous equation .  If is an matrix and the equation has a solution for every vector , then is invertible.      True  False  True  False  True     True. If is invertible, then it has a pivot position in every column, which implies that the columns are linearly independent.  False. We only know this if is a triangular matrix. For instance, the matrix is not invertible.  True. If is invertible, then it has a pivot position in every row, which implies that the columns span .  False. Since there is a pivot position in every column, the homogeneous equation has only the zero solution .  True. In this case, the columns of span so there must be a pivot position in every row. Because is a square matrix, it must be row equivalent to the identity matrix .   "
},
{
  "id": "exercise-89",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-89",
  "type": "Exercise",
  "number": "3.1.5.8",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose that is a square matrix with two identical columns. Can be invertible?  Suppose that is a square matrix with two identical rows. Can be invertible?  Suppose that is an invertible matrix and that . Can you conclude that ?  Suppose that is an invertible matrix. What can you say about the span of the columns of ?  Suppose that is an invertible matrix and that is row equivalent to . Can you guarantee that is invertible?      No  No  Yes  The span is .  Yes     No. If has two identical columns, then the columns are not linearly independent. This means there is a column without a pivot position and so is not row equivalent to the identity matrix .  No. If we perform a replacement operation that multiplies one of the equal rows by and adds it to the other equal row, we obtain a matrix having a row whose entries are all zero. This says that the reduced row echelon form has such a row as well. Therefore, there is a row that does not contain a pivot position so the reduced row echelon form cannot be the identity .  Yes. If we multiply both sides of by on the left, then we see that .  The inverse is also invertible since is its inverse. Therefore, the span of the columns of is .  Yes. Since is row equivalent to the identity matrix , the matrix is as well. Therefore, is invertible.   "
},
{
  "id": "exercise-90",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-90",
  "type": "Exercise",
  "number": "3.1.5.9",
  "title": "",
  "body": " Suppose that we start with the matrix , perform the following sequence of row operations:  Multiply row 1 by -2 and add to row 2.  Multiply row 1 by 4 and add to row 3.  Scale row 2 by .  Multiply row 2 by -1 and add to row 3,  and arrive at the upper triangular matrix    Write the matrices , , , and that perform the four row operations.  Find the matrix .  We then have . Now that we have the matrix , find the original matrix .      We have    .   .     We have   We have .  We write .   "
},
{
  "id": "exercise-91",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-91",
  "type": "Exercise",
  "number": "3.1.5.10",
  "title": "",
  "body": " We say that two square matrices and are similar if there is an invertible matrix such that .   If and are similar, explain why and are similar as well. In particular, if , explain why .    If and are similar and is invertible, explain why is also invertible.    If and are similar and both are invertible, explain why and are similar.    If is similar to and is similar to , explain why is similar to . To begin, you may wish to assume that and .          .     .     .     .          .     .     .     .     "
},
{
  "id": "exercise-92",
  "level": "2",
  "url": "sec-matrix-inverse.html#exercise-92",
  "type": "Exercise",
  "number": "3.1.5.11",
  "title": "",
  "body": " Suppose that and are two matrices and that is invertible. We would like to explain why both and are invertible.   We first explain why is invertible.   Since is invertible, explain why any solution to the homogeneous equation is .    Use this fact to explain why any solution to must be .    Explain why must be invertible.       Now we explain why is invertible.   Since is invertible, explain why the equation is consistent for every vector .    Using the fact that is consistent for every , explain why every equation is consistent.    Explain why must be invertible.                .    If , then .    We now know that .          The span of the columns of is .    A solution to produces a solution to .    The span of the columns of is .               Since is invertible, , which says that there is pivot position in every column. Therefore, is the only solution to the equation .    If is a solution to the equation , then , which says that .    Since the only solution to the homogeneous equation is the zero solution, we know that so is invertible.          Since is invertible, the span of the columns of is , which says that every equation is consistent.    If is the solution to the equation , then satisfies , which means that is consistent.    We now know that the span of the columns of is , which tells us that is invertible.        "
},
{
  "id": "ex-right-inverse",
  "level": "2",
  "url": "sec-matrix-inverse.html#ex-right-inverse",
  "type": "Exercise",
  "number": "3.1.5.12",
  "title": "",
  "body": " We defined an matrix to be invertible if there is a matrix such that . In this exercise, we will explain why it is also true that , which is the statement of . This means that, if , then .  Suppose that is an -dimensional vector. Since , explain why and use this to explain why the only vector for which is .   Explain why this implies that must be invertible. We will call the inverse so that .  Beginning with , explain why and why this tells us that .      If , then .   is invertible because .  Multiply on the left by and the right by and regroup the matrix multiplications.     If , then .  Since the only solution to the homogeneous equation is , we know that the columns of are linearly independent and so has a pivot position in every column. Since is a square matrix, this tells us that so that is invertible.  If we multiply on the left by and the right by , then we have     "
},
{
  "id": "sec-bases",
  "level": "1",
  "url": "sec-bases.html",
  "type": "Section",
  "number": "3.2",
  "title": "Bases and coordinate systems",
  "body": " Bases and coordinate systems   Standard Cartesian coordinates are commonly used to describe points in the plane. If we mention the point , we know that we arrive at this point from the origin by moving four units to the right and three units up.  Sometimes, however, it is more natural to work in a different coordinate system. Suppose that you live in the city whose map is shown in and that you would like to give a guest directions for getting from your house to the store. You would probably say something like, \"Go four blocks up Maple. Then turn left on Main for three blocks.\" The grid of streets in the city gives a more natural coordinate system than standard north-south, east-west coordinates.      A city map.   In this section, we will develop the concept of a basis through which we will create new coordinate systems in . We will see that the right choice of a coordinate system provides a more natural way to approach some problems.    Consider the vectors in , which are shown in .      Linear combinations of and .     Indicate the linear combination on the figure.  Express the vector as a linear combination of and .  Find the linear combination .  Express the vector as a linear combination of and .  Explain why every vector in can be written as a linear combination of and in exactly one way.      We can see graphically, or we can compute, that .  Again, we graphically see that .  Since the linear combination extends beyond the figure, we compute that .  We need to find the solution to the linear system , which is .  The matrix has a pivot position in every row and every column.      In the preview activity, we worked with a set of two vectors in and found that we could express any vector in in two different ways: in the usual way where the components of the vector describe horizontal and vertical changes, and in a new way as a linear combination of and . We could also translate between these two descriptions. This example illustrates the central idea of this section.    Bases  In the preview activity, we created a new coordinate system for using linear combinations of a set of two vectors. More generally, the following definition will guide us.    basis  A set of vectors in is called a basis for if the set of vectors spans and is linearly independent.      We will look at some examples of bases in this activity.  In the preview activity, we worked with the set of vectors in : . Explain why these vectors form a basis for .  Consider the set of vectors in  and determine whether they form a basis for .   Do the vectors form a basis for ?  Explain why the vectors form a basis for .  If a set of vectors forms a basis for , what can you guarantee about the pivot positions of the matrix ?  If the set of vectors is a basis for , how many vectors must be in the set?       The matrix is row equivalent to the identity matrix so it has a pivot position in every row. The span of the columns is therefore . There is also a pivot position in every column, which means that the columns are linearly independent.  We note that Since there is a pivot position in every row, the span of the vectors is . Since there is a pivot position in every column, the vectors are linearly independent. Consequently, this set of vectors forms a basis for .  The matrix whose columns are the vectors , , , and has dimensions . Therefore, there cannot be a pivot position in every column, which tells us that the columns cannot be linearly independent. Therefore, the set of vectors do not form a basis for .  Putting these vectors into a matrix produces the identity matrix, which has a pivot position in every row and every column. Therefore, the span of the vectors is , and they are linearly independent.  There must be a pivot position in every row and every column.  A basis for must have vectors. Because the associated matrix must have a pivot position in every row and every column, there must be the same number of columns as there are rows. Since the vectors are -dimensional, there must be 10 vectors.     We can develop a test to determine if a set of vectors forms a basis for by considering the matrix . To be a basis, this set of vectors must span and be linearly independent.  We know that the span of the set of vectors is if and only if has a pivot position in every row. We also know that the set of vectors is linearly independent if and only if has a pivot position in every column. This means that a set of vectors forms a basis if and only if has a pivot in every row and every column. Therefore, must be row equivalent to the identify matrix : .  In addition to helping identify bases, this fact tells us something important about the number of vectors in a basis. Since the matrix has a pivot position in every row and every column, it must have the same number of rows as columns. Therefore, the number of vectors in a basis for must be . For example, a basis for must have exactly 23 vectors.    A set of vectors forms a basis for if and only if the matrix This means there must be vectors in a basis for .      Notice that the vectors form the columns of the identity matrix, which implies that this set forms a basis for . More generally, the set of vectors forms a basis for , which we call the standard basis for . basis, standard       Coordinate systems  A basis for forms a coordinate system for , as we will describe. Rather than continuing to write a list of vectors, we will find it convenient to denote a basis using a single symbol, such as     In this section's preview activity, we considered the vectors , which form a basis for .   In the standard coordinate system, the point is found by moving 2 units to the right and 3 units down. We would like to define a new coordinate system where we interpret to mean we move two times along and 3 times along . As we see in the figure, doing so leaves us at the point , expressed in the usual coordinate system.    We have seen that . The coordinates of the vector in the new coordinate system are the weights that we use to create as a linear combination of and .  Since we now have two descriptions of the vector , we need some notation to keep track of which coordinate system we are using. Because , we will write . More generally, will denote the coordinates of in the basis ; that is, is the vector of weights such that .  For example, if the coordinates of in the basis are , then and we conclude that . This demonstrates how we can translate coordinates in the basis into standard coordinates.  Suppose we know the expression of a vector in standard coordinates. How can we find its coordinates in the basis ? For instance, suppose and that we would like to find . We can write which means that or This linear system for the weights defines an augmented matrix . which means that .    This example illustrates how a basis in provides a new coordinate system for and shows how we may translate between this coordinate system and the standard one.  More generally, suppose that is a basis for . We know that the span of the vectors is , which implies that any vector in can be written as a linear combination of the vectors. In addition, we know that the vectors are linearly independent, which means that we can write as a linear combination of the vectors in exactly one way. Therefore, we have where the weights are unique. In this case, we write the coordinate description of in the basis as .    Let's begin with the basis of where .  If the coordinates of in the basis are , what is the vector ?  If , find the coordinates of in the basis ; that is, find .  Find a matrix such that, for any vector , we have . Explain why this matrix is invertible.  Using what you found in the previous part, find a matrix such that, for any vector , we have . What is the relationship between the two matrices and ? Explain why this relationship holds.  Suppose we consider the standard basis . What is the relationship between and ?  Suppose we also consider the basis . Find a matrix that converts coordinates in the basis into coordinates in the basis ; that is, . You may wish to think about converting coordinates from the basis into the standard coordinate system and then into the basis .      We know that .  We solve the linear system to find .  If , we have This matrix , whose columns are the vectors and , has a pivot position in every row and every column because the vectors form a basis. It is, therefore, row equivalent to the identity matrix and hence invertible.  Since we have , we also have .  We have   If we define to be the matrix whose columns are and , then Therefore,      This activity demonstrates how we can efficiently convert between coordinate systems defined by different bases. Let's consider a basis and a vector . We know that If we use to denote the matrix whose columns are the basis vectors, then we find that where . This means that the matrix converts coordinates in the basis into standard coordinates.  Since the columns of are the basis vectors , we know that , and is therefore invertible. Since we have , we must also have .    If is a basis and the matrix whose columns are the basis vectors, then     If we have another basis , we find, in the same way, that for the conversion between coordinates in the basis into standard coordinates. We then have . Therefore, is the matrix that converts -coordinates into -coordinates.    Examples of bases  We will now look at some examples of bases that illustrate how it can be useful to study a problem using a different coordinate system.    Let's consider the basis of : It is relatively straightforward to convert a vector's representation in this basis into to the standard basis using the matrix whose columns are the basis vectors: For example, suppose that the vector is described in the coordinate system defined by the basis as . We then have .  Consider now the vector . If we would like to express in the coordinate system defined by , then we compute .      Suppose we work for a company that records its quarterly revenue, in millions of dollars, as:   A company's quarterly revenue    Quarter  Revenue    1  10.3   2  13.1   3  7.5   4  8.2      Rather than using a table to record the data, we could display it in a graph or write it as a vector in : .    Let's consider a new basis for using vectors . We may view these basis elements graphically, as in       A representation of the basis elements of .   To convert our revenue vectors into the coordinates given by , we form the matrices: In particular, if the revenue vector is , then Notice that the first component of is the average of the components of .  For our particular revenue vector , we have This means that our revenue vector is . We will think about what these terms mean by adding them together one at a time.    The first term, gives us the average revenue over the year.     The average revenue for the first two quarters is 11.7, which is 1.925 million dollars above the yearly average. Similarly, the average revenue for the last two quarters is 1.925 million dollars below the yearly average. This is recorded by the second term      Finally, the first quarter's revenue is 1.400 million dollars below the average over the first two quarters and the second quarter's revenue is 1.400 million dollars above that average. This, and the corresponding data for the last two quarters, is captured by the last two terms:      If we write , we see that the coefficient measures the average revenue over the year, measures the deviation from the annual average in the first and second halves of the year, and measures how the revenue in the first and second quarter differs from the average in the first half of the year. In this way, the coefficients provide a view of the revenue over different time scales, from an annual summary to a finer view of quarterly behavior.  This basis is sometimes called a Haar wavelet basis, and the change of basis is known as a Haar wavelet transform. In the next section, we will see how this basis provides a useful way to store digital images.     Edge detection   An important problem in the field of computer vision is to detect edges in a digital photograph, as is shown in . Edge detection algorithms are useful when, say, we want a robot to locate an object in its field of view. Graphic designers also use these algorithms to create artistic effects.       A canyon wall in Capitol Reef National Park and the result of an edge detection algorithm.   We will consider a very simple version of an edge detection algorithm to give a sense of how this works. Rather than considering a two-dimensional photograph, we will think about a one-dimensional row of pixels in a photograph. The grayscale values of a pixel measure the brightness of a pixel; a grayscale value of 0 corresponds to black, and a value of 255 corresponds to white.  Suppose, for simplicity, that the grayscale values for a row of six pixels are represented by a vector in :    .    We can easily see that there is a jump in brightness between pixels 4 and 5, but how can we detect it computationally? We will introduce a new basis for with vectors: .  Construct the matrix that relates the standard coordinate system with the coordinates in the basis .  Determine the matrix that converts the representation of in standard coordinates into the coordinate system defined by .   Suppose the vectors are expressed in general terms as . Using the relationship , determine an expression for the coefficient in terms of . What does measure in terms of the grayscale values of the pixels? What does measure in terms of the grayscale values of the pixels?  Now for the specific vector , determine the representation of in the -coordinate system.  Explain how the coefficients in determine the location of the jump in brightness in the grayscale values represented by the vector .    Readers who are familiar with calculus may recognize that this change of basis converts a vector into , the set of changes in . This process is similar to differentiation in calculus. Similarly, the process of converting into the vector adds together the changes in a process similar to integration. As a result, this change of basis represents a linear algebraic version of the Fundamental Theorem of Calculus.     We form the matrix   We find that   We see that so measures the change in brightness between one pixel and its neighbor. Similarly, , which measures another change in brightness.  We compute that   Most of the coefficients that measure changes are relatively small in absolute value. The coefficient , however, which measures the change in brightness between the fourth and fifth pixel, has a large absolute value. This tells us that there is a large change in brightness between the fourth and fifth pixel, which points to an edge in the image.       Summary  We defined a basis to be a set of vectors that is linearly independent and whose span is .  A set of vectors forms a basis for if and only if the matrix . This means there must be vectors in a basis for .  If forms a basis for , then any vector in can be written as a linear combination of the vectors in exactly one way.  We used the basis to define a coordinate system in which , the coordinates of in the basis , are defined by .   Forming the matrix whose columns are the basis vectors, we can convert between coordinate systems: .       Shown in are two vectors and in the plane .      Vectors and in .    Explain why is a basis for .  Using , indicate the vectors such that            Using , find the representation if   .   .   .   Find if .     The vectors are linearly independent and span .  The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .     .     The vectors are linearly independent and span . We can see this by forming the matrix   The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .    We form the matrix so that . We then need to solve the equation , which gives .     Consider vectors and let and .  Explain why and are both bases of .  If , find and .   If , find and .  If , find and .  Find a matrix such that .     The sets of the vectors are both linearly independent and span .   and .   and .   and .   .     In both cases, we see that so that both sets of vectors are linearly independent and span .  We solve and to find and .  We have . We then solve to find .  In the same way, we find and .  We have , which shows that .     Consider the following vectors in : .  Explain why forms a basis for .  Explain how to convert , the representation of a vector in the coordinates defined by , into , its representation in the standard coordinate system.  Explain how to convert the vector into , its representation in the coordinate system defined by .  If , find .  If , find .     The vectors are linearly independent and span .  We have .  We have .   .   .     Form the matrix which shows that the vectors are linearly independent and span .  We have .  We have where   We find .  We find .     Consider the following vectors in : .  Do these vectors form a basis for ? Explain your thinking.   Find a subset of these vectors that forms a basis of .  Suppose you have a set of vectors in such that . Find a subset of the vectors that forms a basis for .     No, because a basis for must contain exactly three vectors.   , , and .   , , , and .     Looking at the reduced row echelon form, we find This shows that the vectors are not linearly independent since there is not a pivot position in every column. Therefore, the set of vectors does not form a basis for . Of course, we also know this because a set of vectors for must contain exactly three vectors.  From the reduced row echelon form, we see that the set of vectors spans because there is a pivot position in every row. We also see that and . This means that , , and will span and therefore form a basis.  In the same way, we see that , , , and for a basis for .     This exercise involves a simple Fourier transform, which will play an important role in the next section.  Suppose that we have the vectors .  Explain why is a basis for . Notice that you may enter into Sage as cos(pi\/6) .   If , find .  Find the matrices and . If and , explain why is the average of , , and .      The vectors are linearly independent and span .   .  Since we have , we have .     By forming the matrix and finding its reduced row echelon form, we see that the vectors are linearly independent and span . They therefore form a basis of .  We solve to find .  We have Since we have , we have .     Determine whether the following statements are true or false and provide a justification for your response.  If the columns of a matrix form a basis for , then is invertible.  There must be 125 vectors in a basis for .  If is a basis of , then every vector in can be expressed as a linear combination of basis vectors.  The coordinates are the weights that form as a linear combination of basis vectors.  If the basis vectors form the columns of the matrix , then .      True  True  True  True  False     True. If the columns of form a basis, then has a pivot position in every row and every column. Therefore, the reduced row echelon form of is the identity matrix, which implies that is invertible.  True. The number of vectors in a basis of must be .  True. If is a basis, then the vectors in span , which means that every vector in can be written as a linear combination of the vectors in .  True. This is the definition of .  False. The relationship is .     Provide a justification for your response to each of the following questions.  Suppose you have linearly independent vectors in . Can you guarantee that they form a basis of ?  If is an invertible matrix, do the columns necessarily form a basis of ?  Suppose we have an invertible matrix , and we perform a sequence of row operations on to form a matrix . Can you guarantee that the columns of form a basis for ?  Suppose you have a set of 10 vectors in and that every vector in can be written as a linear combination of these vectors. Can you guarantee that this set of vectors is a basis for ?     Yes  Yes  Yes  Yes     Yes. A matrix formed from linearly independent vectors in will have a pivot position in every column. Since the matrix has the same number of rows and columns, there must also be a pivot position in every row. This means that the vectors span and therefore form a basis.  Yes. An invertible matrix is row equivalent to the identity matrix, which means that the columns are linearly independent and span . This implies that the columns form a basis of .  Yes. The matrix is row equivalent to the identity matrix so must be as well. This means that the columns of form a basis for .  Yes. The span of the set of vectors is , which says that the associated matrix is square and has a pivot position in every row. Therefore, it must have a pivot position in every column, which means that the set of vectors forms a basis for .     Crystallographers find it convenient to use coordinate systems that are adapted to the specific geometry of a crystal. As a two-dimensional example, consider a layer of graphite in which carbon atoms are arranged in regular hexagons to form the crystalline structure shown in .      A layer of carbon atoms in a graphite crystal.   The origin of the coordinate system is at the carbon atom labeled by 0 . It is convenient to choose the basis defined by the vectors and and the coordinate system it defines.   Locate the points for which   ,   ,   .    Find the coordinates for all the carbon atoms in the hexagon whose lower left vertex is labeled 0 .  What are the coordinates of the center of that hexagon, which is labeled C ?  How do the coordinates of the atoms in the hexagon whose lower left corner is labeled 1 compare to the coordinates in the hexagon whose lower left corner is labeled \"0\"?  Does the point whose coordinates are correspond to a carbon atom or the center of a hexagon?     The points are indicated in the figure.      .   .  The coordinates differ by   It is the center of a hexagon.      The points are indicated in .   The points requested in part a of this exercise.       Moving counterclockwise around the hexagon, the coordinates are    .  We obtain the coordinates for the hexagon with the vertex labeled 1 by adding the coordinate expression of the point 1 , which is to those of the original hexagon.  It is the center of a hexagon. Adding or subtracting to the coordinates translates one hexagon to another. This means that can be translated to , which is the center of a hexagon.     Suppose that and .  Explain why is a basis for .  Find and .  Use what you found in the previous part of this problem to find and .   If , find .  Find a matrix such that .  You should find that the matrix is a very simple matrix, which means that this basis is well suited to study the effect of multiplication by . This observation is the central idea of the next chapter.    The vectors are linearly independent and span .   and .   and .   .   .     The vectors are linearly independent and span .  We compute that and .   and .  We know that , which means that . Therefore, .  If , then and . Therefore, , which says that .     "
},
{
  "id": "fig-city-map",
  "level": "2",
  "url": "sec-bases.html#fig-city-map",
  "type": "Figure",
  "number": "3.2.1",
  "title": "",
  "body": "    A city map.  "
},
{
  "id": "exploration-10",
  "level": "2",
  "url": "sec-bases.html#exploration-10",
  "type": "Preview Activity",
  "number": "3.2.1",
  "title": "",
  "body": "  Consider the vectors in , which are shown in .      Linear combinations of and .     Indicate the linear combination on the figure.  Express the vector as a linear combination of and .  Find the linear combination .  Express the vector as a linear combination of and .  Explain why every vector in can be written as a linear combination of and in exactly one way.      We can see graphically, or we can compute, that .  Again, we graphically see that .  Since the linear combination extends beyond the figure, we compute that .  We need to find the solution to the linear system , which is .  The matrix has a pivot position in every row and every column.     "
},
{
  "id": "definition-14",
  "level": "2",
  "url": "sec-bases.html#definition-14",
  "type": "Definition",
  "number": "3.2.3",
  "title": "",
  "body": "  basis  A set of vectors in is called a basis for if the set of vectors spans and is linearly independent.   "
},
{
  "id": "activity-31",
  "level": "2",
  "url": "sec-bases.html#activity-31",
  "type": "Activity",
  "number": "3.2.2",
  "title": "",
  "body": "  We will look at some examples of bases in this activity.  In the preview activity, we worked with the set of vectors in : . Explain why these vectors form a basis for .  Consider the set of vectors in  and determine whether they form a basis for .   Do the vectors form a basis for ?  Explain why the vectors form a basis for .  If a set of vectors forms a basis for , what can you guarantee about the pivot positions of the matrix ?  If the set of vectors is a basis for , how many vectors must be in the set?       The matrix is row equivalent to the identity matrix so it has a pivot position in every row. The span of the columns is therefore . There is also a pivot position in every column, which means that the columns are linearly independent.  We note that Since there is a pivot position in every row, the span of the vectors is . Since there is a pivot position in every column, the vectors are linearly independent. Consequently, this set of vectors forms a basis for .  The matrix whose columns are the vectors , , , and has dimensions . Therefore, there cannot be a pivot position in every column, which tells us that the columns cannot be linearly independent. Therefore, the set of vectors do not form a basis for .  Putting these vectors into a matrix produces the identity matrix, which has a pivot position in every row and every column. Therefore, the span of the vectors is , and they are linearly independent.  There must be a pivot position in every row and every column.  A basis for must have vectors. Because the associated matrix must have a pivot position in every row and every column, there must be the same number of columns as there are rows. Since the vectors are -dimensional, there must be 10 vectors.    "
},
{
  "id": "proposition-20",
  "level": "2",
  "url": "sec-bases.html#proposition-20",
  "type": "Proposition",
  "number": "3.2.4",
  "title": "",
  "body": "  A set of vectors forms a basis for if and only if the matrix This means there must be vectors in a basis for .   "
},
{
  "id": "example-20",
  "level": "2",
  "url": "sec-bases.html#example-20",
  "type": "Example",
  "number": "3.2.5",
  "title": "",
  "body": "  Notice that the vectors form the columns of the identity matrix, which implies that this set forms a basis for . More generally, the set of vectors forms a basis for , which we call the standard basis for . basis, standard    "
},
{
  "id": "example-21",
  "level": "2",
  "url": "sec-bases.html#example-21",
  "type": "Example",
  "number": "3.2.6",
  "title": "",
  "body": "  In this section's preview activity, we considered the vectors , which form a basis for .   In the standard coordinate system, the point is found by moving 2 units to the right and 3 units down. We would like to define a new coordinate system where we interpret to mean we move two times along and 3 times along . As we see in the figure, doing so leaves us at the point , expressed in the usual coordinate system.    We have seen that . The coordinates of the vector in the new coordinate system are the weights that we use to create as a linear combination of and .  Since we now have two descriptions of the vector , we need some notation to keep track of which coordinate system we are using. Because , we will write . More generally, will denote the coordinates of in the basis ; that is, is the vector of weights such that .  For example, if the coordinates of in the basis are , then and we conclude that . This demonstrates how we can translate coordinates in the basis into standard coordinates.  Suppose we know the expression of a vector in standard coordinates. How can we find its coordinates in the basis ? For instance, suppose and that we would like to find . We can write which means that or This linear system for the weights defines an augmented matrix . which means that .   "
},
{
  "id": "activity-32",
  "level": "2",
  "url": "sec-bases.html#activity-32",
  "type": "Activity",
  "number": "3.2.3",
  "title": "",
  "body": "  Let's begin with the basis of where .  If the coordinates of in the basis are , what is the vector ?  If , find the coordinates of in the basis ; that is, find .  Find a matrix such that, for any vector , we have . Explain why this matrix is invertible.  Using what you found in the previous part, find a matrix such that, for any vector , we have . What is the relationship between the two matrices and ? Explain why this relationship holds.  Suppose we consider the standard basis . What is the relationship between and ?  Suppose we also consider the basis . Find a matrix that converts coordinates in the basis into coordinates in the basis ; that is, . You may wish to think about converting coordinates from the basis into the standard coordinate system and then into the basis .      We know that .  We solve the linear system to find .  If , we have This matrix , whose columns are the vectors and , has a pivot position in every row and every column because the vectors form a basis. It is, therefore, row equivalent to the identity matrix and hence invertible.  Since we have , we also have .  We have   If we define to be the matrix whose columns are and , then Therefore,     "
},
{
  "id": "proposition-coordinate-transform",
  "level": "2",
  "url": "sec-bases.html#proposition-coordinate-transform",
  "type": "Proposition",
  "number": "3.2.7",
  "title": "",
  "body": "  If is a basis and the matrix whose columns are the basis vectors, then    "
},
{
  "id": "example-22",
  "level": "2",
  "url": "sec-bases.html#example-22",
  "type": "Example",
  "number": "3.2.8",
  "title": "",
  "body": "  Let's consider the basis of : It is relatively straightforward to convert a vector's representation in this basis into to the standard basis using the matrix whose columns are the basis vectors: For example, suppose that the vector is described in the coordinate system defined by the basis as . We then have .  Consider now the vector . If we would like to express in the coordinate system defined by , then we compute .   "
},
{
  "id": "example-wavelet-basis",
  "level": "2",
  "url": "sec-bases.html#example-wavelet-basis",
  "type": "Example",
  "number": "3.2.9",
  "title": "",
  "body": "  Suppose we work for a company that records its quarterly revenue, in millions of dollars, as:   A company's quarterly revenue    Quarter  Revenue    1  10.3   2  13.1   3  7.5   4  8.2      Rather than using a table to record the data, we could display it in a graph or write it as a vector in : .    Let's consider a new basis for using vectors . We may view these basis elements graphically, as in       A representation of the basis elements of .   To convert our revenue vectors into the coordinates given by , we form the matrices: In particular, if the revenue vector is , then Notice that the first component of is the average of the components of .  For our particular revenue vector , we have This means that our revenue vector is . We will think about what these terms mean by adding them together one at a time.    The first term, gives us the average revenue over the year.     The average revenue for the first two quarters is 11.7, which is 1.925 million dollars above the yearly average. Similarly, the average revenue for the last two quarters is 1.925 million dollars below the yearly average. This is recorded by the second term      Finally, the first quarter's revenue is 1.400 million dollars below the average over the first two quarters and the second quarter's revenue is 1.400 million dollars above that average. This, and the corresponding data for the last two quarters, is captured by the last two terms:      If we write , we see that the coefficient measures the average revenue over the year, measures the deviation from the annual average in the first and second halves of the year, and measures how the revenue in the first and second quarter differs from the average in the first half of the year. In this way, the coefficients provide a view of the revenue over different time scales, from an annual summary to a finer view of quarterly behavior.  This basis is sometimes called a Haar wavelet basis, and the change of basis is known as a Haar wavelet transform. In the next section, we will see how this basis provides a useful way to store digital images.   "
},
{
  "id": "activity-33",
  "level": "2",
  "url": "sec-bases.html#activity-33",
  "type": "Activity",
  "number": "3.2.4",
  "title": "Edge detection.",
  "body": " Edge detection   An important problem in the field of computer vision is to detect edges in a digital photograph, as is shown in . Edge detection algorithms are useful when, say, we want a robot to locate an object in its field of view. Graphic designers also use these algorithms to create artistic effects.       A canyon wall in Capitol Reef National Park and the result of an edge detection algorithm.   We will consider a very simple version of an edge detection algorithm to give a sense of how this works. Rather than considering a two-dimensional photograph, we will think about a one-dimensional row of pixels in a photograph. The grayscale values of a pixel measure the brightness of a pixel; a grayscale value of 0 corresponds to black, and a value of 255 corresponds to white.  Suppose, for simplicity, that the grayscale values for a row of six pixels are represented by a vector in :    .    We can easily see that there is a jump in brightness between pixels 4 and 5, but how can we detect it computationally? We will introduce a new basis for with vectors: .  Construct the matrix that relates the standard coordinate system with the coordinates in the basis .  Determine the matrix that converts the representation of in standard coordinates into the coordinate system defined by .   Suppose the vectors are expressed in general terms as . Using the relationship , determine an expression for the coefficient in terms of . What does measure in terms of the grayscale values of the pixels? What does measure in terms of the grayscale values of the pixels?  Now for the specific vector , determine the representation of in the -coordinate system.  Explain how the coefficients in determine the location of the jump in brightness in the grayscale values represented by the vector .    Readers who are familiar with calculus may recognize that this change of basis converts a vector into , the set of changes in . This process is similar to differentiation in calculus. Similarly, the process of converting into the vector adds together the changes in a process similar to integration. As a result, this change of basis represents a linear algebraic version of the Fundamental Theorem of Calculus.     We form the matrix   We find that   We see that so measures the change in brightness between one pixel and its neighbor. Similarly, , which measures another change in brightness.  We compute that   Most of the coefficients that measure changes are relatively small in absolute value. The coefficient , however, which measures the change in brightness between the fourth and fifth pixel, has a large absolute value. This tells us that there is a large change in brightness between the fourth and fifth pixel, which points to an edge in the image.    "
},
{
  "id": "exercise-94",
  "level": "2",
  "url": "sec-bases.html#exercise-94",
  "type": "Exercise",
  "number": "3.2.5.1",
  "title": "",
  "body": " Shown in are two vectors and in the plane .      Vectors and in .    Explain why is a basis for .  Using , indicate the vectors such that            Using , find the representation if   .   .   .   Find if .     The vectors are linearly independent and span .  The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .     .     The vectors are linearly independent and span . We can see this by forming the matrix   The grid on the figure indicates that   .   .   .    The grid on the figure indicates that   .   .   .    We form the matrix so that . We then need to solve the equation , which gives .   "
},
{
  "id": "exercise-95",
  "level": "2",
  "url": "sec-bases.html#exercise-95",
  "type": "Exercise",
  "number": "3.2.5.2",
  "title": "",
  "body": " Consider vectors and let and .  Explain why and are both bases of .  If , find and .   If , find and .  If , find and .  Find a matrix such that .     The sets of the vectors are both linearly independent and span .   and .   and .   and .   .     In both cases, we see that so that both sets of vectors are linearly independent and span .  We solve and to find and .  We have . We then solve to find .  In the same way, we find and .  We have , which shows that .   "
},
{
  "id": "exercise-96",
  "level": "2",
  "url": "sec-bases.html#exercise-96",
  "type": "Exercise",
  "number": "3.2.5.3",
  "title": "",
  "body": " Consider the following vectors in : .  Explain why forms a basis for .  Explain how to convert , the representation of a vector in the coordinates defined by , into , its representation in the standard coordinate system.  Explain how to convert the vector into , its representation in the coordinate system defined by .  If , find .  If , find .     The vectors are linearly independent and span .  We have .  We have .   .   .     Form the matrix which shows that the vectors are linearly independent and span .  We have .  We have where   We find .  We find .   "
},
{
  "id": "exercise-97",
  "level": "2",
  "url": "sec-bases.html#exercise-97",
  "type": "Exercise",
  "number": "3.2.5.4",
  "title": "",
  "body": " Consider the following vectors in : .  Do these vectors form a basis for ? Explain your thinking.   Find a subset of these vectors that forms a basis of .  Suppose you have a set of vectors in such that . Find a subset of the vectors that forms a basis for .     No, because a basis for must contain exactly three vectors.   , , and .   , , , and .     Looking at the reduced row echelon form, we find This shows that the vectors are not linearly independent since there is not a pivot position in every column. Therefore, the set of vectors does not form a basis for . Of course, we also know this because a set of vectors for must contain exactly three vectors.  From the reduced row echelon form, we see that the set of vectors spans because there is a pivot position in every row. We also see that and . This means that , , and will span and therefore form a basis.  In the same way, we see that , , , and for a basis for .   "
},
{
  "id": "exercise-98",
  "level": "2",
  "url": "sec-bases.html#exercise-98",
  "type": "Exercise",
  "number": "3.2.5.5",
  "title": "",
  "body": " This exercise involves a simple Fourier transform, which will play an important role in the next section.  Suppose that we have the vectors .  Explain why is a basis for . Notice that you may enter into Sage as cos(pi\/6) .   If , find .  Find the matrices and . If and , explain why is the average of , , and .      The vectors are linearly independent and span .   .  Since we have , we have .     By forming the matrix and finding its reduced row echelon form, we see that the vectors are linearly independent and span . They therefore form a basis of .  We solve to find .  We have Since we have , we have .   "
},
{
  "id": "exercise-99",
  "level": "2",
  "url": "sec-bases.html#exercise-99",
  "type": "Exercise",
  "number": "3.2.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If the columns of a matrix form a basis for , then is invertible.  There must be 125 vectors in a basis for .  If is a basis of , then every vector in can be expressed as a linear combination of basis vectors.  The coordinates are the weights that form as a linear combination of basis vectors.  If the basis vectors form the columns of the matrix , then .      True  True  True  True  False     True. If the columns of form a basis, then has a pivot position in every row and every column. Therefore, the reduced row echelon form of is the identity matrix, which implies that is invertible.  True. The number of vectors in a basis of must be .  True. If is a basis, then the vectors in span , which means that every vector in can be written as a linear combination of the vectors in .  True. This is the definition of .  False. The relationship is .   "
},
{
  "id": "exercise-100",
  "level": "2",
  "url": "sec-bases.html#exercise-100",
  "type": "Exercise",
  "number": "3.2.5.7",
  "title": "",
  "body": " Provide a justification for your response to each of the following questions.  Suppose you have linearly independent vectors in . Can you guarantee that they form a basis of ?  If is an invertible matrix, do the columns necessarily form a basis of ?  Suppose we have an invertible matrix , and we perform a sequence of row operations on to form a matrix . Can you guarantee that the columns of form a basis for ?  Suppose you have a set of 10 vectors in and that every vector in can be written as a linear combination of these vectors. Can you guarantee that this set of vectors is a basis for ?     Yes  Yes  Yes  Yes     Yes. A matrix formed from linearly independent vectors in will have a pivot position in every column. Since the matrix has the same number of rows and columns, there must also be a pivot position in every row. This means that the vectors span and therefore form a basis.  Yes. An invertible matrix is row equivalent to the identity matrix, which means that the columns are linearly independent and span . This implies that the columns form a basis of .  Yes. The matrix is row equivalent to the identity matrix so must be as well. This means that the columns of form a basis for .  Yes. The span of the set of vectors is , which says that the associated matrix is square and has a pivot position in every row. Therefore, it must have a pivot position in every column, which means that the set of vectors forms a basis for .   "
},
{
  "id": "exercise-101",
  "level": "2",
  "url": "sec-bases.html#exercise-101",
  "type": "Exercise",
  "number": "3.2.5.8",
  "title": "",
  "body": " Crystallographers find it convenient to use coordinate systems that are adapted to the specific geometry of a crystal. As a two-dimensional example, consider a layer of graphite in which carbon atoms are arranged in regular hexagons to form the crystalline structure shown in .      A layer of carbon atoms in a graphite crystal.   The origin of the coordinate system is at the carbon atom labeled by 0 . It is convenient to choose the basis defined by the vectors and and the coordinate system it defines.   Locate the points for which   ,   ,   .    Find the coordinates for all the carbon atoms in the hexagon whose lower left vertex is labeled 0 .  What are the coordinates of the center of that hexagon, which is labeled C ?  How do the coordinates of the atoms in the hexagon whose lower left corner is labeled 1 compare to the coordinates in the hexagon whose lower left corner is labeled \"0\"?  Does the point whose coordinates are correspond to a carbon atom or the center of a hexagon?     The points are indicated in the figure.      .   .  The coordinates differ by   It is the center of a hexagon.      The points are indicated in .   The points requested in part a of this exercise.       Moving counterclockwise around the hexagon, the coordinates are    .  We obtain the coordinates for the hexagon with the vertex labeled 1 by adding the coordinate expression of the point 1 , which is to those of the original hexagon.  It is the center of a hexagon. Adding or subtracting to the coordinates translates one hexagon to another. This means that can be translated to , which is the center of a hexagon.   "
},
{
  "id": "exercise-102",
  "level": "2",
  "url": "sec-bases.html#exercise-102",
  "type": "Exercise",
  "number": "3.2.5.9",
  "title": "",
  "body": " Suppose that and .  Explain why is a basis for .  Find and .  Use what you found in the previous part of this problem to find and .   If , find .  Find a matrix such that .  You should find that the matrix is a very simple matrix, which means that this basis is well suited to study the effect of multiplication by . This observation is the central idea of the next chapter.    The vectors are linearly independent and span .   and .   and .   .   .     The vectors are linearly independent and span .  We compute that and .   and .  We know that , which means that . Therefore, .  If , then and . Therefore, , which says that .   "
},
{
  "id": "sec-jpeg",
  "level": "1",
  "url": "sec-jpeg.html",
  "type": "Section",
  "number": "3.3",
  "title": "Image compression",
  "body": " Image compression   Digital images, such as the photographs taken on your phone, are displayed as a rectangular array of pixels. For example, the photograph in is 1440 pixels wide and 1468 pixels high. If we were to zoom in on the photograph, we would be able to see individual pixels, such as those shown on the right.       An image stored as a array of pixels along with a close-up of a smaller array.   A lot of data is required to display this image. A quantity of digital data is frequently measured in bytes, where one byte is the amount of storage needed to record an integer between 0 and 255. As we will see shortly, each pixel requires three bytes to record that pixel's color. This means the amount of data required to display this image is bytes or about 6.3 megabytes.  Of course, we would like to store this image on a phone or computer and perhaps transmit it through our data plan to share it with others. If possible, we would like to find a way to represent this image using a smaller amount of data so that we don't run out of memory on our phone and quickly exhaust our data plan.  As we will see in this section, the JPEG compression algorithm provides a means for doing just that. This image, when stored in the JPEG format, requires only 467,359 bytes of data, which is about 7% of the 6.3 megabytes required to display the image. That is, when we display this image, we are reconstructing it from only 7% of the original data. This isn't too surprising since there is quite a bit of redundancy in the image; the left half of the image is almost uniformly blue. The JPEG algorithm detects this redundancy by representing the data using bases that are well-suited to the task.    Since we will be using various bases and the coordinate systems they define, let's review how to translate between coordinate systems.  Suppose that we have a basis for . Explain what we mean by the representation of a vector in the coordinate system defined by .  If we are given the representation , how can we recover the vector ?  If we are given the vector , how can we find ?  Suppose that is a basis for . If , find the vector .   If , find .      The components of the vector are the weights that express as a linear combination of the basis vectors; that is, if .  If we form the matrix , then .  As before, .  We find .  We find .       Color models  A color is represented digitally by a vector in . There are different ways in which we can represent colors, however, depending on whether a computer or a human will be processing the color. We will describe two of these representations, called color models , and demonstrate how they are used in the JPEG compression algorithm.  Digital displays typically create colors by blending together various amounts of red, green, and blue. We can therefore describe a color by putting its constituent amounts of red, green, and blue into a vector . The quantities , , and are each stored with one byte of information so they are integers between 0 and 255. This is called the color model.  color model   We define a basis where to define a new coordinate system with coordinates we denote , , and : . luminance  chrominance The coordinate is called luminance while and are called blue and red chrominance , respectively. In this coordinate system, luminance will vary from 0 to 255, while the chrominances vary between -127.5 and 127.5. This is known as the color model. (To be completely accurate, we should add 127.5 to the chrominance values so that they lie between 0 and 255, but we won't worry about that here.)  color model     This activity investigates these two color models, which we view as coordinate systems for describing colors.    First, we will explore the color model.   The color model.     What happens when , (pushed all the way to the left), and is allowed to vary?  What happens when , , and is allowed to vary?  How can you create black in this color model?  How can you create white?     Next, we will explore the color model.   The color model.     What happens when and (kept in the center) and is allowed to vary?  What happens when (pushed to the left), (kept in the center), and is allowed to increase between 0 and 127.5?  What happens when , , and is allowed to increase between 0 and 127.5?  How can you create black in this color model?  How can you create white?    Verify that is a basis for .    Find the matrix that converts from coordinates into coordinates. Then find the matrix that converts from coordinates back into coordinates.   Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.  Pure red is .  Pure blue is .  Pure white is .  Pure black is .    Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.   .   .   .    Write an expression for  The luminance as it depends on , , and .  The blue chrominance as it depends on , , and .  The red chrominance as it depends on , , and .   Explain how these quantities can be roughly interpreted by stating that  the luminance represents the brightness of the color.  the blue chrominance measures the amount of blue in the color.  the red chrominance measures the amount of red in the color.         Working with the color model, we find that  we produce red with varying degrees of brightness.  we produce blue with varying degrees of brightness.   .   .    Working with the color model, we find that  we produce gray with varying degrees of brightness.  we produce blue with varying degrees of brightness.  we produce red with varying degrees of brightness.   .   with .    If we row reduce the matrix whose columns are the vectors in , we obtain the identity matrix, which means that the vectors are linearly independent and span .  The matrices are and   To convert from to , we multiply by so that                To convert from to , we multiply by so that             We have The expression for is a weighted average of the , , and values. The expression for takes half the amount of and subtracts the amounts of red and green. Likewise, the expression for takes half the amount of and subtracts the amounts of green and blue.     These two color models provide us with two ways to represent colors, each of which is useful in a certain context. Digital displays, such as those in phones and computer monitors, create colors by combining various amounts of red, green, and blue. The model is therefore most relevant in digital applications.  By contrast, the color model was created based on research into human vision and aims to concentrate the most visually important data into a single coordinate, the luminance, to which our eyes are most sensitive. Of course, any basis of must have three vectors so we need two more coordinates, blue and red chrominance, if we want to represent all colors.  To see this explicitly, shown in is the original image and the image as rendered with only the luminance. That is, on the right, the color of each pixel is represented by only one byte, which is the luminance. This image essentially looks like a grayscale version of the original image with all its visual detail. In fact, before digital television became the standard, television signals were broadcast using the color model. When a signal was displayed on a black-and-white television, the luminance was displayed and the two chrominance values simply ignored.       The original image rendered with only the luminance values.   For comparison, shown in are the corresponding images created using only the blue chrominance and the red chrominance. Notice that the amount of visual detail is considerably less in these images.       The original image rendered, on the left, with only blue chrominance and, on the right, with only red chrominance.   The aim of the JPEG compression algorithm is to represent an image using the smallest amount of data possible. By converting from the color model to the color model, we are concentrating the most visually important data into the luminance values. This is helpful because we can safely ignore some of the data in the chrominance values since that data is not as visually important.    The JPEG compression algorithm  The key to representing the image using a smaller amount of data is to detect redundancies in the data. To begin, we first break the image, which is composed of pixels, into small blocks of pixels. For example, we will consider the block of pixels outlined in green in the original image, shown on the left of . The image on the right zooms in on the block.       An block of pixels outlined in green in the original image on the left. We see the same block on a smaller scale on the right.   Notice that this block, as seen in the original image, is very small. If we were to change some of the colors in this block slightly, our eyes would probably not notice.    Here we see a close-up of the block. The important point here is that the colors do not change too much over this block. In fact, we expect this to be true for most of the blocks. There will, of course, be some blocks that contain dramatic changes, such as where the sky and rock intersect, but they will be the exception.    The block under consideration.   Following our earlier work, we will change the representation of colors from the color model to the model. This separates the colors into luminance and chrominance values that we will consider separately. In , we see the luminance values of this block. Again, notice how these values do not vary significantly over the block.       The luminance values in this block.   Our strategy in the compression algorithm is to perform a change of basis to take advantage of the fact that the luminance values do not change significantly over the block. Rather than recording the luminance of each of the pixels, this change of basis will allow us to record the average luminance along with some information about how the individual colors vary from the average.  Let's look at the first column of luminance values, which is a vector in : . We will perform a change of basis and describe this vector by the average of the luminance values and information about variations from the average.   Discrete Fourier Transform The JPEG compression algorithm uses the Discrete Fourier Transform , which is defined using the basis whose basis vectors are   On first glance, this probably looks intimidating, but we can make sense of it by looking at these vectors graphically. Shown in are four of these basis vectors. Notice that is constantly 1, varies relatively slowly, varies a little more rapidly, and varies quite rapidly. The main thing to notice is that: the basis vectors vary at different rates with the first vectors varying relatively slowly and the later vectors varying more rapidly.      Four of the basis vectors , , , and .    These vectors form the basis for . Remember that is the vector of luminance values in the first column as seen on the right. We will write in the new coordinates . The coordinates are called the Fourier coefficients of the vector .      We will explore the influence that the Fourier coefficients have on the vector .   To begin, we'll look at the Fourier coefficient .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    By comparison, let's see how the Fourier coefficient influences .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    Let's now investigate how the Fourier coefficient influences the vector .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?   If the components of vary relatively slowly, what would you expect to be true of the Fourier coefficients ?  The Sage cell below will construct the vector , which is denoted P , and its inverse , which is denoted Pinv . Evaluate this Sage cell and notice that it prints the matrix . Now look at the form of and explain why is the average of the luminance values in the vector .  The Sage cell below defines the vector , which is the vector of luminance values in the first column, as seen in . Use the cell below to find the vector of Fourier coefficients . If you have evaluated the cell above, you will still be able to refer to P and Pinv in this cell. Write the Fourier coefficients and discuss the relative sizes of the coefficients.  Let's see what happens when we simply ignore the coefficients and . Form a new vector of Fourier coefficients by rounding the coefficients to the nearest integer and setting and to zero. This is an approximation to , the vector of Fourier coefficients. Use the approximation to to form an approximation of the vector . How much does your approximation differ from the actual vector ?  When we ignore the Fourier coefficients corresponding to rapidly varying basis elements, we see that the vector that we reconstruct is very close to the original one. In fact, the luminance values in the approximation differ by at most one or two from the actual luminance values. Our eyes are not sensitive enough to detect this difference.  So far, we have concentrated on only one column in our block of luminance values. Let's now consider all of the columns. The following Sage cell defines a matrix called luminance , which is the matrix of luminance values. Find the matrix whose columns are the Fourier coefficients of the columns of luminance values.   Notice that the first row of this matrix consists of the Fourier coefficient for each of the columns. Just as we saw before, the entries in this row do not change significantly as we move across the row. In the Sage cell below, write these entries in the vector and find the corresponding Fourier coefficients.        Changing has the effect of adding a constant to the components of .  The coefficient introduces a slow variation into the components of .  The coefficient introduces a rapid variation into the components of .  The coefficients with larger values of will be small.  We have From the top row of this matrix, we see that .  We find that Notice that the largest Fourier coefficient is and that the coefficients , , , and are relatively small. This reflects the fact that the luminance does not vary rapidly.  Notice that showing that the approximation is quite good.  We have   We see that the Fourier coefficients of these Fourier coefficients is . Once again, we see that is the largest Fourier coefficient and the coefficients corresponding to rapid variations are small.     Up to this point, we have been working with the luminance values in one block of our image. We formed the Fourier coefficients for each of the columns of this block. Once we notice that the Fourier coefficients across a row are relatively constant, it seems reasonable to find the Fourier coefficients of the rows of the matrix of Fourier coefficients. Doing so leads to the matrix .  If we were to look inside a JPEG image file, we would see lots of matrices like this. For each block, there would be three matrices of Fourier coefficients of the rows of Fourier coefficients, one matrix for each of the luminance, blue chrominance, and red chrominance values. However, we store these Fourier coefficients as integers inside the JPEG file so we need to round off the coefficients to the nearest integer, as shown here: .  There are many zeroes in this matrix, and we can save space in a JPEG image file by only recording the nonzero Fourier coefficients.  In fact, when a JPEG file is created, there is a quality parameter that can be set, such as that shown in . When the quality parameter is high, we will store many of the Fourier coefficients; when it is low, we will ignore more of them.      When creating a JPEG file, we choose a value of the quality parameter.   To see how this works, suppose the quality setting is relatively high. After rounding off the Fourier coefficients, we will set all of the coefficients whose absolute value is less than 2 to zero, which creates the matrix: Notice that there are 12 nonzero Fourier coefficients, out of 64, that we need to record. Consequently, we only save of the data.  If instead, the quality setting is relatively low, we set all of the Fourier coefficients whose absolute value is less than 4 to zero, creating the matrix: . Notice that there are only 5 nonzero Fourier coefficients that we need to record now, meaning we save only of the data. This will result in a smaller JPEG file describing the image.  With a lower quality setting, we have thrown away more information about the Fourier coefficients so the image will not be reconstructed as accurately. To see this, we can reconstruct the luminance values from the Fourier coefficients by converting back into the standard coordinate system. Rather than showing the luminance values themselves, we will show the difference in the original luminance values and the reconstructed luminance values. When the quality setting was high and we stored 12 Fourier coefficients, we find this difference to be . When the quality setting is lower and we store only 5 Fourier coefficients, the difference is .  This demonstrates the trade off. With a high quality setting, we require more storage to save more of the data, but the reconstructed image is closer to the original. With the lower quality setting, we require less storage, but the reconstructed image differs more from the original.  If we remember that the visual information stored by the blue and red chrominance values is not as important as that contained in the luminance values, we feel safer in discarding more of the Fourier coefficients for the chrominance values resulting in an even greater savings.  Shown in is the original image compared to a version stored with a very low quality setting. If you look carefully, you can individual blocks.       The original image and the result of storing the image with a low quality setting.   This discussion of the JPEG compression algorithm is meant to explore the ideas that underlie its construction and demonstrate the importance of a choice of basis and its accompanying coordinate system. There are a few details, most notably about the rounding of the Fourier coefficients, that are not strictly accurate. The actual implementation is a little more complicated, but the presentation here conveys the spirit of the algorithm.  The JPEG compression algorithm allows us to store image files using only a fraction of the data. Similar ideas are used to efficiently store digital music and video files.    Summary  This section has explored how appropriate changes in bases help us reconstruct an image using only a fraction of its data. This is known as image compression.  There are several ways of representing colors, all of which use vectors in . We explored the color model, which is appropriate in digital applications, and the model, in which the most important visual information is conveyed by the component, known as luminance.  We also explored a change of basis called the Discrete Fourier Transform. In the coordinate system that results, the first coefficient measures the average of the components of a vector. Other coefficients measure variations in the components away from the average.  We put both of these ideas to use in demonstrating the JPEG compression algorithm. An image is broken into blocks, and the colors into luminance, blue chrominance, and red chrominance. Applying the Discrete Fourier Transform allows us to reconstruct a good approximation of the image using only a fraction of the original data.       Consider the vector .  In the Sage cell below is a copy of the change of basis matrices that define the Fourier transform. Find the Fourier coefficients of .   We will now form the vector , which is an approximation of by rounding all the Fourier coefficients of to the nearest integer to obtain . Now find the vector and compare this approximation to . What is the error in this approximation?  Repeat the last part of this problem, but set the rounded Fourier coefficients to zero if they have an absolute value less than five. Use it to create a second approximation of . What is the error in this approximation?  Compare the number of nonzero Fourier coefficients that you have in the two approximations and compare the accuracy of the approximations. Using a few sentences, discuss the comparisons that you find.       .        In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.      .  We round the Fourier coefficients to the vector and find the approximation , which compares to the original vector . In this case, the approximation is the same as the original vector .  Now we found the Fourier coefficients to and find the approximation , which compares to the original vector . Now we see that the approximating vector differs from the original vector , though not significantly.  In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.     There are several steps to the JPEG compression algorithm. The following questions examine the motivation behind some of them.  What is the overall goal of the JPEG compression algorithm?  Why do we convert colors from the the color model to the model?  Why do we decompose the image into a collection of arrays of pixels?  What role does the Discrete Fourier Transform play in the JPEG compression algorithm?  Why is the information conveyed by the rapid-variation Fourier coefficients, generally speaking, less important than the slow-variation coefficients?     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.     The Fourier transform that we used in this section is often called the Discrete Fourier Cosine Transform because it is defined using a basis consisting of cosine functions. There is also a Fourier Sine Transform defined using a basis consisting of sine functions. For instance, in , the basis vectors of are We can think of these vectors graphically, as shown in .      The vectors that form the basis .    The Sage cell below defines the matrix S whose columns are the vectors in the basis as well as the matrix C whose columns form the basis used in the Fourier Cosine Transform.   In the block of luminance values we considered in this section, the first column begins with the four entries 176, 181, 165, and 139, as seen in . These form the vector . Find both and .  Write a sentence or two comparing the values for the Fourier Sine coefficients and the Fourier Cosine coefficients .  Suppose now that . Find the Fourier Sine coefficients and the Fourier Cosine coefficients .  Write a few sentences explaining why we use the Fourier Cosine Transform in the JPEG compression algorithm rather than the Fourier Sine Transform.      and .  The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.   and .  Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .     We find that   The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.  We find that   Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .     In , we looked at a basis for that we called the Haar wavelet basis. The basis vectors are , which may be understood graphically as in . We will denote this basis by .      The Haar wavelet basis represented graphically.   The change of coordinates from a vector in to is called the Haar wavelet transform and we write . The coefficients are called wavelet coefficients.  Let's work with the block of luminance values in the upper left corner of our larger block: .    The following Sage cell defines the matrix W whose columns are the basis vectors in . If is the first column of luminance values in the block above, find the wavelet coefficients .   Notice that gives the average value of the components of and describes how the averages of the first two and last two components differ from the overall average. The coefficients and describe small-scale variations between the first two components and last two components, respectively.  If we set the last wavelet coefficients and , we obtain the wavelet coefficients for a vector that approximates . Find the vector and compare it to the original vector .  What impact does the fact that and have on the form of the vector ? Explain how setting these coefficients to zero ignores the behavior of on a small scale.  In the JPEG compression algorithm, we looked at the Fourier coefficients of all the columns of luminance values and then performed a Fourier transform on the rows. The Sage cell below will perform the same operation using the wavelet transform; that is, it will first find the wavelet coefficients of each of the columns and then perform the wavelet transform on the rows. You only need to evaluate the cell to find the wavelet coefficients obtained in this way.   Now set all the wavelet coefficients equal to zero except those in the upper left block and use them to define the matrix coeffs in the Sage cell below. This has the effect of ignoring all of the small-scale differences. Evaluating this cell will recover the approximate luminance values.    Explain how the wavelet transform and this approximation can be used to create a lower resolution version of the image.   This kind of wavelet transform is the basis of the JPEG 2000 compression algorithm, which is an alternative to the usual JPEG algorithm.     .   .  The first two components are equal as are the last two components.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.     We find that .  We have the approximation and .  The first two components are equal as are the last two components. Since we see no difference in these components, we have lost the information that differentiates the components on a small scale.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.     In this section, we looked at the and color models. In this exercise, we will look at the color model where is the hue, is the saturation, and is the value of the color. All three quantities vary between 0 and 255.   The color model.     If you leave and at some fixed values, what happens when you change the value of ?  Increase the value to 255, which is on the far right. Describe what happens when you vary the saturation using a fixed hue and value .  Describe what happens when and are fixed and varies.  How can you create white in this color model?  How can you create black in this color model?  Find an approximate range of hues that correspond to blue.  Find an approximate range of hues that correspond to green.   The color model concentrates the most important visual information in the luminance coordinate, which roughly measures the brightness of the color. The other two coordinates describe the hue of the color. By contrast, the color model concentrates all the information about the hue in the coordinate.  This is useful in computer vision applications. For instance, if we want a robot to detect a blue ball in its field of vision, we can specify a range of hue values to search for. If the lighting changes in the room, the saturation and value may change, but the hue will not. This increases the likelihood that the robot will still detect the blue ball across a wide range of lighting conditions.    The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .     The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .     "
},
{
  "id": "fig-jpeg-orig",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-orig",
  "type": "Figure",
  "number": "3.3.1",
  "title": "",
  "body": "     An image stored as a array of pixels along with a close-up of a smaller array.  "
},
{
  "id": "exploration-11",
  "level": "2",
  "url": "sec-jpeg.html#exploration-11",
  "type": "Preview Activity",
  "number": "3.3.1",
  "title": "",
  "body": "  Since we will be using various bases and the coordinate systems they define, let's review how to translate between coordinate systems.  Suppose that we have a basis for . Explain what we mean by the representation of a vector in the coordinate system defined by .  If we are given the representation , how can we recover the vector ?  If we are given the vector , how can we find ?  Suppose that is a basis for . If , find the vector .   If , find .      The components of the vector are the weights that express as a linear combination of the basis vectors; that is, if .  If we form the matrix , then .  As before, .  We find .  We find .    "
},
{
  "id": "activity-34",
  "level": "2",
  "url": "sec-jpeg.html#activity-34",
  "type": "Activity",
  "number": "3.3.2",
  "title": "",
  "body": "  This activity investigates these two color models, which we view as coordinate systems for describing colors.    First, we will explore the color model.   The color model.     What happens when , (pushed all the way to the left), and is allowed to vary?  What happens when , , and is allowed to vary?  How can you create black in this color model?  How can you create white?     Next, we will explore the color model.   The color model.     What happens when and (kept in the center) and is allowed to vary?  What happens when (pushed to the left), (kept in the center), and is allowed to increase between 0 and 127.5?  What happens when , , and is allowed to increase between 0 and 127.5?  How can you create black in this color model?  How can you create white?    Verify that is a basis for .    Find the matrix that converts from coordinates into coordinates. Then find the matrix that converts from coordinates back into coordinates.   Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.  Pure red is .  Pure blue is .  Pure white is .  Pure black is .    Find the coordinates for the following colors and check, using the diagrams above, that the two representations agree.   .   .   .    Write an expression for  The luminance as it depends on , , and .  The blue chrominance as it depends on , , and .  The red chrominance as it depends on , , and .   Explain how these quantities can be roughly interpreted by stating that  the luminance represents the brightness of the color.  the blue chrominance measures the amount of blue in the color.  the red chrominance measures the amount of red in the color.         Working with the color model, we find that  we produce red with varying degrees of brightness.  we produce blue with varying degrees of brightness.   .   .    Working with the color model, we find that  we produce gray with varying degrees of brightness.  we produce blue with varying degrees of brightness.  we produce red with varying degrees of brightness.   .   with .    If we row reduce the matrix whose columns are the vectors in , we obtain the identity matrix, which means that the vectors are linearly independent and span .  The matrices are and   To convert from to , we multiply by so that                To convert from to , we multiply by so that             We have The expression for is a weighted average of the , , and values. The expression for takes half the amount of and subtracts the amounts of red and green. Likewise, the expression for takes half the amount of and subtracts the amounts of green and blue.    "
},
{
  "id": "fig-jpeg-luminance",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-luminance",
  "type": "Figure",
  "number": "3.3.4",
  "title": "",
  "body": "     The original image rendered with only the luminance values.  "
},
{
  "id": "fig-jpeg-chrominance",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-chrominance",
  "type": "Figure",
  "number": "3.3.5",
  "title": "",
  "body": "     The original image rendered, on the left, with only blue chrominance and, on the right, with only red chrominance.  "
},
{
  "id": "fig-jpeg-block",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-block",
  "type": "Figure",
  "number": "3.3.6",
  "title": "",
  "body": "     An block of pixels outlined in green in the original image on the left. We see the same block on a smaller scale on the right.  "
},
{
  "id": "fig-jpeg-block-zoom",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-block-zoom",
  "type": "Figure",
  "number": "3.3.7",
  "title": "",
  "body": "  Here we see a close-up of the block. The important point here is that the colors do not change too much over this block. In fact, we expect this to be true for most of the blocks. There will, of course, be some blocks that contain dramatic changes, such as where the sky and rock intersect, but they will be the exception.    The block under consideration.  "
},
{
  "id": "fig-jpeg-block-luminance",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-block-luminance",
  "type": "Figure",
  "number": "3.3.8",
  "title": "",
  "body": "     The luminance values in this block.  "
},
{
  "id": "fig-jpeg-fourier-basis",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-fourier-basis",
  "type": "Figure",
  "number": "3.3.9",
  "title": "",
  "body": "    Four of the basis vectors , , , and .  "
},
{
  "id": "activity-35",
  "level": "2",
  "url": "sec-jpeg.html#activity-35",
  "type": "Activity",
  "number": "3.3.3",
  "title": "",
  "body": "  We will explore the influence that the Fourier coefficients have on the vector .   To begin, we'll look at the Fourier coefficient .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    By comparison, let's see how the Fourier coefficient influences .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?    Let's now investigate how the Fourier coefficient influences the vector .   The effect of the Fourier coefficient on the vector .    Describe the effect that has on the vector . Would you describe the components in as constant, slowly varying, or rapidly varying?   If the components of vary relatively slowly, what would you expect to be true of the Fourier coefficients ?  The Sage cell below will construct the vector , which is denoted P , and its inverse , which is denoted Pinv . Evaluate this Sage cell and notice that it prints the matrix . Now look at the form of and explain why is the average of the luminance values in the vector .  The Sage cell below defines the vector , which is the vector of luminance values in the first column, as seen in . Use the cell below to find the vector of Fourier coefficients . If you have evaluated the cell above, you will still be able to refer to P and Pinv in this cell. Write the Fourier coefficients and discuss the relative sizes of the coefficients.  Let's see what happens when we simply ignore the coefficients and . Form a new vector of Fourier coefficients by rounding the coefficients to the nearest integer and setting and to zero. This is an approximation to , the vector of Fourier coefficients. Use the approximation to to form an approximation of the vector . How much does your approximation differ from the actual vector ?  When we ignore the Fourier coefficients corresponding to rapidly varying basis elements, we see that the vector that we reconstruct is very close to the original one. In fact, the luminance values in the approximation differ by at most one or two from the actual luminance values. Our eyes are not sensitive enough to detect this difference.  So far, we have concentrated on only one column in our block of luminance values. Let's now consider all of the columns. The following Sage cell defines a matrix called luminance , which is the matrix of luminance values. Find the matrix whose columns are the Fourier coefficients of the columns of luminance values.   Notice that the first row of this matrix consists of the Fourier coefficient for each of the columns. Just as we saw before, the entries in this row do not change significantly as we move across the row. In the Sage cell below, write these entries in the vector and find the corresponding Fourier coefficients.        Changing has the effect of adding a constant to the components of .  The coefficient introduces a slow variation into the components of .  The coefficient introduces a rapid variation into the components of .  The coefficients with larger values of will be small.  We have From the top row of this matrix, we see that .  We find that Notice that the largest Fourier coefficient is and that the coefficients , , , and are relatively small. This reflects the fact that the luminance does not vary rapidly.  Notice that showing that the approximation is quite good.  We have   We see that the Fourier coefficients of these Fourier coefficients is . Once again, we see that is the largest Fourier coefficient and the coefficients corresponding to rapid variations are small.    "
},
{
  "id": "fig-jpeg-quality",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-quality",
  "type": "Figure",
  "number": "3.3.13",
  "title": "",
  "body": "    When creating a JPEG file, we choose a value of the quality parameter.  "
},
{
  "id": "fig-jpeg-image-low",
  "level": "2",
  "url": "sec-jpeg.html#fig-jpeg-image-low",
  "type": "Figure",
  "number": "3.3.14",
  "title": "",
  "body": "     The original image and the result of storing the image with a low quality setting.  "
},
{
  "id": "exercise-103",
  "level": "2",
  "url": "sec-jpeg.html#exercise-103",
  "type": "Exercise",
  "number": "3.3.4.1",
  "title": "",
  "body": " Consider the vector .  In the Sage cell below is a copy of the change of basis matrices that define the Fourier transform. Find the Fourier coefficients of .   We will now form the vector , which is an approximation of by rounding all the Fourier coefficients of to the nearest integer to obtain . Now find the vector and compare this approximation to . What is the error in this approximation?  Repeat the last part of this problem, but set the rounded Fourier coefficients to zero if they have an absolute value less than five. Use it to create a second approximation of . What is the error in this approximation?  Compare the number of nonzero Fourier coefficients that you have in the two approximations and compare the accuracy of the approximations. Using a few sentences, discuss the comparisons that you find.       .        In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.      .  We round the Fourier coefficients to the vector and find the approximation , which compares to the original vector . In this case, the approximation is the same as the original vector .  Now we found the Fourier coefficients to and find the approximation , which compares to the original vector . Now we see that the approximating vector differs from the original vector , though not significantly.  In the first approximation, the reconstructed vector agrees with the original vector . In the second approximation, there are only two nonzero Fourier coefficients, and we see that the approximating vector differs from the original vector. In the second case, however, we are using only one quarter of the Fourier coefficients. Though the entries in the vector differ from those in the original vector , the difference is probably not visually noticeable since the possible values range from 0 to 255.   "
},
{
  "id": "exercise-104",
  "level": "2",
  "url": "sec-jpeg.html#exercise-104",
  "type": "Exercise",
  "number": "3.3.4.2",
  "title": "",
  "body": " There are several steps to the JPEG compression algorithm. The following questions examine the motivation behind some of them.  What is the overall goal of the JPEG compression algorithm?  Why do we convert colors from the the color model to the model?  Why do we decompose the image into a collection of arrays of pixels?  What role does the Discrete Fourier Transform play in the JPEG compression algorithm?  Why is the information conveyed by the rapid-variation Fourier coefficients, generally speaking, less important than the slow-variation coefficients?     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.     The goal is to represent the image in a format that requires less storage but still allows us to reconstruct the image with most of the visual information preserved.  In the color model, the most significant visual information is stored in the channel. Therefore, we should store most of this information. The information in the and channels is not as visually significant so we can store a smaller amount of that data without noticing the effect.  As seen in an image, these blocks are relatively small, which means that the approximations we reconstruct are probably not noticeably different from the original. Also, the blocks are small enough so that the image does not typically change a lot over the block. At the same time, the blocks are large enough to allow us to save storage by throwing away some of the Fourier coefficients.  The Discrete Fourier Transform represents the data in a block as its average and variations from the average. Since the blocks are relatively small, there will typically be small variations away from the average. Therefore, we can often ignore those variations without visually detecting the approximation.  Our eyes do not detect individual pixels so we do not detect differences from one pixel to the next. This means that we do not typically notice rapid variations.   "
},
{
  "id": "exercise-105",
  "level": "2",
  "url": "sec-jpeg.html#exercise-105",
  "type": "Exercise",
  "number": "3.3.4.3",
  "title": "",
  "body": " The Fourier transform that we used in this section is often called the Discrete Fourier Cosine Transform because it is defined using a basis consisting of cosine functions. There is also a Fourier Sine Transform defined using a basis consisting of sine functions. For instance, in , the basis vectors of are We can think of these vectors graphically, as shown in .      The vectors that form the basis .    The Sage cell below defines the matrix S whose columns are the vectors in the basis as well as the matrix C whose columns form the basis used in the Fourier Cosine Transform.   In the block of luminance values we considered in this section, the first column begins with the four entries 176, 181, 165, and 139, as seen in . These form the vector . Find both and .  Write a sentence or two comparing the values for the Fourier Sine coefficients and the Fourier Cosine coefficients .  Suppose now that . Find the Fourier Sine coefficients and the Fourier Cosine coefficients .  Write a few sentences explaining why we use the Fourier Cosine Transform in the JPEG compression algorithm rather than the Fourier Sine Transform.      and .  The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.   and .  Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .     We find that   The higher Fourier Cosine coefficients are smaller than their counterparts in the Fourier Sine transform. This means that we can ignore these coefficients in the Fourier Cosine transform without losing too much information.  We find that   Generally speaking, we can safely ignore more of the coefficients in the Fourier Cosine transform. Most of the important information is channeled into the first coefficient, which records the average of the components of the vector .   "
},
{
  "id": "exercise-106",
  "level": "2",
  "url": "sec-jpeg.html#exercise-106",
  "type": "Exercise",
  "number": "3.3.4.4",
  "title": "",
  "body": " In , we looked at a basis for that we called the Haar wavelet basis. The basis vectors are , which may be understood graphically as in . We will denote this basis by .      The Haar wavelet basis represented graphically.   The change of coordinates from a vector in to is called the Haar wavelet transform and we write . The coefficients are called wavelet coefficients.  Let's work with the block of luminance values in the upper left corner of our larger block: .    The following Sage cell defines the matrix W whose columns are the basis vectors in . If is the first column of luminance values in the block above, find the wavelet coefficients .   Notice that gives the average value of the components of and describes how the averages of the first two and last two components differ from the overall average. The coefficients and describe small-scale variations between the first two components and last two components, respectively.  If we set the last wavelet coefficients and , we obtain the wavelet coefficients for a vector that approximates . Find the vector and compare it to the original vector .  What impact does the fact that and have on the form of the vector ? Explain how setting these coefficients to zero ignores the behavior of on a small scale.  In the JPEG compression algorithm, we looked at the Fourier coefficients of all the columns of luminance values and then performed a Fourier transform on the rows. The Sage cell below will perform the same operation using the wavelet transform; that is, it will first find the wavelet coefficients of each of the columns and then perform the wavelet transform on the rows. You only need to evaluate the cell to find the wavelet coefficients obtained in this way.   Now set all the wavelet coefficients equal to zero except those in the upper left block and use them to define the matrix coeffs in the Sage cell below. This has the effect of ignoring all of the small-scale differences. Evaluating this cell will recover the approximate luminance values.    Explain how the wavelet transform and this approximation can be used to create a lower resolution version of the image.   This kind of wavelet transform is the basis of the JPEG 2000 compression algorithm, which is an alternative to the usual JPEG algorithm.     .   .  The first two components are equal as are the last two components.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.     We find that .  We have the approximation and .  The first two components are equal as are the last two components. Since we see no difference in these components, we have lost the information that differentiates the components on a small scale.  We find the wavelet coefficients   We find the approximate luminance values to be   The luminance values are constant on blocks. It is as if the image is represented with pixels that are twice as wide and twice as tall.   "
},
{
  "id": "exercise-107",
  "level": "2",
  "url": "sec-jpeg.html#exercise-107",
  "type": "Exercise",
  "number": "3.3.4.5",
  "title": "",
  "body": " In this section, we looked at the and color models. In this exercise, we will look at the color model where is the hue, is the saturation, and is the value of the color. All three quantities vary between 0 and 255.   The color model.     If you leave and at some fixed values, what happens when you change the value of ?  Increase the value to 255, which is on the far right. Describe what happens when you vary the saturation using a fixed hue and value .  Describe what happens when and are fixed and varies.  How can you create white in this color model?  How can you create black in this color model?  Find an approximate range of hues that correspond to blue.  Find an approximate range of hues that correspond to green.   The color model concentrates the most important visual information in the luminance coordinate, which roughly measures the brightness of the color. The other two coordinates describe the hue of the color. By contrast, the color model concentrates all the information about the hue in the coordinate.  This is useful in computer vision applications. For instance, if we want a robot to detect a blue ball in its field of vision, we can specify a range of hue values to search for. If the lighting changes in the room, the saturation and value may change, but the hue will not. This increases the likelihood that the robot will still detect the blue ball across a wide range of lighting conditions.    The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .     The color changes but not the brightness.  The color appears more washed out with a low saturation and appears more vibrant with a high saturation.  The brightness changes.  Set the saturation and the value .  Set .   .   .   "
},
{
  "id": "sec-determinants",
  "level": "1",
  "url": "sec-determinants.html",
  "type": "Section",
  "number": "3.4",
  "title": "Determinants",
  "body": " Determinants   As invertibility plays a central role in this chapter, we need a criterion that tells us when a matrix is invertible. We already know that a square matrix is invertible if and only if it is row equivalent to the identity matrix. In this section, we will develop a second, numerical criterion that tells us when a square matrix is invertible.  To begin, let's consider a matrix whose columns are vectors and . We have frequently drawn the vectors and studied the linear combinations they form using a figure such as .      Linear combinations of two vectors and form a collection of congruent parallelograms.   Notice how the linear combinations form a set of congruent parallelograms in the plane. In this section, we will use the area of these parallelograms to define a numerical quantity called the determinant that tells us whether the matrix is invertible.   To recall, the area of parallelogram is found by multiplying the length of one side by the perpendicular distance to its parallel side. Using the notation in the figure, the area of the parallelogram is .      We will explore the area formula in this preview activity.  Find the area of the following parallelograms.    1.   2.   3.     4.   5.            Explain why the area of the parallelogram formed by the vectors and is the same as that formed by and .         We find the following areas.  A square has area 1.  A rectangle has area 6.  The square has side length giving an area of 2.  If we consider the horizontal length as the base, we see that so that the area is 4.  In the same way, we can consider both the base and height to be 2 so that the area is 4.    If we consider the base to be the length of , then the height, which is the perpendicular distance to its parallel side, is the same in both parallelograms.       Determinants of matrices  We will begin by defining the determinant of a matrix . First, however, we need to define the orientation of an ordered pair of vectors. As shown in , an ordered pair of vectors and is called positively oriented if the angle, measured in the counterclockwise direction, from to is less than ; we say the pair is negatively oriented if it is more than .      The vectors on the left are positively oriented while the ones on the right are negatively oriented.     determinant  Suppose a matrix has columns and . If the pair of vectors is positively oriented, then the determinant of , denoted , is the area of the parallelogram formed by and . If the pair is negatively oriented, then is minus the area of the parallelogram.      Consider the determinant of the identity matrix . As seen on the left of , the vectors and form a positively oriented pair. Since the parallelogram they form is a square, we have       The determinant , as seen on the left. On the right, we see that where is the matrix whose columns are shown.   Now consider the matrix . As seen on the right of , the vectors and form a negatively oriented pair. The parallelogram they define is a rectangle so we have .      In this activity, we will find the determinant of some simple matrices and discover some important properties of determinants.   The geometric meaning of the determinant of a matrix.      Use the diagram to find the determinant of the matrix . Along with , what does this lead you to believe is generally true about the determinant of a diagonal matrix?  Use the diagram to find the determinant of the matrix . What is the geometric effect of the matrix transformation defined by this matrix?  Use the diagram to find the determinant of the matrix . More generally, what do you notice about the determinant of any matrix of the form ? What does this say about the determinant of an upper triangular matrix?  Use the diagram to find the determinant of any matrix of the form . What does this say about the determinant of a lower triangular matrix?  Use the diagram to find the determinant of the matrix . In general, what is the determinant of a matrix whose columns are linearly dependent?  Consider the matrices . Use the diagram to find the determinants of , , and . What does this suggest is generally true about the relationship of to and ?      The determinant is because the vectors are negatively oriented and the rectangle has sides of length and . The determinant of a diagonal matrix seems to be the product of the diagonal entries.  The matrix transformation is a reflection over the line and we see that the determinant is .  The determinant will continue to be for any value of . This illustrates the fact that the determinant of an upper triangular matrix equals the product of its diagonal entries.  The same reasoning tells us that this determinant is and, in fact, the determinant of a lower triangular matrix equals the product of its diagonal entries.  The determinant of this matrix is because the parallelogram formed by the vector has no area. This suggests that the determinant of a matrix whose columns are linearly dependent is .  We find that , , and . This suggests that .     Later in this section, we will learn an algebraic technique for computing determinants. In the meantime, we will simply note that we can define determinants for matrices by measuring the volume of a box defined by the columns of the matrix, even if this box resides in for some very large .   For example, the columns of a matrix will form a parallelpiped, like the one shown here, and there is a means by which we can classify sets of such vectors as either positively or negatively oriented. Therefore, we can define the determinant in terms of the volume of the parallelpiped, but we will not worry about the details here.    Though the previous activity deals with determinants of matrices, it illustrates some important properties of determinants that are true more generally.  If is a triangular matrix, then equals the product of the entries on the diagonal. For example, , since the two parallelograms in have equal area.       The determinant of a triangular matrix equals the product of its diagonal entries.    We also saw that because the columns form a negatively oriented pair. You may remember from that a matrix such as this is obtained by interchanging two rows of the identity matrix.  The determinant satisfies a multiplicative property, which says that Rather than simply thinking of the determinant as the area of a parallelogram, we may also think of it as a factor by which areas are scaled under the matrix transformation defined by the matrix. Applying the matrix transformation defined by will scale area by . If we then compose with the matrix transformation defined by , area will scale a second time by the factor . The net effect is that the matrix transformation defined by scales area by so that .      The determinant satisfies these properties:   The determinant of a triangular matrix equals the product of its diagonal entries.    If is obtained by interchanging two rows of the identity matrix, then .     .         Determinants and invertibility  Perhaps the most important property of determinants also appeared in the previous activity. We saw that when the columns of the matrix are linearly dependent, the parallelogram formed by those vectors folds down onto a line. For instance, if , then the resulting parallelogram, as shown in , has zero area, which means that .   When the columns of are linearly dependent, we find that .      The condition that the columns of are linearly dependent is precisely the same as the condition that is not invertible. This leads us to believe that is not invertible if and only if its determinant is zero. The following proposition expresses this thought.    The matrix is invertible if and only if .    To understand this proposition more fully, let's remember that the matrix is invertible if and only if it is row equivalent to the identity matrix . We will therefore consider how the determinant changes when we perform row operations on a matrix. Along the way, we will discover an effective means to compute the determinant.  In , we saw how to describe the three row operations, scaling, interchange, and replacement, using matrix multiplication. If we perform a row operation on the matrix to obtain the matrix , we would like to relate and . To do so, remember that   Scalings are performed by multiplying a matrix by a diagonal matrix, such as which has the effect of multiplying the second row of by to obtain . Since is diagonal, we know that its determinant is the product of its diagonal entries so that . This means that and therefore In general, if we scale a row of by , we have .  Interchanges are performed by matrices such as which has the effect of interchanging the first and second rows of . As we saw in , . Therefore, when , we have In other words, when we perform an interchange.  Row replacement operations are performed by matrices such as which multiplies the first row by and adds the result to the third row. Since this is a lower triangular matrix, we know that the determinant is the product of the diagonal entries, which says that . This means that when , we have . In other words, a row replacement does not change the determinant.     The effect of row operations on the determinant      If is obtained from by scaling a row by , then .    If is obtained from by interchanging two rows, then .    If is obtained from by performing a row replacement operation, then .         We will investigate the connection between the determinant of a matrix and its invertibility using Gaussian elimination.  Consider the two upper triangular matrices Remembering , which of the matrices and are invertible? What are the determinants and ?  Explain why an upper triangular matrix is invertible if and only if its determinant is not zero.  Let's now consider the matrix and begin the Gaussian elimination process with a row replacement operation . What is the relationship between and ?  Next we perform another row replacement operation: . What is the relationship between and ?  Finally, we perform an interchange: to arrive at an upper triangular matrix . What is the relationship between and ?  Since is upper triangular, we can compute its determinant, which allows us to find . What is ? Is invertible?  Now consider the matrix Perform a sequence of row operations to find an upper triangular matrix that is row equivalent to . Use this to determine and whether is invertible.  Suppose we apply a sequence of row operations on a matrix to obtain . Explain why if and only if .  Explain why an matrix is invertible if and only if .      The matrix is invertible because we see there is a pivot position in every row and column. The matrix , however, is not invertible because there is not a pivot position in the third row. Also, and .  The determinant of an upper triangular matrix equals the product of its diagonal entries. Consequently, if the determinant of an upper triangular matrix is not zero, then each of its diagonal entries must be nonzero. In this case, there is a pivot position in every row and every column so that the matrix is invertible.  Row replacement operations do not change the determinant so .  In the same way, .  Interchanges change the sign of the determinant so .  The determinant since it is the product of the diagonal entries of . This means that . We see that is invertible because , which has a pivot position in every row and every column, is invertible.  Beginning with a row replacement operation, we arrive at . We next scale the second row by to obtain . Another row replacement operation gives . Putting these operations together, we see that . In this case, is not invertible because , which has a row without a pivot position, is not invertible.  Performing one of the three row operations either leaves the determinant unchanged (row replacement), changes its sign (interchange), or multiplies it by a nonzero number (scaling). Therefore, if we begin with a matrix whose determinant is not zero, the determinant remains nonzero after any row operation is applied.  If we apply a sequence of row operations to to find a row equivalent matrix that is upper triangular, we know that if and only if . We also know that is invertible if and only if is invertible. Putting these facts together, we conclude that if and only if is invertible.     As seen in this activity, row operations can be used to compute the determinant of a matrix. More specifically, applying the forward substitution phase of Gaussian elimination to the matrix leads us to an upper triangular matrix so that .  We know that is invertible when all of its diagonal entries are nonzero. We also know that under the same condition. This tells us is invertible if and only if .  Now if , we also have since applying a sequence of row operations to only multiplies the determinant by a nonzero number. It then follows that is invertible so . Therefore, we also know that and so must also be invertible.  This explains and so we know that is invertible if and only if .  Finally, notice that if is invertible, we have , which tells us that Therefore, .    If is an invertible matrix, then .      Cofactor expansions  We now have a technique for computing the determinant of a matrix using row operations. There is another way to compute determinants, using what are called cofactor expansions , that will be important for us in the next chapter. We will describe this method here.  To begin, the determinant of a matrix is . With a little bit of work, it can be shown that this number is the same as the signed area of the parallelogram we introduced earlier.  Using a cofactor expansion to find the determinant of a more general matrix is a little more work so we will demonstrate it with an example.    We illustrate how to use a cofactor expansion to find the determinant of where   To begin, we choose one row or column. It doesn't matter which we choose because the result will be the same in any case. Here, we choose the second row .  The determinant will be found by creating a sum of terms, one for each entry in the row we have chosen. For each entry in the row, we form its term by multiplying   where and are the row and column numbers, respectively, of the entry,  the entry itself, and  the determinant of the entries left over when we have crossed out the row and column containing the entry.     Since we are computing the determinant of this matrix using the second row, the entry in the first column of this row is . Let's see how to form the term from this entry.  The term itself is , and the matrix that is left over when we cross out the second row and first column is whose determinant is . Since this entry is in the second row and first column, the term we construct is .  Putting this together, we find the determinant to be . Notice that this agrees with the determinant that we found for this matrix using row operations in the last activity.      We will explore cofactor expansions through some examples.  Using a cofactor expansion, show that the determinant of the following matrix . Remember that you can choose any row or column to create the expansion, but the choice of a particular row or column may simplify the computation.  Use a cofactor expansion to find the determinant of . Explain how the cofactor expansion technique shows that the determinant of a triangular matrix is equal to the product of its diagonal entries.  Use a cofactor expansion to determine whether the following vectors form a basis of : .  Sage will compute the determinant of a matrix A with the command A.det() . Use Sage to find the determinant of the matrix .       We will using a cofactor expanion along the first row so that   Expanding along the first row gives   We form the matrix whose columns are the three given vectors. Expanding along either the second row or third column to take advantage of the zero in the entry, we see that , which means that is not invertible. Therefore, the vectors do not form a basis for .  Sage tells us that .       Summary  In this section, we associated a numerical quantity, the determinant, to a square matrix and showed how it tells us whether the matrix is invertible.  The determinant of a matrix has a geometric interpretation. In particular, when , the determinant is the signed area of the parallelogram formed by the two columns of the matrix.  The determinant satisfies many properties. For instance, and the determinant of a triangular matrix is equal to the product of its diagonal entries.  These properties helped us compute the determinant of a matrix using row operations. This also led to the important observation that the determinant of a matrix is nonzero if and only if the matrix is invertible.  Finally, we learned how to compute the determinant of a matrix using cofactor expansions, which will be a valuable tool for us in the next chapter.   We have seen three ways to compute the determinant: by interpreting the determinant as a signed area or volume; by applying appropriate row operations; and by using a cofactor expansion. It's worth spending a moment to think about the relative merits of these approaches.  The geometric definition of the determinant tells us that the determinant is measuring a natural geometric quantity, an insight that does not easily come through the other two approaches. The intuition we gain by thinking about the determinant geometrically makes it seem reasonable that the determinant should be zero for matrices that are not invertible: if the columns are linearly dependent, the vectors cannot create a positive volume.  Approaching the determinant through row operations provides an effective means of computing the determinant. In fact, this is what most computer programs do behind the scenes when they compute a determinant. This approach is also a useful theoretical tool for explaining why the determinant tells us whether a matrix is invertible.  The cofactor expansion method will be useful to us in the next chapter when we look at eigenvalues and eigenvectors. It is not, however, a practical way to compute a determinant. To see why, consider the fact that the determinant of a matrix, written as , requires us to compute two terms, and . To compute the determinant of a matrix, we need to compute three determinants, which involves terms. For a matrix, we need to compute four determinants, which produces terms. Continuing in this way, we see that the cofactor expansion of a matrix would involve terms.  By contrast, we have seen that the number of steps required to perform Gaussian elimination on an matrix is proportional to . When , we have , which points to the fact that finding the determinant using Gaussian elimination is considerably less work.     Consider the matrices .  Find the determinants of and using row operations.   Now find the determinants of and using cofactor expansions to verify your results     and .   We find that and .    This exercise concerns rotations and reflections in .  Suppose that is the matrix that performs a counterclockwise rotation in . Draw a typical picture of the vectors that form the columns of and use the geometric definition of the determinant to determine .  Suppose that is the matrix that performs a reflection in a line passing through the origin. Draw a typical picture of the columns of and use the geometric definition of the determinant to determine .  As we saw in , the matrices have the form . Compute the determinants of and and verify that they agree with what you found in the earlier parts of this exercise.      .   .     The vectors and that form the columns of are found by rotating the standard basis vectors and . Consequently, they are positively oriented and form a square. This says that .  The vectors and that form the columns of are found by reflecting the standard basis vectors and . Consequently, they are negatively oriented and form a square. This says that .  We see that and .     In the next chapter, we will say that matrices and are similar if there is a matrix such that .  Suppose that and are matrices and that there is a matrix such that . Explain why .  Suppose that is a matrix and that there is a matrix such that . Find .       because .   .      .   since .     Consider the matrix where is a parameter.  Find an expression for in terms of the parameter .  Use your expression for to determine the values of for which the vectors are linearly independent.          .     We see that .  The vectors are linearly independent when the matrix is invertible, which means that . Therefore, the vectors are linearly independent when .     Determine whether the following statements are true or false and explain your response.  If we have a square matrix and multiply the first row by and add it to the third row to obtain , then .  If we interchange two rows of a matrix, then the determinant is unchanged.  If we scale a row of the matrix by to obtain , then .  If and are row equivalent and , then also.  If is row equivalent to the identity matrix, then .      False  False  True  True  False     False. This is a row replacement operation, which leaves the determinant unchanged.  False. Applying an interchange operation changes the sign of the determinant.  True. Scaling a row of by multiplies the determinant by .  True. Row operations either leave the determinant unchanged, change its sign, or multiply it by a nonzero number. Therefore, if and and are related through a sequence of row operations, then .  False. It is true that , but a sequence of row operations that cause and to be row equivalent may multiply the determinant by a nonzero number or change its sign. We do know, however, that and so is invertible.     Suppose that and are matrices such that and . Find the following determinants:   .   .   .   .   .      .   .   .   .   .     Multiplying the entire matrix by scales each row by . Therefore, .  We have .  We know that .  Multiplying the matrix by scales each row by so we have .   .     Suppose that and are matrices.  If and are both invertible, use determinants to explain why is invertible.  If is invertible, use determinants to explain why both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.     Provide a justification for your responses to the following questions.  If every entry in one row of a matrix is zero, what can you say about the determinant?  If two rows of a square matrix are identical, what can you say about the determinant?  If two columns of a square matrix are identical, what can you say about the determinant?  If one column of a matrix is a linear combination of the others, what can you say about the determinant?     In all four cases, the determinant must be zero.    The determinant must be zero. This is because the matrix cannot be invertible since there is a row without a pivot. Also, applying a cofactor expansion along that row will produce zero for the determinant.  The determinant is zero again. If we multiply one of the rows by and add it to the other row, we obtain a row whose entries are all zero. This operation does not change the determinant, but we know that the determinant of the new matrix is zero by the previous part of this problem.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.     Consider the matrix .  Assuming that , rewrite the equation in terms of , , and .  Explain why and , the first two columns of , satisfy the equation you found in the previous part.  Explain why the solution space of this equation is the plane spanned by and .      .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent.  If for a vector , then is a linear combination of and .     Using a cofactor expansion, we have .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent. Therefore, the determinant of the matrix must be zero.  If for some vector , then the columns of must be linear dependent. Therefore, is a linear combination of and and lies in the plane that is their span.     In this section, we studied the effect of row operations on the matrix . In this exercise, we will study the effect of analogous column operations.  Suppose that is the matrix . Also consider elementary matrices .  Explain why the matrix is obtained from by replacing the first column by . We call this a column replacement operation. Explain why column replacement operations do not change the determinant.  Explain why the matrix is obtained from by multiplying the second column by . Explain the effect that scaling a column has on the determinant of a matrix.  Explain why the matrix is obtained from by interchanging the first and third columns. What is the effect of this operation on the determinant?  Use column operations to compute the determinant of .      Column replacements do not change the determinant because .  Scaling a column by multiplies the determinant by because .  Interchanges change the sign of the determinant because .   .     The columns of are found by multiplying the columns of by . If we remember that a vector multiplied by a matrix forms a linear combination of the columns of , we see that the first column of is and that the other two columns are unchanged.  Since , we have , which says that this operation does not change the determinant.  Multiplying by multiplies the determinant by since .  Multiplying by changes the sign of the determinant since .  If we interchange the first and third columns of , we have and . Now we will multiply the first column by and add to the second column to obtain and . Let's interchange the second and third columns so that so that and then multiply the second column by and add to the third column so that This gives .     Consider the matrices . Use row operations to find the determinants of these matrices.    , , and .   It takes three interchanges to see that , which implies that .  Two interchanges show us that and hence .  Two interchanges produce a diagonal matrix so that .    Consider the matrices   Use row (and\/or column) operations to find the determinants of these matrices.  Write the and matrices that follow in this pattern and state their determinants based on what you have seen.      , , , and .  If and are the and matrices, respectively, we expect that and .      , , , and .  If and are the and matrices, respectively, we expect that and .     The following matrix is called a Vandermond matrix: .  Use row operations to explain why .  Explain why is invertible if and only if , , and are all distinct real numbers.  There is a natural way to generalize this to a matrix with parameters , , , and . Write this matrix and state its determinant based on your previous work.    This matrix appeared in when we were found a polynomial that passed through a given set of points.    Perform a sequence of row operations to form an upper triangular matrix.  If , , and are distinct, then .  The determinant is .     If , , and are not distinct, we see that because two of the rows are identical. Therefore, we apply the following row operations, assuming , , and are distinct: Because of the scalings by and , we have .  If , , and are distinct, then , which implies that is invertible.  In the case, we have      "
},
{
  "id": "fig-intro-dets",
  "level": "2",
  "url": "sec-determinants.html#fig-intro-dets",
  "type": "Figure",
  "number": "3.4.1",
  "title": "",
  "body": "    Linear combinations of two vectors and form a collection of congruent parallelograms.  "
},
{
  "id": "exploration-12",
  "level": "2",
  "url": "sec-determinants.html#exploration-12",
  "type": "Preview Activity",
  "number": "3.4.1",
  "title": "",
  "body": "  We will explore the area formula in this preview activity.  Find the area of the following parallelograms.    1.   2.   3.     4.   5.            Explain why the area of the parallelogram formed by the vectors and is the same as that formed by and .         We find the following areas.  A square has area 1.  A rectangle has area 6.  The square has side length giving an area of 2.  If we consider the horizontal length as the base, we see that so that the area is 4.  In the same way, we can consider both the base and height to be 2 so that the area is 4.    If we consider the base to be the length of , then the height, which is the perpendicular distance to its parallel side, is the same in both parallelograms.    "
},
{
  "id": "fig-det-orientation",
  "level": "2",
  "url": "sec-determinants.html#fig-det-orientation",
  "type": "Figure",
  "number": "3.4.2",
  "title": "",
  "body": "    The vectors on the left are positively oriented while the ones on the right are negatively oriented.  "
},
{
  "id": "definition-15",
  "level": "2",
  "url": "sec-determinants.html#definition-15",
  "type": "Definition",
  "number": "3.4.3",
  "title": "",
  "body": "  determinant  Suppose a matrix has columns and . If the pair of vectors is positively oriented, then the determinant of , denoted , is the area of the parallelogram formed by and . If the pair is negatively oriented, then is minus the area of the parallelogram.   "
},
{
  "id": "example-det-identity",
  "level": "2",
  "url": "sec-determinants.html#example-det-identity",
  "type": "Example",
  "number": "3.4.4",
  "title": "",
  "body": "  Consider the determinant of the identity matrix . As seen on the left of , the vectors and form a positively oriented pair. Since the parallelogram they form is a square, we have       The determinant , as seen on the left. On the right, we see that where is the matrix whose columns are shown.   Now consider the matrix . As seen on the right of , the vectors and form a negatively oriented pair. The parallelogram they define is a rectangle so we have .   "
},
{
  "id": "activity-36",
  "level": "2",
  "url": "sec-determinants.html#activity-36",
  "type": "Activity",
  "number": "3.4.2",
  "title": "",
  "body": "  In this activity, we will find the determinant of some simple matrices and discover some important properties of determinants.   The geometric meaning of the determinant of a matrix.      Use the diagram to find the determinant of the matrix . Along with , what does this lead you to believe is generally true about the determinant of a diagonal matrix?  Use the diagram to find the determinant of the matrix . What is the geometric effect of the matrix transformation defined by this matrix?  Use the diagram to find the determinant of the matrix . More generally, what do you notice about the determinant of any matrix of the form ? What does this say about the determinant of an upper triangular matrix?  Use the diagram to find the determinant of any matrix of the form . What does this say about the determinant of a lower triangular matrix?  Use the diagram to find the determinant of the matrix . In general, what is the determinant of a matrix whose columns are linearly dependent?  Consider the matrices . Use the diagram to find the determinants of , , and . What does this suggest is generally true about the relationship of to and ?      The determinant is because the vectors are negatively oriented and the rectangle has sides of length and . The determinant of a diagonal matrix seems to be the product of the diagonal entries.  The matrix transformation is a reflection over the line and we see that the determinant is .  The determinant will continue to be for any value of . This illustrates the fact that the determinant of an upper triangular matrix equals the product of its diagonal entries.  The same reasoning tells us that this determinant is and, in fact, the determinant of a lower triangular matrix equals the product of its diagonal entries.  The determinant of this matrix is because the parallelogram formed by the vector has no area. This suggests that the determinant of a matrix whose columns are linearly dependent is .  We find that , , and . This suggests that .    "
},
{
  "id": "fig-parallelogram-f",
  "level": "2",
  "url": "sec-determinants.html#fig-parallelogram-f",
  "type": "Figure",
  "number": "3.4.7",
  "title": "",
  "body": "     The determinant of a triangular matrix equals the product of its diagonal entries.  "
},
{
  "id": "proposition-det-properties",
  "level": "2",
  "url": "sec-determinants.html#proposition-det-properties",
  "type": "Proposition",
  "number": "3.4.8",
  "title": "",
  "body": "  The determinant satisfies these properties:   The determinant of a triangular matrix equals the product of its diagonal entries.    If is obtained by interchanging two rows of the identity matrix, then .     .      "
},
{
  "id": "figure-linear-dep-det",
  "level": "2",
  "url": "sec-determinants.html#figure-linear-dep-det",
  "type": "Figure",
  "number": "3.4.9",
  "title": "",
  "body": " When the columns of are linearly dependent, we find that .     "
},
{
  "id": "prop-invertible-det",
  "level": "2",
  "url": "sec-determinants.html#prop-invertible-det",
  "type": "Proposition",
  "number": "3.4.10",
  "title": "",
  "body": "  The matrix is invertible if and only if .   "
},
{
  "id": "proposition-det-row-operations",
  "level": "2",
  "url": "sec-determinants.html#proposition-det-row-operations",
  "type": "Proposition",
  "number": "3.4.11",
  "title": "The effect of row operations on the determinant.",
  "body": " The effect of row operations on the determinant      If is obtained from by scaling a row by , then .    If is obtained from by interchanging two rows, then .    If is obtained from by performing a row replacement operation, then .      "
},
{
  "id": "activity-37",
  "level": "2",
  "url": "sec-determinants.html#activity-37",
  "type": "Activity",
  "number": "3.4.3",
  "title": "",
  "body": "  We will investigate the connection between the determinant of a matrix and its invertibility using Gaussian elimination.  Consider the two upper triangular matrices Remembering , which of the matrices and are invertible? What are the determinants and ?  Explain why an upper triangular matrix is invertible if and only if its determinant is not zero.  Let's now consider the matrix and begin the Gaussian elimination process with a row replacement operation . What is the relationship between and ?  Next we perform another row replacement operation: . What is the relationship between and ?  Finally, we perform an interchange: to arrive at an upper triangular matrix . What is the relationship between and ?  Since is upper triangular, we can compute its determinant, which allows us to find . What is ? Is invertible?  Now consider the matrix Perform a sequence of row operations to find an upper triangular matrix that is row equivalent to . Use this to determine and whether is invertible.  Suppose we apply a sequence of row operations on a matrix to obtain . Explain why if and only if .  Explain why an matrix is invertible if and only if .      The matrix is invertible because we see there is a pivot position in every row and column. The matrix , however, is not invertible because there is not a pivot position in the third row. Also, and .  The determinant of an upper triangular matrix equals the product of its diagonal entries. Consequently, if the determinant of an upper triangular matrix is not zero, then each of its diagonal entries must be nonzero. In this case, there is a pivot position in every row and every column so that the matrix is invertible.  Row replacement operations do not change the determinant so .  In the same way, .  Interchanges change the sign of the determinant so .  The determinant since it is the product of the diagonal entries of . This means that . We see that is invertible because , which has a pivot position in every row and every column, is invertible.  Beginning with a row replacement operation, we arrive at . We next scale the second row by to obtain . Another row replacement operation gives . Putting these operations together, we see that . In this case, is not invertible because , which has a row without a pivot position, is not invertible.  Performing one of the three row operations either leaves the determinant unchanged (row replacement), changes its sign (interchange), or multiplies it by a nonzero number (scaling). Therefore, if we begin with a matrix whose determinant is not zero, the determinant remains nonzero after any row operation is applied.  If we apply a sequence of row operations to to find a row equivalent matrix that is upper triangular, we know that if and only if . We also know that is invertible if and only if is invertible. Putting these facts together, we conclude that if and only if is invertible.    "
},
{
  "id": "proposition-25",
  "level": "2",
  "url": "sec-determinants.html#proposition-25",
  "type": "Proposition",
  "number": "3.4.12",
  "title": "",
  "body": "  If is an invertible matrix, then .   "
},
{
  "id": "example-25",
  "level": "2",
  "url": "sec-determinants.html#example-25",
  "type": "Example",
  "number": "3.4.13",
  "title": "",
  "body": "  We illustrate how to use a cofactor expansion to find the determinant of where   To begin, we choose one row or column. It doesn't matter which we choose because the result will be the same in any case. Here, we choose the second row .  The determinant will be found by creating a sum of terms, one for each entry in the row we have chosen. For each entry in the row, we form its term by multiplying   where and are the row and column numbers, respectively, of the entry,  the entry itself, and  the determinant of the entries left over when we have crossed out the row and column containing the entry.     Since we are computing the determinant of this matrix using the second row, the entry in the first column of this row is . Let's see how to form the term from this entry.  The term itself is , and the matrix that is left over when we cross out the second row and first column is whose determinant is . Since this entry is in the second row and first column, the term we construct is .  Putting this together, we find the determinant to be . Notice that this agrees with the determinant that we found for this matrix using row operations in the last activity.   "
},
{
  "id": "activity-38",
  "level": "2",
  "url": "sec-determinants.html#activity-38",
  "type": "Activity",
  "number": "3.4.4",
  "title": "",
  "body": "  We will explore cofactor expansions through some examples.  Using a cofactor expansion, show that the determinant of the following matrix . Remember that you can choose any row or column to create the expansion, but the choice of a particular row or column may simplify the computation.  Use a cofactor expansion to find the determinant of . Explain how the cofactor expansion technique shows that the determinant of a triangular matrix is equal to the product of its diagonal entries.  Use a cofactor expansion to determine whether the following vectors form a basis of : .  Sage will compute the determinant of a matrix A with the command A.det() . Use Sage to find the determinant of the matrix .       We will using a cofactor expanion along the first row so that   Expanding along the first row gives   We form the matrix whose columns are the three given vectors. Expanding along either the second row or third column to take advantage of the zero in the entry, we see that , which means that is not invertible. Therefore, the vectors do not form a basis for .  Sage tells us that .    "
},
{
  "id": "exercise-108",
  "level": "2",
  "url": "sec-determinants.html#exercise-108",
  "type": "Exercise",
  "number": "3.4.5.1",
  "title": "",
  "body": " Consider the matrices .  Find the determinants of and using row operations.   Now find the determinants of and using cofactor expansions to verify your results     and .   We find that and .  "
},
{
  "id": "exercise-109",
  "level": "2",
  "url": "sec-determinants.html#exercise-109",
  "type": "Exercise",
  "number": "3.4.5.2",
  "title": "",
  "body": " This exercise concerns rotations and reflections in .  Suppose that is the matrix that performs a counterclockwise rotation in . Draw a typical picture of the vectors that form the columns of and use the geometric definition of the determinant to determine .  Suppose that is the matrix that performs a reflection in a line passing through the origin. Draw a typical picture of the columns of and use the geometric definition of the determinant to determine .  As we saw in , the matrices have the form . Compute the determinants of and and verify that they agree with what you found in the earlier parts of this exercise.      .   .     The vectors and that form the columns of are found by rotating the standard basis vectors and . Consequently, they are positively oriented and form a square. This says that .  The vectors and that form the columns of are found by reflecting the standard basis vectors and . Consequently, they are negatively oriented and form a square. This says that .  We see that and .   "
},
{
  "id": "exercise-110",
  "level": "2",
  "url": "sec-determinants.html#exercise-110",
  "type": "Exercise",
  "number": "3.4.5.3",
  "title": "",
  "body": " In the next chapter, we will say that matrices and are similar if there is a matrix such that .  Suppose that and are matrices and that there is a matrix such that . Explain why .  Suppose that is a matrix and that there is a matrix such that . Find .       because .   .      .   since .   "
},
{
  "id": "exercise-111",
  "level": "2",
  "url": "sec-determinants.html#exercise-111",
  "type": "Exercise",
  "number": "3.4.5.4",
  "title": "",
  "body": " Consider the matrix where is a parameter.  Find an expression for in terms of the parameter .  Use your expression for to determine the values of for which the vectors are linearly independent.          .     We see that .  The vectors are linearly independent when the matrix is invertible, which means that . Therefore, the vectors are linearly independent when .   "
},
{
  "id": "exercise-112",
  "level": "2",
  "url": "sec-determinants.html#exercise-112",
  "type": "Exercise",
  "number": "3.4.5.5",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your response.  If we have a square matrix and multiply the first row by and add it to the third row to obtain , then .  If we interchange two rows of a matrix, then the determinant is unchanged.  If we scale a row of the matrix by to obtain , then .  If and are row equivalent and , then also.  If is row equivalent to the identity matrix, then .      False  False  True  True  False     False. This is a row replacement operation, which leaves the determinant unchanged.  False. Applying an interchange operation changes the sign of the determinant.  True. Scaling a row of by multiplies the determinant by .  True. Row operations either leave the determinant unchanged, change its sign, or multiply it by a nonzero number. Therefore, if and and are related through a sequence of row operations, then .  False. It is true that , but a sequence of row operations that cause and to be row equivalent may multiply the determinant by a nonzero number or change its sign. We do know, however, that and so is invertible.   "
},
{
  "id": "exercise-113",
  "level": "2",
  "url": "sec-determinants.html#exercise-113",
  "type": "Exercise",
  "number": "3.4.5.6",
  "title": "",
  "body": " Suppose that and are matrices such that and . Find the following determinants:   .   .   .   .   .      .   .   .   .   .     Multiplying the entire matrix by scales each row by . Therefore, .  We have .  We know that .  Multiplying the matrix by scales each row by so we have .   .   "
},
{
  "id": "exercise-114",
  "level": "2",
  "url": "sec-determinants.html#exercise-114",
  "type": "Exercise",
  "number": "3.4.5.7",
  "title": "",
  "body": " Suppose that and are matrices.  If and are both invertible, use determinants to explain why is invertible.  If is invertible, use determinants to explain why both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.     If and are invertible, then and . Therefore, , which shows that is invertible.  If is invertible, then . This implies that and , from which we conclude that both and are invertible.   "
},
{
  "id": "exercise-115",
  "level": "2",
  "url": "sec-determinants.html#exercise-115",
  "type": "Exercise",
  "number": "3.4.5.8",
  "title": "",
  "body": " Provide a justification for your responses to the following questions.  If every entry in one row of a matrix is zero, what can you say about the determinant?  If two rows of a square matrix are identical, what can you say about the determinant?  If two columns of a square matrix are identical, what can you say about the determinant?  If one column of a matrix is a linear combination of the others, what can you say about the determinant?     In all four cases, the determinant must be zero.    The determinant must be zero. This is because the matrix cannot be invertible since there is a row without a pivot. Also, applying a cofactor expansion along that row will produce zero for the determinant.  The determinant is zero again. If we multiply one of the rows by and add it to the other row, we obtain a row whose entries are all zero. This operation does not change the determinant, but we know that the determinant of the new matrix is zero by the previous part of this problem.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.  The determinant must be zero. This is because the columns are not linearly independent, which means that the matrix cannot be invertible. Hence, the determinant is zero.   "
},
{
  "id": "exercise-116",
  "level": "2",
  "url": "sec-determinants.html#exercise-116",
  "type": "Exercise",
  "number": "3.4.5.9",
  "title": "",
  "body": " Consider the matrix .  Assuming that , rewrite the equation in terms of , , and .  Explain why and , the first two columns of , satisfy the equation you found in the previous part.  Explain why the solution space of this equation is the plane spanned by and .      .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent.  If for a vector , then is a linear combination of and .     Using a cofactor expansion, we have .  If we replace the third column by either or , we obtain a matrix whose columns are linearly dependent. Therefore, the determinant of the matrix must be zero.  If for some vector , then the columns of must be linear dependent. Therefore, is a linear combination of and and lies in the plane that is their span.   "
},
{
  "id": "exercise-117",
  "level": "2",
  "url": "sec-determinants.html#exercise-117",
  "type": "Exercise",
  "number": "3.4.5.10",
  "title": "",
  "body": " In this section, we studied the effect of row operations on the matrix . In this exercise, we will study the effect of analogous column operations.  Suppose that is the matrix . Also consider elementary matrices .  Explain why the matrix is obtained from by replacing the first column by . We call this a column replacement operation. Explain why column replacement operations do not change the determinant.  Explain why the matrix is obtained from by multiplying the second column by . Explain the effect that scaling a column has on the determinant of a matrix.  Explain why the matrix is obtained from by interchanging the first and third columns. What is the effect of this operation on the determinant?  Use column operations to compute the determinant of .      Column replacements do not change the determinant because .  Scaling a column by multiplies the determinant by because .  Interchanges change the sign of the determinant because .   .     The columns of are found by multiplying the columns of by . If we remember that a vector multiplied by a matrix forms a linear combination of the columns of , we see that the first column of is and that the other two columns are unchanged.  Since , we have , which says that this operation does not change the determinant.  Multiplying by multiplies the determinant by since .  Multiplying by changes the sign of the determinant since .  If we interchange the first and third columns of , we have and . Now we will multiply the first column by and add to the second column to obtain and . Let's interchange the second and third columns so that so that and then multiply the second column by and add to the third column so that This gives .   "
},
{
  "id": "exercise-118",
  "level": "2",
  "url": "sec-determinants.html#exercise-118",
  "type": "Exercise",
  "number": "3.4.5.11",
  "title": "",
  "body": " Consider the matrices . Use row operations to find the determinants of these matrices.    , , and .   It takes three interchanges to see that , which implies that .  Two interchanges show us that and hence .  Two interchanges produce a diagonal matrix so that .  "
},
{
  "id": "exercise-119",
  "level": "2",
  "url": "sec-determinants.html#exercise-119",
  "type": "Exercise",
  "number": "3.4.5.12",
  "title": "",
  "body": " Consider the matrices   Use row (and\/or column) operations to find the determinants of these matrices.  Write the and matrices that follow in this pattern and state their determinants based on what you have seen.      , , , and .  If and are the and matrices, respectively, we expect that and .      , , , and .  If and are the and matrices, respectively, we expect that and .   "
},
{
  "id": "exercise-120",
  "level": "2",
  "url": "sec-determinants.html#exercise-120",
  "type": "Exercise",
  "number": "3.4.5.13",
  "title": "",
  "body": " The following matrix is called a Vandermond matrix: .  Use row operations to explain why .  Explain why is invertible if and only if , , and are all distinct real numbers.  There is a natural way to generalize this to a matrix with parameters , , , and . Write this matrix and state its determinant based on your previous work.    This matrix appeared in when we were found a polynomial that passed through a given set of points.    Perform a sequence of row operations to form an upper triangular matrix.  If , , and are distinct, then .  The determinant is .     If , , and are not distinct, we see that because two of the rows are identical. Therefore, we apply the following row operations, assuming , , and are distinct: Because of the scalings by and , we have .  If , , and are distinct, then , which implies that is invertible.  In the case, we have    "
},
{
  "id": "sec-subspaces",
  "level": "1",
  "url": "sec-subspaces.html",
  "type": "Section",
  "number": "3.5",
  "title": "Subspaces",
  "body": " Subspaces   In this chapter, we have been looking at bases for , sets of vectors that are linearly independent and span . Frequently, however, we focus on only a subset of . In particular, if we are given an matrix , we have been interested in both the span of the columns of and the solution space to the homogeneous equation . In this section, we will expand the concept of basis to describe sets like these.    Let's consider the following matrix and its reduced row echelon form. .  Are the columns of linearly independent? Is the span of the columns ?  Give a parametric description of the solution space to the homogeneous equation .  Explain how this parametric description produces two vectors and whose span is the solution space to the equation .  What can you say about the linear independence of the set of vectors and ?  Let's denote the columns of as , , , and . Explain why and can be written as linear combinations of and .  Explain why and are linearly independent and       The columns of are not linearly independent since there is not a pivot position in every column. Also, the span of the columns is not because there is not a pivot position in every row.  From the reduced row echelon form, we see that the homogeneous equation leads to the equations which leads to the parametric description   We see that every vector in the solution space is a linear combination of the vectors and .  This pair of vectors is linearly independent because one is not a scalar multiple of the other.  From the reduced row echelon form of , we see that and .  We see that and are linearly independent from the reduced row echelon form of . Moreover, we know that and can be written as linear combinations of and . Therefore, any linear combination of , , , and can be written as a linear combination of and alone.       Subspaces  Our goal is to develop a common framework for describing subsets like the span of the columns of a matrix and the solution space to a homogeneous equation. That leads us to the following definition.   subspace   A subspace of is a subset of that is the span of a set of vectors.    Since we have explored the concept of span in some detail, this definition just gives us a new word to describe something familiar. Let's look at some examples.   Subspaces of   In and the following discussion, we looked at subspaces in without explicitly using that language. Let's recall some of those examples.     Suppose we have a single nonzero vector . The span of is a subspace, which we'll write as . As we have seen, the span of a single vector consists of all scalar multiples of that vector, and these form a line passing through the origin.        If instead we have two linearly independent vectors and , the subspace is a plane passing through the origin.      Consider the three vectors , , and . Since we know that every 3-dimensional vector can be written as a linear combination, we have .    One more subspace worth mentioning is . Since any linear combination of the zero vector is itself the zero vector, this subspace consists of a single vector, .     In fact, any subspace of is one of these types: the origin, a line, a plane, or all of .     We will look at some sets of vectors and the subspaces they form.   If is a set of vectors in , explain why can be expressed as a linear combination of these vectors. Use this fact to explain why the zero vector belongs to any subspace in .    Explain why the line on the left of is not a subspace of and why the line on the right is.   Two lines in , one of which is a subspace and one of which is not.         Consider the vectors and describe the subspace of .     Consider the vectors    Write as a linear combination of and .    Explain why .    Describe the subspace of .       Suppose that , , , and are four vectors in and that Give a description of the subspace of .          If we choose all the weights , then the linear combination This means that is the subspace .    The line on the left cannot be a subspace of since it does not contain the zero vector. The line on the right is a subspace because it can be represented as the span of any nonzero vector on the line.    The matrix whose columns are the given vectors has a pivot in every row. Therefore, the span of these vectors is and so .       We see that .    Any linear combination     The subspace is a plane in .       Since we can write and , then which is a plane in .       As the activity shows, it is possible to represent some subspaces as the span of more than one set of vectors. We are particularly interested in representing a subspace as the span of a linearly independent set of vectors.    dimension  A basis for a subspace of is a set of vectors in that are linearly independent and whose span is . We say that the dimension of the subspace , denoted , is the number of vectors in any basis.     A subspace of   Suppose we have the 4-dimensional vectors , , and that define the subspace of . Suppose also that From the reduced row echelon form of the matrix, we see that . Therefore, any linear combination of , , and can be rewritten as a linear combination of and . This tells us that   Furthermore, the reduced row echelon form of the matrix shows that and are linearly independent. Therefore, is a basis for , which means that is a two-dimensional subspace of .   Subspaces of are either   0-dimensional, consisting of the single vector ,    a 1-dimensional line,    a 2-dimensional plane, or    the 3-dimensional subspace .   There is no 4-dimensional subspace of because there is no linearly independent set of four vectors in .  There are two important subspaces associated to any matrix, each of which springs from one of our two fundamental questions, as we will now see.    The column space of   The first subspace associated to a matrix that we'll consider is its column space.    column space  If is an matrix, we call the span of its columns the column space of and denote it as .    Notice that the columns of are vectors in , which means that any linear combination of the columns is also in . Since the column space is described as the span of a set of vectors, we see that is a subspace of .    We will explore some column spaces in this activity.  Consider the matrix Since is the span of the columns, we have Explain why can be written as a linear combination of and and why .   Explain why the vectors and form a basis for and why is a 2-dimensional subspace of and therefore a plane.  Now consider the matrix and its reduced row echelon form: Explain why is a 1-dimensional subspace of and is therefore a line.  For a general matrix , what is the relationship between the dimension and the number of pivot positions in ?  How does the location of the pivot positions indicate a basis for ?  If is an invertible matrix, what can you say about the column space ?   Suppose that is an matrix and that . If is an 8-dimensional vector, what can you say about the equation ?       We have which shows that the vectors are not linearly independent and, in fact, that . As we've seen several times, this means that any linear combination of , , and can be written as a linear combination of and alone and hence that   The reduced row echelon form of shows that and are linearly independent. We also know that the span of these two vectors is . Therefore, they form a basis for .  Denoting the columns of as , the reduced row echelon form shows that , , and . Therefore, any linear combination of , , , and can be written as a linear combination of alone. This means that forms a basis for , which is then the line consisting of all scalar multiples of . p  The number of vectors in a basis of equals the number of pivot positions. Therefore, equals the number of pivot positions in .  As the examples in this activity illustrate, the columns of that contain pivot positions form a basis for .  If is invertible, then it has a pivot position in every row, which means that the span of the columns is . Therefore, .  Since , we know that every 8-dimensional vector is in . This means that is in the span of the columns of so the equation must be consistent.      Consider the matrix and its reduced row echelon form: and denote the columns of as .  It is certainly true that by the definition of the column space. However, the reduced row echelon form of the matrix shows us that the vectors are not linearly independent so do not form a basis for .  From the reduced row echelon form, however, we can see that . This means that any linear combination of can be written as a linear combination of just and . Therefore, we see that .  Moreover, the reduced row echelon form shows that and are linearly independent, which implies that they form a basis for . This means that is a 2-dimensional subspace of , which is a plane in , having basis .   In general, a column without a pivot position can be written as a linear combination of the columns that have pivot positions. This means that a basis for will always be given by the columns of having pivot positions. This leads us to the following definition and proposition.   rank  matrix, rank   The rank of a matrix is the number of pivot positions in and is denoted by .      If is an matrix, then is a subspace of whose dimension equals . The columns of that contain pivot positions form a basis for .    For example, the rank of the matrix in is two because there are two pivot positions. A basis for is given by the first two columns of since those columns have pivot positions.  As a note of caution, we determine the pivot positions by looking at the reduced row echelon form of . However, we form a basis of from the columns of rather than the columns of the reduced row echelon matrix.    The null space of   The second subspace associated to a matrix is its null space.    null space  If is an matrix, we call the subset of vectors in satisfying the null space of and denote it by .    Remember that a subspace is a subset that can be represented as the span of a set of vectors. The column space of , which is simply the span of the columns of , fits this definition. It may not be immediately clear how the null space of , which is the solution space of the equation , does, but we will see that is a subspace of .    We will explore some null spaces in this activity and see why satisfies the definition of a subspace.  Consider the matrix and give a parametric description of the solution space to the equation . In other words, give a parametric description of .    This parametric description shows that the vectors satisfying the equation can be written as a linear combination of a set of vectors. In other words, this description shows why is the span of a set of vectors and is therefore a subspace. Identify a set of vectors whose span is .    Use this set of vectors to find a basis for and state the dimension of .   The null space is a subspace of for which value of ?  Now consider the matrix whose reduced row echelon form is given by Give a parametric description of .   The parametric description gives a set of vectors that span . Explain why this set of vectors is linearly independent and hence forms a basis. What is the dimension of ?    For a general matrix , how does the number of pivot positions indicate the dimension of ?    Suppose that the columns of a matrix are linearly independent. What can you say about ?       We have which leads to the parametric description of the solution space to the homogeneous equation:   The parametric description shows that every solution to the equation is a linear combination of and .   The vectors and are linearly independent so they form a basis for . Therefore, is 2-dimensional.   The vectors in are 4-dimensional so is a subspace of .  A parametric description of the null space is . We can check that the vectors are linearly independent so they form a basis for . This means that is 3-dimensional.  The number of vectors in a basis of the null space equals the number of free variables that appear in the equation , which is the number of columns that do not have pivot positions. This says that equals the number of columns of minus the number of pivot positions.  If the columns are linearly independent, then the homogeneous equation has only the zero solution . Therefore, .      Consider the matrix along with its reduced row echelon form:   To find a parametric description of the solution space to , imagine that we augment both and its reduced row echelon form by a column of zeroes, which leads to the equations Notice that , , and are free variables so we rewrite these equations as In vector form, we have   This expression says that any vector satisfying is a linear combination of the vectors It is straightforward to check that these vectors are linearly independent, which means that , , and form a basis for , a 3-dimensional subspace of .   As illustrated in this example, the dimension of is equal to the number of free variables in the equation , which equals the number of columns of without pivot positions or the number of columns of minus the number of pivot positions.    If is an matrix, then is a subspace of whose dimension is     Combining and shows that    If is an matrix, then       Summary  Once again, we find ourselves revisiting our two fundamental questions concerning the existence and uniqueness of solutions to linear systems. The column space contains all the vectors for which the equation is consistent. The null space is the solution space to the equation , which reflects on the uniqueness of solutions to this and other equations.    A subspace of is a subset of that can be represented as the span of a set of vectors. A basis of is a linearly independent set of vectors whose span is .  If is an matrix, the column space is the span of the columns of and forms a subspace of .  A basis for is found from the columns of that have pivot positions. The dimension is therefore .  The null space is the solution space to the homogeneous equation and is a subspace of .  A basis for is found through a parametric description of the solution space of , and we have that .       Suppose that and its reduced row echelon form are .  The null space is a subspace of for what ? The column space is a subspace of for what ?  What are the dimensions and ?  Find a basis for the column space .  Find a basis for the null space .       is a subspace of and is a subspace of .   and .            is a subspace of and is a subspace of .  Because there are three pivot positions, we see that . Therefore, and .  A basis for is given by the columns of that contain pivot positions. Therefore, a basis is   We can write a parametric description for the solution space to the homogeneous equation as Therefore, a basis for is      Suppose that .  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?      Yes  No  No  Yes  No     Yes. This vector is a column of so it may be written as a linear combination of the columns of .  No. Vectors in must be three-dimensional.  No. Vectors in must be four-dimensional.  Yes, because this vector, when multiplied by , gives .  No, because this vector, when multiplied by , does not give .     Determine whether the following statements are true or false and provide a justification for your response. Unless otherwise stated, assume that is an matrix.  If is a matrix, then is a subspace of .  If , then the columns of are linearly independent.  If , then is invertible.  If has a pivot position in every column, then .  If and , then is invertible.      False  True  False  False  True     False. is a subspace of .  True. In this case, the only solution to the homogeneous equation is the zero solution . This means that every column has a pivot position so the columns are linearly independent.  False. The matrix is not necessarily a square matrix.  False. If has a pivot position in every column, then .  True. Since , we know that has a pivot position in every row. Since , we know that has a pivot position in every column. Therefore, must be a square matrix and invertible.     Explain why the following statements are true.  If is invertible, then .  If is invertible, then .  If , then .      If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .     If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .     For each of the following conditions, construct a matrix having the given properties.   .   .   .   .      .   .   .   .      .   .   .   .     Suppose that is a matrix.  Is it possible that ?  If , what can you say about ?  If , what can you say about ?   If , what can you say about ?  If , what can you say about ?      No      is a plane in    is a line in    .     No. There are more columns than rows so there must be at least one column without a pivot position.  Remember that we have . Therefore, we have in this case, which implies that .  Here, so is a plane in .  Here, so is a line in .  Here, so .     Suppose we have the vectors and that is a matrix such that and .  What are the dimensions of ?  Find such a matrix .              Since and are three-dimensional vectors, must have three rows. Since and are 4-dimensional, must have four columns. Alternatively, we know that and . Therefore, the number of columns is . Hence, is a matrix.  We can use and as the first two columns of . We also know that and . If we call the other two columns and , then implies that so that . Since , we have , which says that . This gives      Suppose that is an matrix and that .  What can you conclude about ?  What can you conclude about ?      and .   We know that is invertible so and .    Suppose that is a matrix and there is an invertible matrix such that .  What can you conclude about ?  What can you conclude about ?      and .   We know that so is invertible. Therefore, and .    In this section, we saw that the solution space to the homogeneous equation is a subspace of for some . In this exercise, we will investigate whether the solution space to another equation can form a subspace.  Let's consider the matrix .  Find a parametric description of the solution space to the homogeneous equation .   Graph the solution space to the homogeneous equation to the right.     Find a parametric description of the solution space to the equation and graph it above.  Is the solution space to the equation a subspace of ?  Find a parametric description of the solution space to the equation and graph it above.  What can you say about all the solution spaces to equations of the form when is a vector in ?  Suppose that the solution space to the equation forms a subspace. Explain why it must be true that .        The solution space forms a line through the origin.     No   . The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.   must be in the solution space.     We have which shows that describes the solution space to the homogeneous equation.  The solution space forms a line through the origin.  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation.  This cannot form a subspace since it does not contain the vector .  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation. The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.  If the solution space is a subspace, then must be in the solution space. This means that .     "
},
{
  "id": "exploration-13",
  "level": "2",
  "url": "sec-subspaces.html#exploration-13",
  "type": "Preview Activity",
  "number": "3.5.1",
  "title": "",
  "body": "  Let's consider the following matrix and its reduced row echelon form. .  Are the columns of linearly independent? Is the span of the columns ?  Give a parametric description of the solution space to the homogeneous equation .  Explain how this parametric description produces two vectors and whose span is the solution space to the equation .  What can you say about the linear independence of the set of vectors and ?  Let's denote the columns of as , , , and . Explain why and can be written as linear combinations of and .  Explain why and are linearly independent and       The columns of are not linearly independent since there is not a pivot position in every column. Also, the span of the columns is not because there is not a pivot position in every row.  From the reduced row echelon form, we see that the homogeneous equation leads to the equations which leads to the parametric description   We see that every vector in the solution space is a linear combination of the vectors and .  This pair of vectors is linearly independent because one is not a scalar multiple of the other.  From the reduced row echelon form of , we see that and .  We see that and are linearly independent from the reduced row echelon form of . Moreover, we know that and can be written as linear combinations of and . Therefore, any linear combination of , , , and can be written as a linear combination of and alone.    "
},
{
  "id": "definition-16",
  "level": "2",
  "url": "sec-subspaces.html#definition-16",
  "type": "Definition",
  "number": "3.5.1",
  "title": "",
  "body": " subspace   A subspace of is a subset of that is the span of a set of vectors.   "
},
{
  "id": "example-26",
  "level": "2",
  "url": "sec-subspaces.html#example-26",
  "type": "Example",
  "number": "3.5.2",
  "title": "Subspaces of <span class=\"process-math\">\\(\\real^3\\)<\/span>.",
  "body": " Subspaces of   In and the following discussion, we looked at subspaces in without explicitly using that language. Let's recall some of those examples.     Suppose we have a single nonzero vector . The span of is a subspace, which we'll write as . As we have seen, the span of a single vector consists of all scalar multiples of that vector, and these form a line passing through the origin.        If instead we have two linearly independent vectors and , the subspace is a plane passing through the origin.      Consider the three vectors , , and . Since we know that every 3-dimensional vector can be written as a linear combination, we have .    One more subspace worth mentioning is . Since any linear combination of the zero vector is itself the zero vector, this subspace consists of a single vector, .     In fact, any subspace of is one of these types: the origin, a line, a plane, or all of .  "
},
{
  "id": "activity-39",
  "level": "2",
  "url": "sec-subspaces.html#activity-39",
  "type": "Activity",
  "number": "3.5.2",
  "title": "",
  "body": "  We will look at some sets of vectors and the subspaces they form.   If is a set of vectors in , explain why can be expressed as a linear combination of these vectors. Use this fact to explain why the zero vector belongs to any subspace in .    Explain why the line on the left of is not a subspace of and why the line on the right is.   Two lines in , one of which is a subspace and one of which is not.         Consider the vectors and describe the subspace of .     Consider the vectors    Write as a linear combination of and .    Explain why .    Describe the subspace of .       Suppose that , , , and are four vectors in and that Give a description of the subspace of .          If we choose all the weights , then the linear combination This means that is the subspace .    The line on the left cannot be a subspace of since it does not contain the zero vector. The line on the right is a subspace because it can be represented as the span of any nonzero vector on the line.    The matrix whose columns are the given vectors has a pivot in every row. Therefore, the span of these vectors is and so .       We see that .    Any linear combination     The subspace is a plane in .       Since we can write and , then which is a plane in .      "
},
{
  "id": "definition-17",
  "level": "2",
  "url": "sec-subspaces.html#definition-17",
  "type": "Definition",
  "number": "3.5.4",
  "title": "",
  "body": "  dimension  A basis for a subspace of is a set of vectors in that are linearly independent and whose span is . We say that the dimension of the subspace , denoted , is the number of vectors in any basis.   "
},
{
  "id": "example-27",
  "level": "2",
  "url": "sec-subspaces.html#example-27",
  "type": "Example",
  "number": "3.5.5",
  "title": "A subspace of <span class=\"process-math\">\\(\\real^4\\)<\/span>.",
  "body": " A subspace of   Suppose we have the 4-dimensional vectors , , and that define the subspace of . Suppose also that From the reduced row echelon form of the matrix, we see that . Therefore, any linear combination of , , and can be rewritten as a linear combination of and . This tells us that   Furthermore, the reduced row echelon form of the matrix shows that and are linearly independent. Therefore, is a basis for , which means that is a two-dimensional subspace of .  "
},
{
  "id": "definition-18",
  "level": "2",
  "url": "sec-subspaces.html#definition-18",
  "type": "Definition",
  "number": "3.5.6",
  "title": "",
  "body": "  column space  If is an matrix, we call the span of its columns the column space of and denote it as .   "
},
{
  "id": "activity-40",
  "level": "2",
  "url": "sec-subspaces.html#activity-40",
  "type": "Activity",
  "number": "3.5.3",
  "title": "",
  "body": "  We will explore some column spaces in this activity.  Consider the matrix Since is the span of the columns, we have Explain why can be written as a linear combination of and and why .   Explain why the vectors and form a basis for and why is a 2-dimensional subspace of and therefore a plane.  Now consider the matrix and its reduced row echelon form: Explain why is a 1-dimensional subspace of and is therefore a line.  For a general matrix , what is the relationship between the dimension and the number of pivot positions in ?  How does the location of the pivot positions indicate a basis for ?  If is an invertible matrix, what can you say about the column space ?   Suppose that is an matrix and that . If is an 8-dimensional vector, what can you say about the equation ?       We have which shows that the vectors are not linearly independent and, in fact, that . As we've seen several times, this means that any linear combination of , , and can be written as a linear combination of and alone and hence that   The reduced row echelon form of shows that and are linearly independent. We also know that the span of these two vectors is . Therefore, they form a basis for .  Denoting the columns of as , the reduced row echelon form shows that , , and . Therefore, any linear combination of , , , and can be written as a linear combination of alone. This means that forms a basis for , which is then the line consisting of all scalar multiples of . p  The number of vectors in a basis of equals the number of pivot positions. Therefore, equals the number of pivot positions in .  As the examples in this activity illustrate, the columns of that contain pivot positions form a basis for .  If is invertible, then it has a pivot position in every row, which means that the span of the columns is . Therefore, .  Since , we know that every 8-dimensional vector is in . This means that is in the span of the columns of so the equation must be consistent.    "
},
{
  "id": "example-col-basis",
  "level": "2",
  "url": "sec-subspaces.html#example-col-basis",
  "type": "Example",
  "number": "3.5.7",
  "title": "",
  "body": " Consider the matrix and its reduced row echelon form: and denote the columns of as .  It is certainly true that by the definition of the column space. However, the reduced row echelon form of the matrix shows us that the vectors are not linearly independent so do not form a basis for .  From the reduced row echelon form, however, we can see that . This means that any linear combination of can be written as a linear combination of just and . Therefore, we see that .  Moreover, the reduced row echelon form shows that and are linearly independent, which implies that they form a basis for . This means that is a 2-dimensional subspace of , which is a plane in , having basis .  "
},
{
  "id": "definition-19",
  "level": "2",
  "url": "sec-subspaces.html#definition-19",
  "type": "Definition",
  "number": "3.5.8",
  "title": "",
  "body": " rank  matrix, rank   The rank of a matrix is the number of pivot positions in and is denoted by .   "
},
{
  "id": "proposition-col-dim",
  "level": "2",
  "url": "sec-subspaces.html#proposition-col-dim",
  "type": "Proposition",
  "number": "3.5.9",
  "title": "",
  "body": "  If is an matrix, then is a subspace of whose dimension equals . The columns of that contain pivot positions form a basis for .   "
},
{
  "id": "definition-20",
  "level": "2",
  "url": "sec-subspaces.html#definition-20",
  "type": "Definition",
  "number": "3.5.10",
  "title": "",
  "body": "  null space  If is an matrix, we call the subset of vectors in satisfying the null space of and denote it by .   "
},
{
  "id": "activity-41",
  "level": "2",
  "url": "sec-subspaces.html#activity-41",
  "type": "Activity",
  "number": "3.5.4",
  "title": "",
  "body": "  We will explore some null spaces in this activity and see why satisfies the definition of a subspace.  Consider the matrix and give a parametric description of the solution space to the equation . In other words, give a parametric description of .    This parametric description shows that the vectors satisfying the equation can be written as a linear combination of a set of vectors. In other words, this description shows why is the span of a set of vectors and is therefore a subspace. Identify a set of vectors whose span is .    Use this set of vectors to find a basis for and state the dimension of .   The null space is a subspace of for which value of ?  Now consider the matrix whose reduced row echelon form is given by Give a parametric description of .   The parametric description gives a set of vectors that span . Explain why this set of vectors is linearly independent and hence forms a basis. What is the dimension of ?    For a general matrix , how does the number of pivot positions indicate the dimension of ?    Suppose that the columns of a matrix are linearly independent. What can you say about ?       We have which leads to the parametric description of the solution space to the homogeneous equation:   The parametric description shows that every solution to the equation is a linear combination of and .   The vectors and are linearly independent so they form a basis for . Therefore, is 2-dimensional.   The vectors in are 4-dimensional so is a subspace of .  A parametric description of the null space is . We can check that the vectors are linearly independent so they form a basis for . This means that is 3-dimensional.  The number of vectors in a basis of the null space equals the number of free variables that appear in the equation , which is the number of columns that do not have pivot positions. This says that equals the number of columns of minus the number of pivot positions.  If the columns are linearly independent, then the homogeneous equation has only the zero solution . Therefore, .    "
},
{
  "id": "example-null-intro",
  "level": "2",
  "url": "sec-subspaces.html#example-null-intro",
  "type": "Example",
  "number": "3.5.11",
  "title": "",
  "body": " Consider the matrix along with its reduced row echelon form:   To find a parametric description of the solution space to , imagine that we augment both and its reduced row echelon form by a column of zeroes, which leads to the equations Notice that , , and are free variables so we rewrite these equations as In vector form, we have   This expression says that any vector satisfying is a linear combination of the vectors It is straightforward to check that these vectors are linearly independent, which means that , , and form a basis for , a 3-dimensional subspace of .  "
},
{
  "id": "proposition-nul-dim",
  "level": "2",
  "url": "sec-subspaces.html#proposition-nul-dim",
  "type": "Proposition",
  "number": "3.5.12",
  "title": "",
  "body": "  If is an matrix, then is a subspace of whose dimension is    "
},
{
  "id": "proposition-28",
  "level": "2",
  "url": "sec-subspaces.html#proposition-28",
  "type": "Proposition",
  "number": "3.5.13",
  "title": "",
  "body": "  If is an matrix, then    "
},
{
  "id": "exercise-121",
  "level": "2",
  "url": "sec-subspaces.html#exercise-121",
  "type": "Exercise",
  "number": "3.5.5.1",
  "title": "",
  "body": " Suppose that and its reduced row echelon form are .  The null space is a subspace of for what ? The column space is a subspace of for what ?  What are the dimensions and ?  Find a basis for the column space .  Find a basis for the null space .       is a subspace of and is a subspace of .   and .            is a subspace of and is a subspace of .  Because there are three pivot positions, we see that . Therefore, and .  A basis for is given by the columns of that contain pivot positions. Therefore, a basis is   We can write a parametric description for the solution space to the homogeneous equation as Therefore, a basis for is    "
},
{
  "id": "exercise-122",
  "level": "2",
  "url": "sec-subspaces.html#exercise-122",
  "type": "Exercise",
  "number": "3.5.5.2",
  "title": "",
  "body": " Suppose that .  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?  Is the vector in ?      Yes  No  No  Yes  No     Yes. This vector is a column of so it may be written as a linear combination of the columns of .  No. Vectors in must be three-dimensional.  No. Vectors in must be four-dimensional.  Yes, because this vector, when multiplied by , gives .  No, because this vector, when multiplied by , does not give .   "
},
{
  "id": "exercise-123",
  "level": "2",
  "url": "sec-subspaces.html#exercise-123",
  "type": "Exercise",
  "number": "3.5.5.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response. Unless otherwise stated, assume that is an matrix.  If is a matrix, then is a subspace of .  If , then the columns of are linearly independent.  If , then is invertible.  If has a pivot position in every column, then .  If and , then is invertible.      False  True  False  False  True     False. is a subspace of .  True. In this case, the only solution to the homogeneous equation is the zero solution . This means that every column has a pivot position so the columns are linearly independent.  False. The matrix is not necessarily a square matrix.  False. If has a pivot position in every column, then .  True. Since , we know that has a pivot position in every row. Since , we know that has a pivot position in every column. Therefore, must be a square matrix and invertible.   "
},
{
  "id": "exercise-124",
  "level": "2",
  "url": "sec-subspaces.html#exercise-124",
  "type": "Exercise",
  "number": "3.5.5.4",
  "title": "",
  "body": " Explain why the following statements are true.  If is invertible, then .  If is invertible, then .  If , then .      If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .     If , then if is invertible.  The column space consists of all the vectors for which is consistent. We can rewrite this condition as , which means that is consistent exactly when is.  If , then and have the same reduced row echelon form, which determines .   "
},
{
  "id": "exercise-125",
  "level": "2",
  "url": "sec-subspaces.html#exercise-125",
  "type": "Exercise",
  "number": "3.5.5.5",
  "title": "",
  "body": " For each of the following conditions, construct a matrix having the given properties.   .   .   .   .      .   .   .   .      .   .   .   .   "
},
{
  "id": "exercise-126",
  "level": "2",
  "url": "sec-subspaces.html#exercise-126",
  "type": "Exercise",
  "number": "3.5.5.6",
  "title": "",
  "body": " Suppose that is a matrix.  Is it possible that ?  If , what can you say about ?  If , what can you say about ?   If , what can you say about ?  If , what can you say about ?      No      is a plane in    is a line in    .     No. There are more columns than rows so there must be at least one column without a pivot position.  Remember that we have . Therefore, we have in this case, which implies that .  Here, so is a plane in .  Here, so is a line in .  Here, so .   "
},
{
  "id": "exercise-127",
  "level": "2",
  "url": "sec-subspaces.html#exercise-127",
  "type": "Exercise",
  "number": "3.5.5.7",
  "title": "",
  "body": " Suppose we have the vectors and that is a matrix such that and .  What are the dimensions of ?  Find such a matrix .              Since and are three-dimensional vectors, must have three rows. Since and are 4-dimensional, must have four columns. Alternatively, we know that and . Therefore, the number of columns is . Hence, is a matrix.  We can use and as the first two columns of . We also know that and . If we call the other two columns and , then implies that so that . Since , we have , which says that . This gives    "
},
{
  "id": "exercise-128",
  "level": "2",
  "url": "sec-subspaces.html#exercise-128",
  "type": "Exercise",
  "number": "3.5.5.8",
  "title": "",
  "body": " Suppose that is an matrix and that .  What can you conclude about ?  What can you conclude about ?      and .   We know that is invertible so and .  "
},
{
  "id": "exercise-129",
  "level": "2",
  "url": "sec-subspaces.html#exercise-129",
  "type": "Exercise",
  "number": "3.5.5.9",
  "title": "",
  "body": " Suppose that is a matrix and there is an invertible matrix such that .  What can you conclude about ?  What can you conclude about ?      and .   We know that so is invertible. Therefore, and .  "
},
{
  "id": "exercise-130",
  "level": "2",
  "url": "sec-subspaces.html#exercise-130",
  "type": "Exercise",
  "number": "3.5.5.10",
  "title": "",
  "body": " In this section, we saw that the solution space to the homogeneous equation is a subspace of for some . In this exercise, we will investigate whether the solution space to another equation can form a subspace.  Let's consider the matrix .  Find a parametric description of the solution space to the homogeneous equation .   Graph the solution space to the homogeneous equation to the right.     Find a parametric description of the solution space to the equation and graph it above.  Is the solution space to the equation a subspace of ?  Find a parametric description of the solution space to the equation and graph it above.  What can you say about all the solution spaces to equations of the form when is a vector in ?  Suppose that the solution space to the equation forms a subspace. Explain why it must be true that .        The solution space forms a line through the origin.     No   . The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.   must be in the solution space.     We have which shows that describes the solution space to the homogeneous equation.  The solution space forms a line through the origin.  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation.  This cannot form a subspace since it does not contain the vector .  The solution space has the parametric description , which is a line parallel to the solution space to the homogeneous equation. The graph of the solution spaces is shown below.     They are all parallel to the solution space to the homogeneous equation.  If the solution space is a subspace, then must be in the solution space. This means that .   "
},
{
  "id": "sec-eigen-intro",
  "level": "1",
  "url": "sec-eigen-intro.html",
  "type": "Section",
  "number": "4.1",
  "title": "An introduction to eigenvalues and eigenvectors",
  "body": " An introduction to eigenvalues and eigenvectors   This section introduces the concept of eigenvalues and eigenvectors and offers an example that motivates our interest in them. The point here is to develop an intuitive understanding of eigenvalues and eigenvectors and explain how they can be used to simplify some problems that we have previously encountered. In the rest of this chapter, we will develop this concept into a richer theory and illustrate its use with more meaningful examples.    Before we introduce the definition of eigenvectors and eigenvalues, it will be helpful to remember some ideas we have seen previously.    Suppose that is the vector shown in the figure. Sketch the vector and the vector .     State the geometric effect that scalar multiplication has on the vector . Then sketch all the vectors of the form where is a scalar.  State the geometric effect of the matrix transformation defined by .  Suppose that is a matrix and that and are vectors such that . Use the linearity of matrix multiplication to express the following vectors in terms of and .   .   .   .   .   .   .        The vectors are as shown.     Scalar multiplication has the effect of stretching and possibly flipping along the line defined by .  This matrix transformation stretches vectors by a factor of in the horizontal direction and flips vectors vertically.  Applying linearity, we see that   .   .   .   .   .   .         A few examples  We will now introduce the definition of eigenvalues and eigenvectors and then look at a few simple examples.    eigenvalue  eigenvector  Given a square matrix , we say that a nonzero vector is an eigenvector of if there is a scalar such that . The scalar is called the eigenvalue associated to the eigenvector .    At first glance, there is a lot going on in this definition so let's look at an example.    Consider the matrix and the vector . We find that . In other words, , which says that is an eigenvector of the matrix with associated eigenvalue .  Similarly, if , we find that . Here again, we have showing that is an eigenvector of with associated eigenvalue .      This definition has an important geometric interpretation that we will investigate here.  Suppose that is a nonzero vector and that is a scalar. What is the geometric relationship between and ?  Let's now consider the eigenvector condition: . Here we have two vectors, and . If , what is the geometric relationship between and ?    A geometric interpretation of the eigenvalue-eigenvector condition .    Choose the matrix . Move the vector so that the eigenvector condition holds. What is the eigenvector and what is the associated eigenvalue?  By algebraically computing , verify that the eigenvector condition holds for the vector that you found.  If you multiply the eigenvector that you found by , do you still have an eigenvector? If so, what is the associated eigenvalue?  Are you able to find another eigenvector that is not a scalar multiple of the first one that you found? If so, what is the eigenvector and what is the associated eigenvalue?  Now consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues.  Finally, consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues. What geometric transformation does this matrix perform on vectors? How does this explain the presence of any eigenvectors?       The vectors and lie on the same line.  The vectors and lie on the same line.  There are many possibilities, but we see that is an eigenvector with associated eigenvalue .  If we perform the matrix multiplication, we see that .  Yes, is still an eigenvector with associated eigenvalue .  We see that is an eigenvector with associated eigenvalue .  The only eigenvectors that appear are scalar multiples of with associated eigenvalue .  There are no eigenvectors. The matrix transformation rotates vectors by so it is not possible for and to lie on the same line.     Let's consider the ideas we saw in the activity in some more depth. To be an eigenvector of , the vector must satisfy for some scalar . This means that and are scalar multiples of each other so they must lie on the same line.  Consider now the matrix . On the left of , we see that is not an eigenvector of since the vectors and do not lie on the same line. On the right, however, we see that is an eigenvector. In fact, is obtained from by stretching by a factor of . Therefore, is an eigenvector of with eigenvalue .       On the left, the vector is not an eigenvector. On the right, the vector is an eigenvector with eigenvalue .   It is not difficult to see that any multiple of is also an eigenvector of with eigenvalue . Indeed, we will see later that all the eigenvectors associated to a given eigenvalue form a subspace of .  In , we see that is also an eigenvector with eigenvalue .      Here we see another eigenvector with eigenvalue .   The interactive diagram we used in the activity is meant to convey the fact that the eigenvectors of a matrix are special vectors. Most of the time, the vectors and appear visually unrelated. For certain vectors, however, and line up with one another. Something important is going on when that happens so we call attention to these vectors by calling them eigenvectors. For these vectors, the operation of multiplying by reduces to the much simpler operation of scalar multiplying by . The reason eigenvectors are important is because it is extremely convenient to be able to replace matrix multiplication by scalar multiplication.    The usefulness of eigenvalues and eigenvectors  In the next section, we will introduce an algebraic technique for finding the eigenvalues and eigenvectors of a matrix. Before doing that, however, we would like to discuss why eigenvalues and eigenvectors are so useful.  Let's continue looking at the example . We have seen that is an eigenvector with eigenvalue and is an eigenvector with eigenvalue . This means that and . By the linearity of matrix multiplication, we can determine what happens when we multiply a linear combination of and by : .   For instance, if we consider the vector , we find that as seen in the figure.    In other words, multiplying by has the effect of stretching a vector in the direction by a factor of and flipping in direction.  We can draw an analogy with the more familiar example of the diagonal matrix . As we have seen, the matrix transformation defined by combines a horizontal stretching by a factor of 3 with a reflection across the horizontal axis, as is illustrated in .      The diagonal matrix stretches vectors horizontally by a factor of and flips vectors vertically.   The matrix has a similar effect when viewed in the basis defined by the eigenvectors and , as seen in .      The matrix has the same geometric effect as the diagonal matrix when expressed in the coordinate system defined by the basis of eigenvectors.   In a sense that will be made precise later, having a set of eigenvectors of that forms a basis of enables us to think of as being equivalent to a diagonal matrix . Of course, as the other examples in the previous activity show, it may not always be possible to form a basis from the eigenvectors of a matrix. For example, the only eigenvectors of the matrix , which represents a shear, have the form . In this example, we are not able to create a basis for consisting of eigenvectors of the matrix. This is also true for the matrix , which represents a rotation.    Let's consider an example that illustrates how we can put these ideas to use.  Suppose that we work for a car rental company that has two locations, and . When a customer rents a car at one location, they have the option to return it to either location at the end of the day. After doing some market research, we determine:  80% of the cars rented at location are returned to and 20% are returned to .  40% of the cars rented at location are returned to and 60% are returned to .    Suppose that there are 1000 cars at location and no cars at location on Monday morning. How many cars are there are locations and at the end of the day on Monday?  How many are at locations and at end of the day on Tuesday?  If we let and be the number of cars at locations and , respectively, at the end of day , we then have We can write the vector to reflect the number of cars at the two locations at the end of day , which says that or where .  Suppose that . Compute and to demonstrate that and are eigenvectors of . What are the associated eigenvalues and ?  We said that 1000 cars are initially at location and none at location . This means that the initial vector describing the number of cars is . Write as a linear combination of and .  Remember that and are eigenvectors of . Use the linearity of matrix multiplication to write the vector , describing the number of cars at the two locations at the end of the first day, as a linear combination of and .  Write the vector as a linear combination of and . Then write the next few vectors as linear combinations of and :   .   .   .   .    What will happen to the number of cars at the two locations after a very long time? Explain how writing as a linear combination of eigenvectors helps you determine the long-term behavior.     The solution to this activity is given in the text below.    This activity is important and motivates much of our work with eigenvalues and eigenvectors so it's worth reviewing to make sure we have a clear understanding of the concepts.  First, we compute This shows that is an eigenvector of with eigenvalue and is an eigenvector of with eigenvalue .  By the linearity of matrix matrix multiplication, we have . Therefore, we will write the vector describing the initial distribution of cars as a linear combination of and ; that is, . To do, we form the augmented matrix and row reduce: . Therefore, .  To determine the distribution of cars on subsequent days, we will repeatedly multiply by . We find that .  In particular, this shows us that . Taking notice of the pattern, we may write . Multiplying a number by is the same as taking 20% of that number. As each day goes by, the second term is multiplied by so the coefficient of in the expression for will eventually become extremely small. We therefore see that the distribution of cars will stabilize at .  Notice how our understanding of the eigenvectors of the matrix allows us to replace matrix multiplication with the simpler operation of scalar multiplication. As a result, we can look far into the future without having to repeatedly perform matrix multiplication.  Furthermore, notice how this example relies on the fact that we can express the initial vector as a linear combination of eigenvectors. For this reason, we would like, when given an matrix, to be able to create a basis of that consists of its eigenvectors. We will frequently return to this question in later sections.    If is an matrix, can we form a basis of consisting of eigenvectors of ?      Summary  We defined an eigenvector of a square matrix to be a nonzero vector such that for some scalar , which is called the eigenvalue associated to .  If is an eigenvector, then matrix multiplication by reduces to the simpler operation of scalar multiplication by .  Scalar multiples of an eigenvector are also eigenvectors. In fact, we will see that the eigenvectors associated to an eigenvalue form a subspace.  If we can form a basis for consisting of eigenvectors of , then is, in some sense, equivalent to a diagonal matrix.  Rewriting a vector as a linear combination of eigenvectors of simplifies the process of repeatedly multiplying by .       Consider the matrix and vectors .  Show that and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of and .  Use this expression to compute , , and as a linear combination of eigenvectors.     We find that and so the associated eigenvalues are and .   .  We find      We find that and so the associated eigenvalues are and .  Setting up an augmented matrix and row reducing shows us that .  We then have      Consider the matrix and vectors   Show that the vectors , , and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of the eigenvectors.  Use this expression to compute , , and as a linear combination of eigenvectors.     We see that , , and . The associated eigenvalues are , , and .   .  We find      We see that , , and . The associated eigenvalues are , , and .  After forming an augmented matrix, we find .  We then have      Suppose that is an matrix.  Explain why is an eigenvalue of if and only if there is a nonzero solution to the homogeneous equation .  Explain why is not invertible if and only if is an eigenvalue.  If is an eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  If is invertible and is eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  The matrix has eigenvectors and and associated eigenvalues and . What are some eigenvectors and associated eigenvalues for ?      If is an eigenvalue, then there is a nonzero vector such that .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation .  If , we can multiply both sides by and to obtain .  Notice that , which means that is an eigenvector with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .     If is an eigenvalue, then there is a nonzero vector such that . This means that an associated eigenvector is a nonzero solution to the homogeneous equation .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation , which happens exactly when is not invertible.  If is an eigenvector of with associated eigenvalue , then . Therefore, , which means that is an eigenvector with associated eigenvalue .  If , we can multiply both sides by and to obtain . This shows that is an eigenvector of with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .     Suppose that is a matrix with eigenvectors and and eigenvalues and as shown in .   The vectors and are eigenvectors of .      Sketch the vectors , , and .                For the following matrices, find the eigenvectors and associated eigenvalues by thinking geometrically about the corresponding matrix transformation.   .   .  What are the eigenvectors and associated eigenvalues of the identity matrix?  What are the eigenvectors and associated eigenvalues of a diagonal matrix with distinct diagonal entries?     Every two-dimensional vector is an eigenvector with associated eigenvalue .  We have eigenvectors with associated eigenvalue and with .  Every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.     The corresponding matrix transformation stretches every two-dimensional vector by a factor of . Therefore, every two-dimensional vector is an eigenvector with associated eigenvalue .  The corresponding matrix transformation stretches vectors horizontally by a factor of and reflects them while stretching by a factor of vertically. We have eigenvectors with associated eigenvalue and with .  For any vector , we have . Therefore, every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.     Suppose that is a matrix having eigenvectors and associated eigenvalues and .   If , find the vector .    Find the vectors and .    What is the matrix ?          .     and .     .         We have . Therefore, .    We have and so     From the results of the previous part, we have .       Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a diagonal matrix are equal to the entries on the diagonal.  If , then as well.  Every vector is an eigenvector of the identity matrix.  If is an eigenvalue of , then is invertible.  For every matrix , it is possible to find a basis of consisting of eigenvectors of .     True  False  True  False  False     True. The associated eigenvectors are the standard basis vectors .  False. .  True, because .  False. If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation so is not invertible.  False. We saw the example , which represents a rotation and has no eigenvectors.     Suppose that is an matrix.   Assuming that is an eigenvector of whose associated eigenvector is nonzero, explain why is in .    Assuming that is an eigenvector of whose associated eigenvector is zero, explain why is in .    Consider the two special matrices below and find their eigenvectors and associated eigenvalues.      Because .  Because .  For the matrix , with associated eigenvalue , and and with associated eigenvalue .  For , with associated eigenvalue and and with associated eigenvalue .      In this case, we have or , which says that the equation is consistent.    With this assumption, , which means that is a solution to the homogeneous equation .    The column space of is spanned by and we notice that . Therefore, is an eigenvector with associated eigenvalue .  We also know that that the rank of this matrix is 1 so is two-dimensional. A basis for the null space is and so these vectors are eigenvectors with associated eigenvalue . In the same way, is an eigenvector of with associated eigenvalue and and are eigenvectors with associated eigenvalue .     For each of the following matrix transformations, describe the eigenvalues and eigenvectors of the corresponding matrix .  A reflection in in the line .  A rotation in .  A rotation in about the -axis.  A rotation in about the -axis.       with associated eigenvalue and with associated eigenvalue .  Every two-dimensional vector is an eigenvector with associated eigenvalue .   with associated eigenvalue . and with associated eigenvalue .   with associated eigenvalue .      A vector lying along the line of reflection is unchanged so , which shows that is an eigenvector with associated eigenvalue . At the same time, so is an eigenvector with associated eigenvalue .  Every vector satisfies so every two-dimensional vector is an eigenvector with associated eigenvalue .  Vectors along the -axis are unchanged so is an eigenvector with associated eigenvalue . Vectors in the -plane are multiplied by so and are eigenvectors with associated eigenvalue .  The vector is an eigenvector with associated eigenvalue . There are no other eigenvectors that are not scalar multiples of this one.      Suppose we have two species, and , where species preys on . Their populations, in millions, in year are denoted by and and satisfy . We will keep track of the populations in year using the vector so that .  Show that and are eigenvectors of and find their associated eigenvalues.  Suppose that the initial populations are described by the vector . Express as a linear combination of and .  Find the populations after one year, two years, and three years by writing the vectors , , and as linear combinations of and .  What is the general form for ?  After a very long time, what is the ratio of to ?      with associated eigenvalue and with associated eigenvalue .   .  We have   In general, .  The ratio of to is 1:3.     We can compute and . This means that is an eigenvector with associated eigenvalue and is an eigenvector with associated eigenvalue .  Setting up an augmented matrix and row reducing shows that .  We have   In general, .  After a long time, becomes large so that becomes very close to zero. This means that . So and . This means the ratio of to is 1:3.     "
},
{
  "id": "exploration-14",
  "level": "2",
  "url": "sec-eigen-intro.html#exploration-14",
  "type": "Preview Activity",
  "number": "4.1.1",
  "title": "",
  "body": "  Before we introduce the definition of eigenvectors and eigenvalues, it will be helpful to remember some ideas we have seen previously.    Suppose that is the vector shown in the figure. Sketch the vector and the vector .     State the geometric effect that scalar multiplication has on the vector . Then sketch all the vectors of the form where is a scalar.  State the geometric effect of the matrix transformation defined by .  Suppose that is a matrix and that and are vectors such that . Use the linearity of matrix multiplication to express the following vectors in terms of and .   .   .   .   .   .   .        The vectors are as shown.     Scalar multiplication has the effect of stretching and possibly flipping along the line defined by .  This matrix transformation stretches vectors by a factor of in the horizontal direction and flips vectors vertically.  Applying linearity, we see that   .   .   .   .   .   .      "
},
{
  "id": "definition-21",
  "level": "2",
  "url": "sec-eigen-intro.html#definition-21",
  "type": "Definition",
  "number": "4.1.1",
  "title": "",
  "body": "  eigenvalue  eigenvector  Given a square matrix , we say that a nonzero vector is an eigenvector of if there is a scalar such that . The scalar is called the eigenvalue associated to the eigenvector .   "
},
{
  "id": "example-30",
  "level": "2",
  "url": "sec-eigen-intro.html#example-30",
  "type": "Example",
  "number": "4.1.2",
  "title": "",
  "body": "  Consider the matrix and the vector . We find that . In other words, , which says that is an eigenvector of the matrix with associated eigenvalue .  Similarly, if , we find that . Here again, we have showing that is an eigenvector of with associated eigenvalue .   "
},
{
  "id": "activity-eigen-geom",
  "level": "2",
  "url": "sec-eigen-intro.html#activity-eigen-geom",
  "type": "Activity",
  "number": "4.1.2",
  "title": "",
  "body": "  This definition has an important geometric interpretation that we will investigate here.  Suppose that is a nonzero vector and that is a scalar. What is the geometric relationship between and ?  Let's now consider the eigenvector condition: . Here we have two vectors, and . If , what is the geometric relationship between and ?    A geometric interpretation of the eigenvalue-eigenvector condition .    Choose the matrix . Move the vector so that the eigenvector condition holds. What is the eigenvector and what is the associated eigenvalue?  By algebraically computing , verify that the eigenvector condition holds for the vector that you found.  If you multiply the eigenvector that you found by , do you still have an eigenvector? If so, what is the associated eigenvalue?  Are you able to find another eigenvector that is not a scalar multiple of the first one that you found? If so, what is the eigenvector and what is the associated eigenvalue?  Now consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues.  Finally, consider the matrix . Use the diagram to describe any eigenvectors and associated eigenvalues. What geometric transformation does this matrix perform on vectors? How does this explain the presence of any eigenvectors?       The vectors and lie on the same line.  The vectors and lie on the same line.  There are many possibilities, but we see that is an eigenvector with associated eigenvalue .  If we perform the matrix multiplication, we see that .  Yes, is still an eigenvector with associated eigenvalue .  We see that is an eigenvector with associated eigenvalue .  The only eigenvectors that appear are scalar multiples of with associated eigenvalue .  There are no eigenvectors. The matrix transformation rotates vectors by so it is not possible for and to lie on the same line.    "
},
{
  "id": "fig-eigen-intro",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro",
  "type": "Figure",
  "number": "4.1.4",
  "title": "",
  "body": "     On the left, the vector is not an eigenvector. On the right, the vector is an eigenvector with eigenvalue .  "
},
{
  "id": "fig-eigen-intro-2",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro-2",
  "type": "Figure",
  "number": "4.1.5",
  "title": "",
  "body": "    Here we see another eigenvector with eigenvalue .  "
},
{
  "id": "fig-eigen-intro-diagonal",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro-diagonal",
  "type": "Figure",
  "number": "4.1.6",
  "title": "",
  "body": "    The diagonal matrix stretches vectors horizontally by a factor of and flips vectors vertically.  "
},
{
  "id": "fig-eigen-intro-A",
  "level": "2",
  "url": "sec-eigen-intro.html#fig-eigen-intro-A",
  "type": "Figure",
  "number": "4.1.7",
  "title": "",
  "body": "    The matrix has the same geometric effect as the diagonal matrix when expressed in the coordinate system defined by the basis of eigenvectors.  "
},
{
  "id": "activity-eigen-intro",
  "level": "2",
  "url": "sec-eigen-intro.html#activity-eigen-intro",
  "type": "Activity",
  "number": "4.1.3",
  "title": "",
  "body": "  Let's consider an example that illustrates how we can put these ideas to use.  Suppose that we work for a car rental company that has two locations, and . When a customer rents a car at one location, they have the option to return it to either location at the end of the day. After doing some market research, we determine:  80% of the cars rented at location are returned to and 20% are returned to .  40% of the cars rented at location are returned to and 60% are returned to .    Suppose that there are 1000 cars at location and no cars at location on Monday morning. How many cars are there are locations and at the end of the day on Monday?  How many are at locations and at end of the day on Tuesday?  If we let and be the number of cars at locations and , respectively, at the end of day , we then have We can write the vector to reflect the number of cars at the two locations at the end of day , which says that or where .  Suppose that . Compute and to demonstrate that and are eigenvectors of . What are the associated eigenvalues and ?  We said that 1000 cars are initially at location and none at location . This means that the initial vector describing the number of cars is . Write as a linear combination of and .  Remember that and are eigenvectors of . Use the linearity of matrix multiplication to write the vector , describing the number of cars at the two locations at the end of the first day, as a linear combination of and .  Write the vector as a linear combination of and . Then write the next few vectors as linear combinations of and :   .   .   .   .    What will happen to the number of cars at the two locations after a very long time? Explain how writing as a linear combination of eigenvectors helps you determine the long-term behavior.     The solution to this activity is given in the text below.   "
},
{
  "id": "question-eigen-basis",
  "level": "2",
  "url": "sec-eigen-intro.html#question-eigen-basis",
  "type": "Question",
  "number": "4.1.8",
  "title": "",
  "body": "  If is an matrix, can we form a basis of consisting of eigenvectors of ?   "
},
{
  "id": "exercise-131",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-131",
  "type": "Exercise",
  "number": "4.1.4.1",
  "title": "",
  "body": " Consider the matrix and vectors .  Show that and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of and .  Use this expression to compute , , and as a linear combination of eigenvectors.     We find that and so the associated eigenvalues are and .   .  We find      We find that and so the associated eigenvalues are and .  Setting up an augmented matrix and row reducing shows us that .  We then have    "
},
{
  "id": "exercise-132",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-132",
  "type": "Exercise",
  "number": "4.1.4.2",
  "title": "",
  "body": " Consider the matrix and vectors   Show that the vectors , , and are eigenvectors of and find their associated eigenvalues.    Express the vector as a linear combination of the eigenvectors.  Use this expression to compute , , and as a linear combination of eigenvectors.     We see that , , and . The associated eigenvalues are , , and .   .  We find      We see that , , and . The associated eigenvalues are , , and .  After forming an augmented matrix, we find .  We then have    "
},
{
  "id": "exercise-133",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-133",
  "type": "Exercise",
  "number": "4.1.4.3",
  "title": "",
  "body": " Suppose that is an matrix.  Explain why is an eigenvalue of if and only if there is a nonzero solution to the homogeneous equation .  Explain why is not invertible if and only if is an eigenvalue.  If is an eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  If is invertible and is eigenvector of having associated eigenvalue , explain why is also an eigenvector of with associated eigenvalue .  The matrix has eigenvectors and and associated eigenvalues and . What are some eigenvectors and associated eigenvalues for ?      If is an eigenvalue, then there is a nonzero vector such that .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation .  If , we can multiply both sides by and to obtain .  Notice that , which means that is an eigenvector with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .     If is an eigenvalue, then there is a nonzero vector such that . This means that an associated eigenvector is a nonzero solution to the homogeneous equation .  If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation , which happens exactly when is not invertible.  If is an eigenvector of with associated eigenvalue , then . Therefore, , which means that is an eigenvector with associated eigenvalue .  If , we can multiply both sides by and to obtain . This shows that is an eigenvector of with associated eigenvalue .  The vectors and are eigenvectors of with associated eigenvalues and .   "
},
{
  "id": "exercise-134",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-134",
  "type": "Exercise",
  "number": "4.1.4.4",
  "title": "",
  "body": " Suppose that is a matrix with eigenvectors and and eigenvalues and as shown in .   The vectors and are eigenvectors of .      Sketch the vectors , , and .              "
},
{
  "id": "exercise-135",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-135",
  "type": "Exercise",
  "number": "4.1.4.5",
  "title": "",
  "body": " For the following matrices, find the eigenvectors and associated eigenvalues by thinking geometrically about the corresponding matrix transformation.   .   .  What are the eigenvectors and associated eigenvalues of the identity matrix?  What are the eigenvectors and associated eigenvalues of a diagonal matrix with distinct diagonal entries?     Every two-dimensional vector is an eigenvector with associated eigenvalue .  We have eigenvectors with associated eigenvalue and with .  Every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.     The corresponding matrix transformation stretches every two-dimensional vector by a factor of . Therefore, every two-dimensional vector is an eigenvector with associated eigenvalue .  The corresponding matrix transformation stretches vectors horizontally by a factor of and reflects them while stretching by a factor of vertically. We have eigenvectors with associated eigenvalue and with .  For any vector , we have . Therefore, every vector is an eigenvector of the identity matrix with associated eigenvalue .  The standard basis vectors are eigenvectors and the associated eigenvalues are the corresponding diagonal entries.   "
},
{
  "id": "exercise-136",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-136",
  "type": "Exercise",
  "number": "4.1.4.6",
  "title": "",
  "body": " Suppose that is a matrix having eigenvectors and associated eigenvalues and .   If , find the vector .    Find the vectors and .    What is the matrix ?          .     and .     .         We have . Therefore, .    We have and so     From the results of the previous part, we have .     "
},
{
  "id": "exercise-137",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-137",
  "type": "Exercise",
  "number": "4.1.4.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a diagonal matrix are equal to the entries on the diagonal.  If , then as well.  Every vector is an eigenvector of the identity matrix.  If is an eigenvalue of , then is invertible.  For every matrix , it is possible to find a basis of consisting of eigenvectors of .     True  False  True  False  False     True. The associated eigenvectors are the standard basis vectors .  False. .  True, because .  False. If is an eigenvalue of , then there is a nonzero solution to the homogeneous equation so is not invertible.  False. We saw the example , which represents a rotation and has no eigenvectors.   "
},
{
  "id": "exercise-138",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-138",
  "type": "Exercise",
  "number": "4.1.4.8",
  "title": "",
  "body": " Suppose that is an matrix.   Assuming that is an eigenvector of whose associated eigenvector is nonzero, explain why is in .    Assuming that is an eigenvector of whose associated eigenvector is zero, explain why is in .    Consider the two special matrices below and find their eigenvectors and associated eigenvalues.      Because .  Because .  For the matrix , with associated eigenvalue , and and with associated eigenvalue .  For , with associated eigenvalue and and with associated eigenvalue .      In this case, we have or , which says that the equation is consistent.    With this assumption, , which means that is a solution to the homogeneous equation .    The column space of is spanned by and we notice that . Therefore, is an eigenvector with associated eigenvalue .  We also know that that the rank of this matrix is 1 so is two-dimensional. A basis for the null space is and so these vectors are eigenvectors with associated eigenvalue . In the same way, is an eigenvector of with associated eigenvalue and and are eigenvectors with associated eigenvalue .   "
},
{
  "id": "exercise-139",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-139",
  "type": "Exercise",
  "number": "4.1.4.9",
  "title": "",
  "body": " For each of the following matrix transformations, describe the eigenvalues and eigenvectors of the corresponding matrix .  A reflection in in the line .  A rotation in .  A rotation in about the -axis.  A rotation in about the -axis.       with associated eigenvalue and with associated eigenvalue .  Every two-dimensional vector is an eigenvector with associated eigenvalue .   with associated eigenvalue . and with associated eigenvalue .   with associated eigenvalue .      A vector lying along the line of reflection is unchanged so , which shows that is an eigenvector with associated eigenvalue . At the same time, so is an eigenvector with associated eigenvalue .  Every vector satisfies so every two-dimensional vector is an eigenvector with associated eigenvalue .  Vectors along the -axis are unchanged so is an eigenvector with associated eigenvalue . Vectors in the -plane are multiplied by so and are eigenvectors with associated eigenvalue .  The vector is an eigenvector with associated eigenvalue . There are no other eigenvectors that are not scalar multiples of this one.    "
},
{
  "id": "exercise-140",
  "level": "2",
  "url": "sec-eigen-intro.html#exercise-140",
  "type": "Exercise",
  "number": "4.1.4.10",
  "title": "",
  "body": " Suppose we have two species, and , where species preys on . Their populations, in millions, in year are denoted by and and satisfy . We will keep track of the populations in year using the vector so that .  Show that and are eigenvectors of and find their associated eigenvalues.  Suppose that the initial populations are described by the vector . Express as a linear combination of and .  Find the populations after one year, two years, and three years by writing the vectors , , and as linear combinations of and .  What is the general form for ?  After a very long time, what is the ratio of to ?      with associated eigenvalue and with associated eigenvalue .   .  We have   In general, .  The ratio of to is 1:3.     We can compute and . This means that is an eigenvector with associated eigenvalue and is an eigenvector with associated eigenvalue .  Setting up an augmented matrix and row reducing shows that .  We have   In general, .  After a long time, becomes large so that becomes very close to zero. This means that . So and . This means the ratio of to is 1:3.   "
},
{
  "id": "sec-eigen-find",
  "level": "1",
  "url": "sec-eigen-find.html",
  "type": "Section",
  "number": "4.2",
  "title": "Finding eigenvalues and eigenvectors",
  "body": " Finding eigenvalues and eigenvectors   The last section introduced eigenvalues and eigenvectors, presented the underlying geometric intuition behind their definition, and demonstrated their use in understanding the long-term behavior of certain systems. We will now develop a more algebraic understanding of eigenvalues and eigenvectors. In particular, we will find an algebraic method for determining the eigenvalues and eigenvectors of a square matrix.    Let's begin by reviewing some important ideas that we have seen previously.  Suppose that is a square matrix and that the nonzero vector is a solution to the homogeneous equation . What can we conclude about the invertibility of ?  How does the determinant tell us if there is a nonzero solution to the homogeneous equation ?  Suppose that . Find the determinant . What does this tell us about the solution space to the homogeneous equation ?  Find a basis for .  What is the relationship between the rank of a matrix and the dimension of its null space?      The matrix cannot have a pivot position in every column so it is not invertible.  If there is a nonzero solution to the homogeneous equation , then is not invertible so .  We find that so there is a nonzero solution to the homogeneous equation.  The reduced row echelon form of is so the solution space to the homogeneous equation may be described parametrically as . A basis for is therefore .  If is an matrix, then .       The characteristic polynomial  We will first see that the eigenvalues of a square matrix appear as the roots of a particular polynomial. To begin, notice that we originally defined an eigenvector as a nonzero vector that satisfies the equation . We will rewrite this as In other words, an eigenvector is a solution of the homogeneous equation . This puts us in the familiar territory explored in the next activity.    The eigenvalues of a square matrix are defined by the condition that there be a nonzero solution to the homogeneous equation .  If there is a nonzero solution to the homogeneous equation , what can we conclude about the invertibility of the matrix ?  If there is a nonzero solution to the homogeneous equation , what can we conclude about the determinant ?  Let's consider the matrix from which we construct . Find the determinant . What kind of equation do you obtain when we set this determinant to zero to obtain ?  Use the determinant you found in the previous part to find the eigenvalues by solving the equation . We considered this matrix in so we should find the same eigenvalues for that we found by reasoning geometrically there.  Consider the matrix and find its eigenvalues by solving the equation .  Consider the matrix and find its eigenvalues by solving the equation .  Find the eigenvalues of the triangular matrix . What is generally true about the eigenvalues of a triangular matrix?         The matrix cannot be invertible.    It must be the case that .    We find that .     so we find eigenvalues and .    For this matrix, we have so there is one eigenvalue, .     so there are complex eigenvalues, and .    Because the determinant of a triangular matrix equals the product of its diagonal entries, The eigenvalues are equal to the entries on the diagonal.       This activity demonstrates a technique that enables us to find the eigenvalues of a square matrix . Since an eigenvalue is a scalar for which the equation has a nonzero solution, it must be the case that is not invertible. Therefore, its determinant is zero. This gives us the equation whose solutions are the eigenvalues of . This equation is called the characteristic equation of . characteristic equation    If we write the characteristic equation for the matrix , we see that This shows us that the eigenvalues are and .    characteristic polynomial In general, the expression is a polynomial in , which is called the characteristic polynomial of . If is an matrix, the degree of the characteristic polynomial is . For instance, if is a matrix, then is a quadratic polynomial; if is a matrix, then is a cubic polynomial.  The matrix in has a characteristic polynomial with two real and distinct roots. This will not always be the case, as demonstrated in the next two examples.   Consider the matrix , whose characteristic equation is In this case, the characteristic polynomial has one real root, which means that this matrix has a single real eigenvalue, .    To find the eigenvalues of a triangular matrix, we remember that the determinant of a triangular matrix is the product of the entries on the diagonal. For instance, the following triangular matrix has the characteristic equation showing that the eigenvalues are the diagonal entries .     Finding eigenvectors  Now that we can find the eigenvalues of a square matrix by solving the characteristic equation , we will turn to the question of finding the eigenvectors associated to an eigenvalue . The key, as before, is to note that an eigenvector is a nonzero solution to the homogeneous equation . In other words, the eigenvectors associated to an eigenvalue form the null space .  This shows that the eigenvectors associated to an eigenvalue form a subspace of . We will denote the subspace of eigenvectors of a matrix associated to the eigenvalue by and note that . We say that is the eigenspace of associated to the eigenvalue . eigenspace     In this activity, we will find the eigenvectors of a matrix as the null space of the matrix .  Let's begin with the matrix . We have seen that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  We also saw that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  Is it possible to form a basis of consisting of eigenvectors of ?  Now consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Next, consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Finally, find the eigenvalues and eigenvectors of the diagonal matrix . Explain your result by considering the geometric effect of the matrix transformation defined by .       We have The null space is one-dimensional with basis .  We have The null space is one-dimensional with basis .  We can form a basis for consisting of eigenvectors of by taking .  The characteristic equation is , which means that there is a single eigenvalue . This eigenspace is two-dimensional with basis . In this case, we can form a basis for consisting of eigenvectors of .  The characteristic equation is so there is again a single eigenvalue . In this case, the eigenspace is one-dimensional with basis vector . It is not possible to form a basis for consisting of eigenvectors.  We have eigenvectors with associated eigenvector and with associated eigenvector .     Once we find an eigenvalue of a matrix , describing the associated eigenspace amounts to the familiar task of describing the null space .   Revisiting the matrix from , we recall that we found eigenvalues and .  Considering the eigenvalue , we have Since the eigenvectors are the solutions of the equation , we see that they are determined by the single equation or . Therefore the eigenvectors in have the form In other words, is a one-dimensional subspace of with basis vector or basis vector . In the same way, we find that a basis for the eigenspace is .  We note that, for this matrix, it is possible to construct a basis of consisting of eigenvectors, namely,     Consider the matrix whose characteristic equation is   There is a single eigenvalue , and we find that Therefore, the eigenspace is one-dimensional with a basis vector .    If , then which implies that there is a single eigenvalue . We find that which says that every two-dimensional vector satisfies . Therefore, every vector is an eigenvector and so . This eigenspace is two-dimensional.  We can see this in another way. The matrix transformation defined by rotates vectors by , which says that for every vector . In other words, every two-dimensional vector is an eigenvector with associated eigenvalue .   These last two examples illustrate two types of behavior when there is a single eigenvalue. In one case, we are able to construct a basis of using eigenvectors; in the other, we are not. We will explore this behavior more in the next subsection.   A check on our work  When finding eigenvalues and their associated eigenvectors in this way, we first find eigenvalues by solving the characteristic equation. If is a solution to the characteristic equation, then is not invertible and, consequently, must contain a row without a pivot position.  This serves as a check on our work. If we row reduce and find the identity matrix, then we have made an error either in solving the characteristic equation or in finding .     The characteristic polynomial and the dimension of eigenspaces  Given a square matrix , we saw in the previous section the value of being able to express any vector in as a linear combination of eigenvectors of . For this reason, asks when we can construct a basis of consisting of eigenvectors. We will explore this question more fully now.  As we saw above, the eigenvalues of are the solutions of the characteristic equation . The examples we have considered demonstrate some different types of behavior. For instance, we have seen the characteristic equations    , which has real and distinct roots,     , which has repeated roots, and     , which has complex roots.     If is an matrix, then the characteristic polynomial is a degree polynomial, and this means that it has roots. Therefore, the characteristic equation can be written as giving eigenvalues . As we have seen, some of the eigenvalues may be complex. Moreover, some of the eigenvalues may appear in this list more than once. However, we can always write the characteristic equation in the form The number of times that appears as a factor in the characteristic polynomial, is called the multiplicity of the eigenvalue . multiplicity     We have seen that the matrix has the characteristic equation . This matrix has a single eigenvalue , which has multiplicity .      If a matrix has the characteristic equation , then that matrix has four eigenvalues: having multiplicity 2; having multiplicity 1; having multiplicity 7; and having multiplicity 2. The degree of the characteristic polynomial is the sum of the multiplicities so this matrix must be a matrix.    The multiplicities of the eigenvalues are important because they influence the dimension of the eigenspaces. We know that the dimension of an eigenspace must be at least one; the following proposition also tells us the dimension of an eigenspace can be no larger than the multiplicity of its associated eigenvalue.    If is a real eigenvalue of the matrix with multiplicity , then .      The diagonal matrix has the characteristic equation . There is a single eigenvalue having multiplicity , and we saw earlier that .      The matrix has the characteristic equation . This tells us that there is a single eigenvalue having multiplicity . In contrast with the previous example, we have .      We saw earlier that the matrix has the characteristic equation . There are three eigenvalues each having multiplicity . By the proposition, we are guaranteed that the dimension of each eigenspace is ; that is, . It turns out that this is enough to guarantee that there is a basis of consisting of eigenvectors.      If a matrix has the characteristic equation , we know there are four eigenvalues . Without more information, all we can say about the dimensions of the eigenspaces is We can guarantee that , but we cannot be more specific about the dimensions of the other eigenspaces.    Fortunately, if we have an matrix, it frequently happens that the characteristic equation has the form where there are distinct real eigenvalues, each of which has multiplicity . In this case, the dimension of each of the eigenspaces . With a little work, it can be seen that choosing a basis vector for each of the eigenspaces produces a basis for . We therefore have the following proposition.    If is an matrix having distinct real eigenvalues, then there is a basis of consisting of eigenvectors of .    This proposition provides one answer to our . The next activity explores this question further.      Identify the eigenvalues, and their multiplicities, of an matrix whose characteristic polynomial is . What can you conclude about the dimensions of the eigenspaces? What is the shape of the matrix? Do you have enough information to guarantee that there is a basis of consisting of eigenvectors?  Find the eigenvalues of and state their multiplicities. Can you find a basis of consisting of eigenvectors of this matrix?  Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Now consider the matrix whose characteristic equation is also .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?        There are three eigenvalues, has multiplicity , has multiplicity , and has multiplicity . We know that We can guarantee that , but we can say nothing further about the other two eigenspaces.  The dimension of the matrix is since the degree of the characteristic polynomial is . We cannot guarantee that we can form a basis for consisting of eigenvectors, however.  There is one eigenvalue having multiplicity two. Because the eigenspace is one-dimensional, however, we cannot find a basis for consisting of eigenvectors of .  For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is two-dimensional with basis . The eigenspace is one-dimensional with basis vector .  We are able to form a basis for consisting of eigenvectors of .    For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is one-dimensional with basis vector . The eigenspace is also one-dimensional with basis vector .  It is not possible to form a basis for consisting of eigenvectors of .    For this matrix,  There are three eigenvalues , , and , each having multiplicity .  A basis vector for the eigenspace is . A basis vector for the eigenspace is . A basis vector for the eigenspace is .  We can form a basis for consisting of eigenvectors of .         Using Sage to find eigenvalues and eigenvectors  We can use Sage to find the characteristic polynomial, eigenvalues, and eigenvectors of a matrix. As we will see, however, some care is required when dealing with matrices whose entries include decimals.    We will use Sage to find the eigenvalues and eigenvectors of a matrix. Let's begin with the matrix .  We can find the characteristic polynomial of by writing A.charpoly('lambda') . Notice that we have to give Sage a variable in which to write the polynomial; here, we use lambda though x works just as well.   The factored form of the characteristic polynomial may be more useful since it will tell us the eigenvalues and their multiplicities. The factored characteristic polynomial is found with A.fcp('lambda') .    If we only want the eigenvalues, we can use A.eigenvalues() .   Notice that the multiplicity of an eigenvalue is the number of times it is repeated in the list of eigenvalues.   Finally, we can find eigenvectors by A.eigenvectors_right() . (We are looking for right eigenvalues since the vector appears to the right of in the definition .)   At first glance, the result of this command can be a little confusing to interpret. What we see is a list with one entry for each eigenvalue. For each eigenvalue, there is a triple consisting of (i) the eigenvalue , (ii) a basis for , and (iii) the multiplicity of .   When working with decimal entries, which are called floating point numbers in computer science, we must remember that computers perform only approximate arithmetic. This is a problem when we wish to find the eigenvectors of such a matrix. To illustrate, consider the matrix .  Without using Sage, find the eigenvalues of this matrix.  What do you find for the reduced row echelon form of ?  Let's now use Sage to determine the reduced row echelon form of :   What result does Sage report for the reduced row echelon form? Why is this result not correct?   Because the arithmetic Sage performs with floating point entries is only approximate, we are not able to find the eigenspace . In this next chapter, we will learn how to address this issue. In the meantime, we can get around this problem by writing the entries in the matrix as rational numbers:          The fcp command will return the factored characteristic polynomial lambda^2 - 2*lambda - 3 .  The eigenvalues command returns a list of eigenvalues [-3, -3] .  The eigenvectors_right command returns [(-3, [(1, 0)], 2)] .  If we begin with the matrix , we find  The eigenvalues are and .  The reduced row echelon form is , which shows that is not invertible, as expected.  Sage returns , which is not correct because cannot be invertible if is an eigenvalue of .  Here we find the correct eigenvalues, with basis vector for and with basis vector for .         Summary  In this section, we developed a technique for finding the eigenvalues and eigenvectors of an matrix .  The expression is a degree polynomial, known as the characteristic polynomial of . The eigenvalues of are the roots of the characteristic polynomial found by solving the characteristic equation .  The set of eigenvectors associated to the eigenvalue forms a subspace of , the eigenspace .  If the factor appears times in the characteristic polynomial, we say that the eigenvalue has multiplicity and note that .  If each of the eigenvalues is real and has multiplicity , then we can form a basis of consisting of eigenvectors of .  We can use Sage to find the eigenvalues and eigenvalues of matrices. However, we need to be careful working with floating point numbers since floating point arithmetic is only an approximation.      For each of the following matrices, find its characteristic polynomial, its eigenvalues, and the multiplicity of each eigenvalue.   .   .   .   .      . There is a single eigenvalue having multiplicity .   . There are three eigenvalues , each of multiplicity .   . There is one eigenvalue having multiplicity .   . There are two eigenvalues and , each having multiplicity .     The characteristic polynomial is . There is a single eigenvalue having multiplicity .  The characteristic polynomial is . There are three eigenvalues , each of multiplicity .  The characteristic polynomial is , showing that there is one eigenvalue having multiplicity .  The characteristic polynomial is . There are two eigenvalues and , each having multiplicity .     Given an matrix , an important question, , asks whether we can find a basis of consisting of eigenvectors of . For each of the matrices in the previous exercise, find a basis of consisting of eigenvectors or state why such a basis does not exist.    It is not possible.   .            There is a single eigenvalue and so it is not possible to find a basis for consisting of eigenvectors of .  The three eigenvalues each have multiplicity one so we know that their eigenspaces are one-dimensional. We find a basis vector for is , for is , and for is . A basis for consisting of eigenvectors of is therefore   Since is the zero matrix, every vector is an eigenvector of . This means that is a basis of consisting of eigenvectors.  The multiplicity of each eigenvalue is one so there will be a basis of consisting of eigenvectors. In particular, a basis vector for is and a basis vector for is . This means that is a basis for consisting of eigenvectors of .      Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a matrix are the entries on the diagonal of .  If is an eigenvalue of multiplicity , then is one-dimensional.  If a matrix is invertible, then cannot be an eigenvalue.  If is a matrix, the characteristic polynomial has degree less than .  The eigenspace of is the same as the null space .     False  True  True  False  True     False. This is true for a diagonal matrix, but it is not generally true as we see by considering the matrix whose eigenvalues are and .  True. If is the multiplicity, we have so we must have .  True. If is an eigenvalue, then an associated eigenvector is a nonzero solution to the homogeneous equation . This would say that is not invertible.  False. The degree of the characteristic polynomial equals the number of rows and columns of the square matrix.  True. An eigenvector associated to the eigenvalue satisfies . This is the same equation that characterizes the null space .     Provide a justification for your response to the following questions.  Suppose that is a matrix having eigenvalues . What are the eigenvalues of ?  Suppose that is a diagonal matrix. Why can you guarantee that there is a basis of consisting of eigenvectors of ?  If is a matrix whose eigenvalues are , can you guarantee that there is a basis of consisting of eigenvectors of ?  Suppose that the characteristic polynomial of a matrix is . What are the eigenvalues of ? Is invertible? Is there a basis of consisting of eigenvectors of ?  If the characteristic polynomial of is , what is the characteristic polynomial of ? what is the characteristic polynomial of ?      .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes.   . The matrix is not invertible, but there is a basis for consisting of eigenvectors of .   .     If , then . This says that the eigenvalues of are .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes. Since there are three distinct eigenvalues, the multiplicity of each eigenvalue must be one. Therefore, the dimension of each eigenspace is one. If we choose a basis vector for each of the eigenspaces, we will obtain a basis for .  The eigenvalues are determined by , which shows that the eigenvalues are . This shows that is not invertible since is an eigenvalue. There is, however, a basis for consisting of eigenvectors of since the three eigenvalues are distinct.  If is an eigenvalue of , then is an eigenvalue of . Therefore, the characteristic polynomial of is .     For each of the following matrices, use Sage to determine its eigenvalues, their multiplicities, and a basis for each eigenspace. For which matrices is it possible to construct a basis for consisting of eigenvectors?                 A basis of eigenvectors is   It is not possible to find a basis of eigenvectors.  A basis of eigenvectors is      Sage tells us there are three distinct eigenvalues , each having multiplicity one. A basis of eigenvectors is   It is not possible to find a basis of eigenvectors because the eigenvalue has multiplicity two but its eigenspace is one-dimensional.  It is possible to find a basis for consisting of eignevectors because one eigenvalue has multiplicity one while the other has multiplicity two and a two-dimensional eigenspace. A basis is      There is a relationship between the determinant of a matrix and the product of its eigenvalues.  We have seen that the eigenvalues of the matrix are . What is ? What is the product of the eigenvalues of ?  Consider the triangular matrix . What are the eigenvalues of ? What is ? What is the product of the eigenvalues of ?  Based on these examples, what do you think is the relationship between the determinant of a matrix and the product of its eigenvalues?  Suppose the characteristic polynomial is written as . By substituting into this equation, explain why the determinant of a matrix equals the product of its eigenvalues.     Both equal .  Both equal .   equals the product of the eigenvalues.  We see that .     We have , and the product of the eigenvalues is .  We have , and the product of the eigenvalues is   We suspect that equals the product of the eigenvalues.  Setting , we have Notice that we mean the product of the eigenvalues, including their multiplicities.     Consider the matrix .  Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We find the eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find that .  Since , we have .  We have . As grows larger, becomes less significant. Eventually, .     Consider the matrix   Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We have eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find .  Since , we have   We have . As grows larger, becomes less significant. Eventually, .     "
},
{
  "id": "exploration-15",
  "level": "2",
  "url": "sec-eigen-find.html#exploration-15",
  "type": "Preview Activity",
  "number": "4.2.1",
  "title": "",
  "body": "  Let's begin by reviewing some important ideas that we have seen previously.  Suppose that is a square matrix and that the nonzero vector is a solution to the homogeneous equation . What can we conclude about the invertibility of ?  How does the determinant tell us if there is a nonzero solution to the homogeneous equation ?  Suppose that . Find the determinant . What does this tell us about the solution space to the homogeneous equation ?  Find a basis for .  What is the relationship between the rank of a matrix and the dimension of its null space?      The matrix cannot have a pivot position in every column so it is not invertible.  If there is a nonzero solution to the homogeneous equation , then is not invertible so .  We find that so there is a nonzero solution to the homogeneous equation.  The reduced row echelon form of is so the solution space to the homogeneous equation may be described parametrically as . A basis for is therefore .  If is an matrix, then .    "
},
{
  "id": "activity-44",
  "level": "2",
  "url": "sec-eigen-find.html#activity-44",
  "type": "Activity",
  "number": "4.2.2",
  "title": "",
  "body": "  The eigenvalues of a square matrix are defined by the condition that there be a nonzero solution to the homogeneous equation .  If there is a nonzero solution to the homogeneous equation , what can we conclude about the invertibility of the matrix ?  If there is a nonzero solution to the homogeneous equation , what can we conclude about the determinant ?  Let's consider the matrix from which we construct . Find the determinant . What kind of equation do you obtain when we set this determinant to zero to obtain ?  Use the determinant you found in the previous part to find the eigenvalues by solving the equation . We considered this matrix in so we should find the same eigenvalues for that we found by reasoning geometrically there.  Consider the matrix and find its eigenvalues by solving the equation .  Consider the matrix and find its eigenvalues by solving the equation .  Find the eigenvalues of the triangular matrix . What is generally true about the eigenvalues of a triangular matrix?         The matrix cannot be invertible.    It must be the case that .    We find that .     so we find eigenvalues and .    For this matrix, we have so there is one eigenvalue, .     so there are complex eigenvalues, and .    Because the determinant of a triangular matrix equals the product of its diagonal entries, The eigenvalues are equal to the entries on the diagonal.      "
},
{
  "id": "example-eigenvalues-poly",
  "level": "2",
  "url": "sec-eigen-find.html#example-eigenvalues-poly",
  "type": "Example",
  "number": "4.2.1",
  "title": "",
  "body": " If we write the characteristic equation for the matrix , we see that This shows us that the eigenvalues are and .  "
},
{
  "id": "example-32",
  "level": "2",
  "url": "sec-eigen-find.html#example-32",
  "type": "Example",
  "number": "4.2.2",
  "title": "",
  "body": " Consider the matrix , whose characteristic equation is In this case, the characteristic polynomial has one real root, which means that this matrix has a single real eigenvalue, .  "
},
{
  "id": "example-33",
  "level": "2",
  "url": "sec-eigen-find.html#example-33",
  "type": "Example",
  "number": "4.2.3",
  "title": "",
  "body": " To find the eigenvalues of a triangular matrix, we remember that the determinant of a triangular matrix is the product of the entries on the diagonal. For instance, the following triangular matrix has the characteristic equation showing that the eigenvalues are the diagonal entries .  "
},
{
  "id": "activity-45",
  "level": "2",
  "url": "sec-eigen-find.html#activity-45",
  "type": "Activity",
  "number": "4.2.3",
  "title": "",
  "body": "  In this activity, we will find the eigenvectors of a matrix as the null space of the matrix .  Let's begin with the matrix . We have seen that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  We also saw that is an eigenvalue. Form the matrix and find a basis for the eigenspace . What is the dimension of this eigenspace? For each of the basis vectors , verify that .  Is it possible to form a basis of consisting of eigenvectors of ?  Now consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Next, consider the matrix . Write the characteristic equation for and use it to find the eigenvalues of . For each eigenvalue, find a basis for its eigenspace . Is it possible to form a basis of consisting of eigenvectors of ?  Finally, find the eigenvalues and eigenvectors of the diagonal matrix . Explain your result by considering the geometric effect of the matrix transformation defined by .       We have The null space is one-dimensional with basis .  We have The null space is one-dimensional with basis .  We can form a basis for consisting of eigenvectors of by taking .  The characteristic equation is , which means that there is a single eigenvalue . This eigenspace is two-dimensional with basis . In this case, we can form a basis for consisting of eigenvectors of .  The characteristic equation is so there is again a single eigenvalue . In this case, the eigenspace is one-dimensional with basis vector . It is not possible to form a basis for consisting of eigenvectors.  We have eigenvectors with associated eigenvector and with associated eigenvector .    "
},
{
  "id": "example-34",
  "level": "2",
  "url": "sec-eigen-find.html#example-34",
  "type": "Example",
  "number": "4.2.4",
  "title": "",
  "body": " Revisiting the matrix from , we recall that we found eigenvalues and .  Considering the eigenvalue , we have Since the eigenvectors are the solutions of the equation , we see that they are determined by the single equation or . Therefore the eigenvectors in have the form In other words, is a one-dimensional subspace of with basis vector or basis vector . In the same way, we find that a basis for the eigenspace is .  We note that, for this matrix, it is possible to construct a basis of consisting of eigenvectors, namely,   "
},
{
  "id": "example-35",
  "level": "2",
  "url": "sec-eigen-find.html#example-35",
  "type": "Example",
  "number": "4.2.5",
  "title": "",
  "body": " Consider the matrix whose characteristic equation is   There is a single eigenvalue , and we find that Therefore, the eigenspace is one-dimensional with a basis vector .  "
},
{
  "id": "example-36",
  "level": "2",
  "url": "sec-eigen-find.html#example-36",
  "type": "Example",
  "number": "4.2.6",
  "title": "",
  "body": " If , then which implies that there is a single eigenvalue . We find that which says that every two-dimensional vector satisfies . Therefore, every vector is an eigenvector and so . This eigenspace is two-dimensional.  We can see this in another way. The matrix transformation defined by rotates vectors by , which says that for every vector . In other words, every two-dimensional vector is an eigenvector with associated eigenvalue .  "
},
{
  "id": "example-37",
  "level": "2",
  "url": "sec-eigen-find.html#example-37",
  "type": "Example",
  "number": "4.2.7",
  "title": "",
  "body": "  We have seen that the matrix has the characteristic equation . This matrix has a single eigenvalue , which has multiplicity .   "
},
{
  "id": "example-38",
  "level": "2",
  "url": "sec-eigen-find.html#example-38",
  "type": "Example",
  "number": "4.2.8",
  "title": "",
  "body": "  If a matrix has the characteristic equation , then that matrix has four eigenvalues: having multiplicity 2; having multiplicity 1; having multiplicity 7; and having multiplicity 2. The degree of the characteristic polynomial is the sum of the multiplicities so this matrix must be a matrix.   "
},
{
  "id": "prop-eigen-basis",
  "level": "2",
  "url": "sec-eigen-find.html#prop-eigen-basis",
  "type": "Proposition",
  "number": "4.2.9",
  "title": "",
  "body": "  If is a real eigenvalue of the matrix with multiplicity , then .   "
},
{
  "id": "example-39",
  "level": "2",
  "url": "sec-eigen-find.html#example-39",
  "type": "Example",
  "number": "4.2.10",
  "title": "",
  "body": "  The diagonal matrix has the characteristic equation . There is a single eigenvalue having multiplicity , and we saw earlier that .   "
},
{
  "id": "example-40",
  "level": "2",
  "url": "sec-eigen-find.html#example-40",
  "type": "Example",
  "number": "4.2.11",
  "title": "",
  "body": "  The matrix has the characteristic equation . This tells us that there is a single eigenvalue having multiplicity . In contrast with the previous example, we have .   "
},
{
  "id": "example-41",
  "level": "2",
  "url": "sec-eigen-find.html#example-41",
  "type": "Example",
  "number": "4.2.12",
  "title": "",
  "body": "  We saw earlier that the matrix has the characteristic equation . There are three eigenvalues each having multiplicity . By the proposition, we are guaranteed that the dimension of each eigenspace is ; that is, . It turns out that this is enough to guarantee that there is a basis of consisting of eigenvectors.   "
},
{
  "id": "example-42",
  "level": "2",
  "url": "sec-eigen-find.html#example-42",
  "type": "Example",
  "number": "4.2.13",
  "title": "",
  "body": "  If a matrix has the characteristic equation , we know there are four eigenvalues . Without more information, all we can say about the dimensions of the eigenspaces is We can guarantee that , but we cannot be more specific about the dimensions of the other eigenspaces.   "
},
{
  "id": "proposition-30",
  "level": "2",
  "url": "sec-eigen-find.html#proposition-30",
  "type": "Proposition",
  "number": "4.2.14",
  "title": "",
  "body": "  If is an matrix having distinct real eigenvalues, then there is a basis of consisting of eigenvectors of .   "
},
{
  "id": "activity-46",
  "level": "2",
  "url": "sec-eigen-find.html#activity-46",
  "type": "Activity",
  "number": "4.2.4",
  "title": "",
  "body": "    Identify the eigenvalues, and their multiplicities, of an matrix whose characteristic polynomial is . What can you conclude about the dimensions of the eigenspaces? What is the shape of the matrix? Do you have enough information to guarantee that there is a basis of consisting of eigenvectors?  Find the eigenvalues of and state their multiplicities. Can you find a basis of consisting of eigenvectors of this matrix?  Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Now consider the matrix whose characteristic equation is also .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?   Consider the matrix whose characteristic equation is .  Identify the eigenvalues and their multiplicities.  For each eigenvalue , find a basis of the eigenspace and state its dimension.  Is there a basis of consisting of eigenvectors of ?        There are three eigenvalues, has multiplicity , has multiplicity , and has multiplicity . We know that We can guarantee that , but we can say nothing further about the other two eigenspaces.  The dimension of the matrix is since the degree of the characteristic polynomial is . We cannot guarantee that we can form a basis for consisting of eigenvectors, however.  There is one eigenvalue having multiplicity two. Because the eigenspace is one-dimensional, however, we cannot find a basis for consisting of eigenvectors of .  For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is two-dimensional with basis . The eigenspace is one-dimensional with basis vector .  We are able to form a basis for consisting of eigenvectors of .    For the matrix ,  We have eigenvalues with multiplicity and with multiplicity .  The eigenspace is one-dimensional with basis vector . The eigenspace is also one-dimensional with basis vector .  It is not possible to form a basis for consisting of eigenvectors of .    For this matrix,  There are three eigenvalues , , and , each having multiplicity .  A basis vector for the eigenspace is . A basis vector for the eigenspace is . A basis vector for the eigenspace is .  We can form a basis for consisting of eigenvectors of .      "
},
{
  "id": "activity-47",
  "level": "2",
  "url": "sec-eigen-find.html#activity-47",
  "type": "Activity",
  "number": "4.2.5",
  "title": "",
  "body": "  We will use Sage to find the eigenvalues and eigenvectors of a matrix. Let's begin with the matrix .  We can find the characteristic polynomial of by writing A.charpoly('lambda') . Notice that we have to give Sage a variable in which to write the polynomial; here, we use lambda though x works just as well.   The factored form of the characteristic polynomial may be more useful since it will tell us the eigenvalues and their multiplicities. The factored characteristic polynomial is found with A.fcp('lambda') .    If we only want the eigenvalues, we can use A.eigenvalues() .   Notice that the multiplicity of an eigenvalue is the number of times it is repeated in the list of eigenvalues.   Finally, we can find eigenvectors by A.eigenvectors_right() . (We are looking for right eigenvalues since the vector appears to the right of in the definition .)   At first glance, the result of this command can be a little confusing to interpret. What we see is a list with one entry for each eigenvalue. For each eigenvalue, there is a triple consisting of (i) the eigenvalue , (ii) a basis for , and (iii) the multiplicity of .   When working with decimal entries, which are called floating point numbers in computer science, we must remember that computers perform only approximate arithmetic. This is a problem when we wish to find the eigenvectors of such a matrix. To illustrate, consider the matrix .  Without using Sage, find the eigenvalues of this matrix.  What do you find for the reduced row echelon form of ?  Let's now use Sage to determine the reduced row echelon form of :   What result does Sage report for the reduced row echelon form? Why is this result not correct?   Because the arithmetic Sage performs with floating point entries is only approximate, we are not able to find the eigenspace . In this next chapter, we will learn how to address this issue. In the meantime, we can get around this problem by writing the entries in the matrix as rational numbers:          The fcp command will return the factored characteristic polynomial lambda^2 - 2*lambda - 3 .  The eigenvalues command returns a list of eigenvalues [-3, -3] .  The eigenvectors_right command returns [(-3, [(1, 0)], 2)] .  If we begin with the matrix , we find  The eigenvalues are and .  The reduced row echelon form is , which shows that is not invertible, as expected.  Sage returns , which is not correct because cannot be invertible if is an eigenvalue of .  Here we find the correct eigenvalues, with basis vector for and with basis vector for .      "
},
{
  "id": "exercise-141",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-141",
  "type": "Exercise",
  "number": "4.2.6.1",
  "title": "",
  "body": " For each of the following matrices, find its characteristic polynomial, its eigenvalues, and the multiplicity of each eigenvalue.   .   .   .   .      . There is a single eigenvalue having multiplicity .   . There are three eigenvalues , each of multiplicity .   . There is one eigenvalue having multiplicity .   . There are two eigenvalues and , each having multiplicity .     The characteristic polynomial is . There is a single eigenvalue having multiplicity .  The characteristic polynomial is . There are three eigenvalues , each of multiplicity .  The characteristic polynomial is , showing that there is one eigenvalue having multiplicity .  The characteristic polynomial is . There are two eigenvalues and , each having multiplicity .   "
},
{
  "id": "exercise-142",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-142",
  "type": "Exercise",
  "number": "4.2.6.2",
  "title": "",
  "body": " Given an matrix , an important question, , asks whether we can find a basis of consisting of eigenvectors of . For each of the matrices in the previous exercise, find a basis of consisting of eigenvectors or state why such a basis does not exist.    It is not possible.   .            There is a single eigenvalue and so it is not possible to find a basis for consisting of eigenvectors of .  The three eigenvalues each have multiplicity one so we know that their eigenspaces are one-dimensional. We find a basis vector for is , for is , and for is . A basis for consisting of eigenvectors of is therefore   Since is the zero matrix, every vector is an eigenvector of . This means that is a basis of consisting of eigenvectors.  The multiplicity of each eigenvalue is one so there will be a basis of consisting of eigenvectors. In particular, a basis vector for is and a basis vector for is . This means that is a basis for consisting of eigenvectors of .    "
},
{
  "id": "exercise-143",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-143",
  "type": "Exercise",
  "number": "4.2.6.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  The eigenvalues of a matrix are the entries on the diagonal of .  If is an eigenvalue of multiplicity , then is one-dimensional.  If a matrix is invertible, then cannot be an eigenvalue.  If is a matrix, the characteristic polynomial has degree less than .  The eigenspace of is the same as the null space .     False  True  True  False  True     False. This is true for a diagonal matrix, but it is not generally true as we see by considering the matrix whose eigenvalues are and .  True. If is the multiplicity, we have so we must have .  True. If is an eigenvalue, then an associated eigenvector is a nonzero solution to the homogeneous equation . This would say that is not invertible.  False. The degree of the characteristic polynomial equals the number of rows and columns of the square matrix.  True. An eigenvector associated to the eigenvalue satisfies . This is the same equation that characterizes the null space .   "
},
{
  "id": "exercise-144",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-144",
  "type": "Exercise",
  "number": "4.2.6.4",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  Suppose that is a matrix having eigenvalues . What are the eigenvalues of ?  Suppose that is a diagonal matrix. Why can you guarantee that there is a basis of consisting of eigenvectors of ?  If is a matrix whose eigenvalues are , can you guarantee that there is a basis of consisting of eigenvectors of ?  Suppose that the characteristic polynomial of a matrix is . What are the eigenvalues of ? Is invertible? Is there a basis of consisting of eigenvectors of ?  If the characteristic polynomial of is , what is the characteristic polynomial of ? what is the characteristic polynomial of ?      .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes.   . The matrix is not invertible, but there is a basis for consisting of eigenvectors of .   .     If , then . This says that the eigenvalues of are .  The standard basis vectors are eigenvectors associated to the diagonal entries.  Yes. Since there are three distinct eigenvalues, the multiplicity of each eigenvalue must be one. Therefore, the dimension of each eigenspace is one. If we choose a basis vector for each of the eigenspaces, we will obtain a basis for .  The eigenvalues are determined by , which shows that the eigenvalues are . This shows that is not invertible since is an eigenvalue. There is, however, a basis for consisting of eigenvectors of since the three eigenvalues are distinct.  If is an eigenvalue of , then is an eigenvalue of . Therefore, the characteristic polynomial of is .   "
},
{
  "id": "exercise-145",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-145",
  "type": "Exercise",
  "number": "4.2.6.5",
  "title": "",
  "body": " For each of the following matrices, use Sage to determine its eigenvalues, their multiplicities, and a basis for each eigenspace. For which matrices is it possible to construct a basis for consisting of eigenvectors?                 A basis of eigenvectors is   It is not possible to find a basis of eigenvectors.  A basis of eigenvectors is      Sage tells us there are three distinct eigenvalues , each having multiplicity one. A basis of eigenvectors is   It is not possible to find a basis of eigenvectors because the eigenvalue has multiplicity two but its eigenspace is one-dimensional.  It is possible to find a basis for consisting of eignevectors because one eigenvalue has multiplicity one while the other has multiplicity two and a two-dimensional eigenspace. A basis is    "
},
{
  "id": "exercise-146",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-146",
  "type": "Exercise",
  "number": "4.2.6.6",
  "title": "",
  "body": " There is a relationship between the determinant of a matrix and the product of its eigenvalues.  We have seen that the eigenvalues of the matrix are . What is ? What is the product of the eigenvalues of ?  Consider the triangular matrix . What are the eigenvalues of ? What is ? What is the product of the eigenvalues of ?  Based on these examples, what do you think is the relationship between the determinant of a matrix and the product of its eigenvalues?  Suppose the characteristic polynomial is written as . By substituting into this equation, explain why the determinant of a matrix equals the product of its eigenvalues.     Both equal .  Both equal .   equals the product of the eigenvalues.  We see that .     We have , and the product of the eigenvalues is .  We have , and the product of the eigenvalues is   We suspect that equals the product of the eigenvalues.  Setting , we have Notice that we mean the product of the eigenvalues, including their multiplicities.   "
},
{
  "id": "exercise-147",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-147",
  "type": "Exercise",
  "number": "4.2.6.7",
  "title": "",
  "body": " Consider the matrix .  Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We find the eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find that .  Since , we have .  We have . As grows larger, becomes less significant. Eventually, .   "
},
{
  "id": "exercise-148",
  "level": "2",
  "url": "sec-eigen-find.html#exercise-148",
  "type": "Exercise",
  "number": "4.2.6.8",
  "title": "",
  "body": " Consider the matrix   Find the eigenvalues of and a basis for their associated eigenspaces.  Suppose that . Express as a linear combination of eigenvectors of .  Define the vectors . Write , , and as a linear combination of eigenvectors of .  What happens to as grows larger and larger?      with associated eigenvector and with associated eigenvector .   .  We have .   .     We have eigenvalues with associated eigenvector and with associated eigenvector .  After constructing an augmented matrix, we find .  Since , we have   We have . As grows larger, becomes less significant. Eventually, .   "
},
{
  "id": "sec-eigen-diag",
  "level": "1",
  "url": "sec-eigen-diag.html",
  "type": "Section",
  "number": "4.3",
  "title": "Diagonalization, similarity, and powers of a matrix",
  "body": " Diagonalization, similarity, and powers of a matrix   The first example we considered in this chapter was the matrix , which has eigenvectors and and associated eigenvalues and . In , we described how is, in some sense, equivalent to the diagonal matrix .  This equivalence is summarized by . The diagonal matrix has the geometric effect of stretching vectors horizontally by a factor of and flipping vectors vertically. The matrix has the geometric effect of stretching vectors by a factor of in the direction and flipping them in the direction. That is, the geometric effect of is the same as that of when viewed in a basis of eigenvectors of .      The matrix has the same geometric effect as the diagonal matrix when viewed in the basis of eigenvectors.   Our goal in this section is to express this geometric observation in algebraic terms. In doing so, we will make precise the sense in which and are equivalent.    In this preview activity, we will review some familiar properties about matrix multiplication that appear in this section.   Remember that matrix-vector multiplication constructs linear combinations of the columns of the matrix. For instance, if , express the product in terms of and .    What is the product in terms of and ?    Next, remember how matrix-matrix multiplication is defined. Suppose that we have matrices and and that . How can we express the matrix product in terms of the columns of ?    Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Express the product in terms of and .    Suppose that is the matrix from the previous part and that . What is the matrix product          .     .     .     .     .        Diagonalization of matrices  When working with an matrix , demonstrated the value of having a basis of consisting of eigenvectors of . In fact, tells us that if the eigenvalues of are real and distinct, then there is a such a basis. As we'll see later, there are other conditions on that guarantee a basis of eigenvectors. For now, suffice it to say that we can find a basis of eigenvectors for many matrices. With this assumption, we will see how the matrix is equivalent to a diagonal matrix .    Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Because the eigenvalues are real and distinct, we know by that these eigenvectors form a basis of .   What are the products and in terms of and ?    If we form the matrix , what is the product in terms of and ?    Use the eigenvalues to form the diagonal matrix and determine the product in terms of and .    The results from the previous two parts of this activity demonstrate that . Using the fact that the eigenvectors and form a basis of , explain why is invertible and that we must have .    Suppose that . Verify that and are eigenvectors of with eigenvalues and .    Use the Sage cell below to define the matrices and and then verify that .        We have and .    .     . Comparing the result of this part of the activity to the previous, we see that .    Since the eigenvectors form a basis, the columns of are linearly independent and their span is . This guarantees that is invertible. Multiplying the equation on the right by gives .    The rest of the activity can be verified using Sage.    More generally, suppose that we have an matrix and that there is a basis of consisting of eigenvectors of with associated eigenvalues . If we use the eigenvectors to form the matrix and the eigenvalues to form the diagonal matrix and apply the same reasoning demonstrated in the activity, we find that and hence   We have now seen the following proposition.    If is an matrix and there is a basis of consisting of eigenvectors of having associated eigenvalues , then we can write where is the diagonal matrix whose diagonal entries are the eigenvalues of  and the matrix .     We have seen that has eigenvectors and with associated eigenvalues and . Forming the matrices we see that .  This is the sense in which we mean that is equivalent to a diagonal matrix . The expression says that , expressed in the basis defined by the columns of , has the same geometric effect as , expressed in the standard basis .     diagonalizable  We say that the matrix is diagonalizable if there is a diagonal matrix and invertible matrix such that       We will try to find a diagonalization of whose characteristic equation is . This shows that the eigenvalues of are and .  By constructing , we find a basis for consisting of the vector . Similarly, a basis for consists of the vector . This shows that we can construct a basis of consisting of eigenvectors of .  We now form the matrices and verify that .  There are, in fact, many ways to diagonalize . For instance, we could change the order of the eigenvalues and eigenvectors and write .  If we choose a different basis for the eigenspaces, we will also find a different matrix that diagonalizes . The point is that there are many ways in which can be written in the form .      We will try to find a diagonalization of .  Once again, we find the eigenvalues by solving the characteristic equation: . In this case, there is a single eigenvalue .  We find a basis for the eigenspace by describing : . This shows that the eigenspace is one-dimensional with forming a basis.  In this case, there is not a basis of consisting of eigenvectors of , which tells us that is not diagonalizable.    In fact, if we only know that , we can say that the columns of are eigenvectors of and that the diagonal entries of are the associated eigenvalues.    An matrix is diagonalizable if and only if there is a basis of consisting of eigenvectors of .      Suppose we know that where The columns of form eigenvectors of so that is an eigenvector of with eigenvalue and is an eigenvector with eigenvalue .  We can verify this by computing and checking that and .        Find a diagonalization of , if one exists, when .  Can the diagonal matrix be diagonalized? If so, explain how to find the matrices and .  Find a diagonalization of , if one exists, when .    Find a diagonalization of , if one exists, when .    Suppose that where .  Explain why is invertible.  Find a diagonalization of .  Find a diagonalization of .        We find that has eigenvectors with associated eigenvalue and with associated eigenvalue . We then have and .  Yes. We know that the eigenvectors are with associated eigenvalue and with associated eigenvalue . Therefore, and . This shows that the diagonalization is ; that is, since is already diagonal, it is diagonalized by the identity matrix.  We find eigenvectors , , and with associated eigenvalues , , and . Therefore, where   Once again, we see that is an eigenvalue with multiplicity one and is an eigenvalue with multiplicity two. However, so we are not able to find a basis for consisting of eigenvalues of . Therefore, is not diagonalizable.  If ,   is invertible since .  We know that and are eigenvectors of with associated eigenvalues and . If is an eigenvector of with associated eigenvalue , then is an eigenvector of with associated eigenvalue . Therefore, where .  We have where .         Powers of a diagonalizable matrix  In several earlier examples, we have been interested in computing powers of a given matrix. For instance, in , we had the matrix and an initial vector , and we wanted to compute In particular, we wanted to find and determine what happens as becomes very large. If a matrix is diagonalizable, writing can help us understand powers of more easily.      Let's begin with the diagonal matrix . Find the powers , , and . What is for a general value of ?  Suppose that is a matrix with eigenvector and associated eigenvalue ; that is, . By considering , explain why is also an eigenvector of with eigenvalue .  Suppose that where . Remembering that the columns of are eigenvectors of , explain why is diagonalizable and find a diagonalization in terms of and .  Give another explanation of the diagonalizability of by writing .  In the same way, find a diagonalization of , , and .  Suppose that is a diagonalizable matrix with eigenvalues and . What happens to as becomes very large?       We have   We know that so that is also an eigenvector of with associated eigenvalue .  Since eigenvectors of are also eigenvectors of , we can use the matrix to diagonalize . The eigenvalues are squared, however, so we have where .  We can also see this by noting that    , , and .  We can write where . Therefore, where . As becomes very large, and become very close to zero. Hence and become very close to the zero matrix.     If is diagonalizable, the activity demonstrates that any power of is as well.    If , then . When is invertible, we also have .     Let's revisit where we had the matrix and the initial vector . We were interested in understanding the sequence of vectors , which means that .  We can verify that and are eigenvectors of having associated eigenvalues and . This means that where Therefore, the powers of have the form .  Notice that . As increases, becomes closer and closer to zero. This means that for very large powers , we have and therefore   Beginning with the vector , we find that when is very large.     Similarity and complex eigenvalues  We have been interested in diagonalizing a matrix because doing so relates a matrix to a simpler diagonal matrix . In particular, the effect of multiplying a vector by , viewed in the basis defined by the columns of , is the same as the effect of multiplying by in the standard basis.  While many matrices are diagonalizable, there are some that are not. For example, if a matrix has complex eigenvalues, it is not possible to find a basis of consisting of eigenvectors, which means that the matrix is not diagonalizable. In this case, however, we can still relate the matrix to a simpler form that explains the geometric effect this matrix has on vectors.    similarity  We say that is similar to if there is an invertible matrix such that .    Notice that a matrix is diagonalizable if and only if it is similar to a diagonal matrix. In case a matrix has complex eigenvalues, we will find a simpler matrix that is similar to and note that has the same effect, when viewed in the basis defined by the columns of , as , when viewed in the standard basis.  To begin, suppose that is a matrix having a complex eigenvalue . It turns out that is similar to .   The next activity shows that has a simple geometric effect on . First, however, we will use polar coordinates to rewrite . As shown in the figure, the point defines , the distance from the origin, and , the angle formed with the positive horizontal axis. We then have Notice that the Pythagorean theorem says that .      We begin by rewriting in terms of and and noting that   Explain why has the geometric effect of rotating vectors by and scaling them by a factor of .  Let's now consider the matrix whose eigenvalues are and . We will choose to focus on one of the eigenvalues   Form the matrix using these values of and . Then rewrite the point in polar coordinates by identifying the values of and . Explain the geometric effect of multiplying vectors by .   Suppose that . Verify that .    Explain why .  We formed the matrix by choosing the eigenvalue . Suppose we had instead chosen . Form the matrix and use polar coordinates to describe the geometric effect of .  Using the matrix , show that .      The matrix has the geometric effect of scaling vectors uniformly by a factor of while the matrix rotates vectors by .  We have so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .  Sage will verify this relationship.  As we saw earlier, we have and hence .  We have and so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .     If the matrix has a complex eigenvalue , it turns out that is always similar to the matrix whose geometric effect on vectors can be described in terms of a rotation and a scaling. There is, in fact, a method for finding the matrix so that that we'll see in . For now, we note that has the same geometric effect as , when viewed in the basis provided by the columns of . We will put this fact to use in the next section to understand certain dynamical systems.    If is a matrix with a complex eigenvalue , then is similar to ; that is, there is a matrix such that .      Summary  Our goal in this section has been to use the eigenvalues and eigenvectors of a matrix to relate to a simpler matrix.  We said that is diagonalizable if we can write where is a diagonal matrix. The columns of consist of eigenvectors of and the diagonal entries of are the associated eigenvalues.  An matrix is diagonalizable if and only if there is a basis of consisting of eigenvectors of .  We said that and are similar if there is an invertible matrix such that . In this case, .  If is a matrix with complex eigenvalue , then is similar to . Writing the point in polar coordinates and , we see that rotates vectors through an angle and scales them by a factor of .       Determine whether the following matrices are diagonalizable. If so, find matrices and such that .   .   .   .   .   .       and    is not diagonalizable.   is not diagonalizable.   and    and      We find that is an eigenvector with associated eigenvalue as is with associated eigenvalue . Therefore, we have where   We see that is an eigenvalue having multiplicity two. However, so we cannot find a basis for consisting of eigenvectors of . Therefore, is not diagonalizable.  We see that has complex eigenvalues so it is not diagonalizable in the form for a diagonal matrix .  The matrix has three distinct eigenvalues , , and with associated eigenvectors , , and . This shows us that where   We have an eigenvalue having multiplicity two, but its eigenspace has dimension two so is diagonalizable. In particular, we choose and have .     Determine whether the following matrices have complex eigenvalues. If so, find the matrix such that .   .   .   .     There are two real eigenvalues.  There is a single real eigenvalue.   or .     We have two real eigenvalues .  We have a single real eigenvalue .  We have complex eigenvalues so we can choose or .     Determine whether the following statements are true or false and provide a justification for your response.  If is invertible, then is diagonalizable.  If and are similar and is invertible, then is also invertible.  If is a diagonalizable matrix, then there is a basis of consisting of eigenvectors of .  If is diagonalizable, then is also diagonalizable.  If is diagonalizable, then is invertible.      False  True  True  True  False     False. A matrix can be invertible without us being able to form a basis consisting of eigenvectors. An example is .  True. If and are similar, then so that . If is invertible, we know that , which also tells us that and hence that is invertible.  True. If , then the columns of are eigenvectors of that form a basis for .  True. If , then .  False. It is possible that has eigenvalue , which would imply that is not invertible.     Provide a justification for your response to the following questions.  If is a matrix having eigenvalues , can you guarantee that is diagonalizable?  If is a matrix with a complex eigenvalue, can you guarantee that is diagonalizable?  If is similar to the matrix , is diagonalizable?  What can you say about a matrix that is similar to the identity matrix?  If is a diagonalizable matrix with a single eigenvalue , what is ?      Yes  No  Yes  Only the identity   .     Yes. If has real and distinct eigenvalues, then there is a basis of consisting of eigenvectors of . Therefore, is diagonalizable.  No. We can write where , but is not diagonalizable.  Yes. Since is diagonal and , then is diagonalizable.  Only the identity because .   . If we denote , then .     Describe geometric effect that the following matrices have on :                     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .     We say that is similar to if there is a matrix such that .  If is similar to , explain why is similar to .  If is similar to and is similar to , explain why is similar to .  If is similar to and is diagonalizable, explain why is diagonalizable.  If and are similar, explain why and have the same characteristic polynomial; that is, explain why .  If and are similar, explain why and have the same eigenvalues.      If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.     If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.     Suppose that where .  Explain the geometric effect that has on vectors in .  Explain the geometric effect that has on vectors in .  What can you say about and other powers of ?  Is invertible?      The matrix projects vectors onto the horizontal axis.   projects vectors onto the line defined by the eigenvector .   .  No     The matrix projects vectors onto the horizontal axis; that is, it produces the shadow of a vector on the horizontal axis from a flashlight shining down the vertical axis.   projects vectors onto the line defined by the eigenvector .  Since and , we have . That is, and .   is not invertible because is an eigenvalue.     When is a matrix with a complex eigenvalue , we have said that there is a matrix such that where . In this exercise, we will learn how to find the matrix . As an example, we will consider the matrix .  Show that the eigenvalues of are complex.  Choose one of the complex eigenvalues and construct the usual matrix .  Using the same eigenvalue, we will find an eigenvector where the entries of are complex numbers. As always, we will describe by constructing the matrix and finding its reduced row echelon form. In doing so, we will necessarily need to use complex arithmetic.  We have now found a complex eigenvector . Write to identify vectors and having real entries.  Construct the matrix and verify that .       .  If , then .      and .       The eigenvalues are .  We will choose so that .  Construct the matrix so that a basis vector for the null space is .  We have giving and .  We let and verify that .     For each of the following matrices, sketch the vector and powers for .     .        .        .     Consider a matrix of the form with . What happens when becomes very large when   .   .   .      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.       For each of the following matrices and vectors, sketch the vector along with for .     .                        Find the eigenvalues and eigenvectors of to create your sketch.     If is a matrix with eigenvalues and and is any vector, what happens to when becomes very large?                          Vectors will be pulled into the origin.       Vectors are expanded horizontally and compressed vertically.       Vectors are compressed both horizontally and vertically.       Vectors are expanded both horizontally and vertically.       There are eigenvectors with and with . Vectors are expanded in the direction of and compressed in the direction of .     Because both eigenvalues have an absolute value smaller than , vectors will be compressed in the direction of both eigenvectors and hence be pulled into the origin.     "
},
{
  "id": "fig-eigen-diag-A",
  "level": "2",
  "url": "sec-eigen-diag.html#fig-eigen-diag-A",
  "type": "Figure",
  "number": "4.3.1",
  "title": "",
  "body": "    The matrix has the same geometric effect as the diagonal matrix when viewed in the basis of eigenvectors.  "
},
{
  "id": "exploration-16",
  "level": "2",
  "url": "sec-eigen-diag.html#exploration-16",
  "type": "Preview Activity",
  "number": "4.3.1",
  "title": "",
  "body": "  In this preview activity, we will review some familiar properties about matrix multiplication that appear in this section.   Remember that matrix-vector multiplication constructs linear combinations of the columns of the matrix. For instance, if , express the product in terms of and .    What is the product in terms of and ?    Next, remember how matrix-matrix multiplication is defined. Suppose that we have matrices and and that . How can we express the matrix product in terms of the columns of ?    Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Express the product in terms of and .    Suppose that is the matrix from the previous part and that . What is the matrix product          .     .     .     .     .     "
},
{
  "id": "activity-48",
  "level": "2",
  "url": "sec-eigen-diag.html#activity-48",
  "type": "Activity",
  "number": "4.3.2",
  "title": "",
  "body": "  Suppose that is a matrix having eigenvectors and with associated eigenvalues and . Because the eigenvalues are real and distinct, we know by that these eigenvectors form a basis of .   What are the products and in terms of and ?    If we form the matrix , what is the product in terms of and ?    Use the eigenvalues to form the diagonal matrix and determine the product in terms of and .    The results from the previous two parts of this activity demonstrate that . Using the fact that the eigenvectors and form a basis of , explain why is invertible and that we must have .    Suppose that . Verify that and are eigenvectors of with eigenvalues and .    Use the Sage cell below to define the matrices and and then verify that .        We have and .    .     . Comparing the result of this part of the activity to the previous, we see that .    Since the eigenvectors form a basis, the columns of are linearly independent and their span is . This guarantees that is invertible. Multiplying the equation on the right by gives .    The rest of the activity can be verified using Sage.   "
},
{
  "id": "prop-diagonalizable",
  "level": "2",
  "url": "sec-eigen-diag.html#prop-diagonalizable",
  "type": "Proposition",
  "number": "4.3.2",
  "title": "",
  "body": "  If is an matrix and there is a basis of consisting of eigenvectors of having associated eigenvalues , then we can write where is the diagonal matrix whose diagonal entries are the eigenvalues of  and the matrix .   "
},
{
  "id": "example-43",
  "level": "2",
  "url": "sec-eigen-diag.html#example-43",
  "type": "Example",
  "number": "4.3.3",
  "title": "",
  "body": " We have seen that has eigenvectors and with associated eigenvalues and . Forming the matrices we see that .  This is the sense in which we mean that is equivalent to a diagonal matrix . The expression says that , expressed in the basis defined by the columns of , has the same geometric effect as , expressed in the standard basis .  "
},
{
  "id": "definition-22",
  "level": "2",
  "url": "sec-eigen-diag.html#definition-22",
  "type": "Definition",
  "number": "4.3.4",
  "title": "",
  "body": "  diagonalizable  We say that the matrix is diagonalizable if there is a diagonal matrix and invertible matrix such that    "
},
{
  "id": "example-44",
  "level": "2",
  "url": "sec-eigen-diag.html#example-44",
  "type": "Example",
  "number": "4.3.5",
  "title": "",
  "body": "  We will try to find a diagonalization of whose characteristic equation is . This shows that the eigenvalues of are and .  By constructing , we find a basis for consisting of the vector . Similarly, a basis for consists of the vector . This shows that we can construct a basis of consisting of eigenvectors of .  We now form the matrices and verify that .  There are, in fact, many ways to diagonalize . For instance, we could change the order of the eigenvalues and eigenvectors and write .  If we choose a different basis for the eigenspaces, we will also find a different matrix that diagonalizes . The point is that there are many ways in which can be written in the form .   "
},
{
  "id": "example-45",
  "level": "2",
  "url": "sec-eigen-diag.html#example-45",
  "type": "Example",
  "number": "4.3.6",
  "title": "",
  "body": "  We will try to find a diagonalization of .  Once again, we find the eigenvalues by solving the characteristic equation: . In this case, there is a single eigenvalue .  We find a basis for the eigenspace by describing : . This shows that the eigenspace is one-dimensional with forming a basis.  In this case, there is not a basis of consisting of eigenvectors of , which tells us that is not diagonalizable.   "
},
{
  "id": "proposition-32",
  "level": "2",
  "url": "sec-eigen-diag.html#proposition-32",
  "type": "Proposition",
  "number": "4.3.7",
  "title": "",
  "body": "  An matrix is diagonalizable if and only if there is a basis of consisting of eigenvectors of .   "
},
{
  "id": "example-46",
  "level": "2",
  "url": "sec-eigen-diag.html#example-46",
  "type": "Example",
  "number": "4.3.8",
  "title": "",
  "body": "  Suppose we know that where The columns of form eigenvectors of so that is an eigenvector of with eigenvalue and is an eigenvector with eigenvalue .  We can verify this by computing and checking that and .   "
},
{
  "id": "activity-49",
  "level": "2",
  "url": "sec-eigen-diag.html#activity-49",
  "type": "Activity",
  "number": "4.3.3",
  "title": "",
  "body": "    Find a diagonalization of , if one exists, when .  Can the diagonal matrix be diagonalized? If so, explain how to find the matrices and .  Find a diagonalization of , if one exists, when .    Find a diagonalization of , if one exists, when .    Suppose that where .  Explain why is invertible.  Find a diagonalization of .  Find a diagonalization of .        We find that has eigenvectors with associated eigenvalue and with associated eigenvalue . We then have and .  Yes. We know that the eigenvectors are with associated eigenvalue and with associated eigenvalue . Therefore, and . This shows that the diagonalization is ; that is, since is already diagonal, it is diagonalized by the identity matrix.  We find eigenvectors , , and with associated eigenvalues , , and . Therefore, where   Once again, we see that is an eigenvalue with multiplicity one and is an eigenvalue with multiplicity two. However, so we are not able to find a basis for consisting of eigenvalues of . Therefore, is not diagonalizable.  If ,   is invertible since .  We know that and are eigenvectors of with associated eigenvalues and . If is an eigenvector of with associated eigenvalue , then is an eigenvector of with associated eigenvalue . Therefore, where .  We have where .      "
},
{
  "id": "activity-50",
  "level": "2",
  "url": "sec-eigen-diag.html#activity-50",
  "type": "Activity",
  "number": "4.3.4",
  "title": "",
  "body": "    Let's begin with the diagonal matrix . Find the powers , , and . What is for a general value of ?  Suppose that is a matrix with eigenvector and associated eigenvalue ; that is, . By considering , explain why is also an eigenvector of with eigenvalue .  Suppose that where . Remembering that the columns of are eigenvectors of , explain why is diagonalizable and find a diagonalization in terms of and .  Give another explanation of the diagonalizability of by writing .  In the same way, find a diagonalization of , , and .  Suppose that is a diagonalizable matrix with eigenvalues and . What happens to as becomes very large?       We have   We know that so that is also an eigenvector of with associated eigenvalue .  Since eigenvectors of are also eigenvectors of , we can use the matrix to diagonalize . The eigenvalues are squared, however, so we have where .  We can also see this by noting that    , , and .  We can write where . Therefore, where . As becomes very large, and become very close to zero. Hence and become very close to the zero matrix.    "
},
{
  "id": "proposition-33",
  "level": "2",
  "url": "sec-eigen-diag.html#proposition-33",
  "type": "Proposition",
  "number": "4.3.9",
  "title": "",
  "body": "  If , then . When is invertible, we also have .   "
},
{
  "id": "example-47",
  "level": "2",
  "url": "sec-eigen-diag.html#example-47",
  "type": "Example",
  "number": "4.3.10",
  "title": "",
  "body": " Let's revisit where we had the matrix and the initial vector . We were interested in understanding the sequence of vectors , which means that .  We can verify that and are eigenvectors of having associated eigenvalues and . This means that where Therefore, the powers of have the form .  Notice that . As increases, becomes closer and closer to zero. This means that for very large powers , we have and therefore   Beginning with the vector , we find that when is very large.  "
},
{
  "id": "definition-23",
  "level": "2",
  "url": "sec-eigen-diag.html#definition-23",
  "type": "Definition",
  "number": "4.3.11",
  "title": "",
  "body": "  similarity  We say that is similar to if there is an invertible matrix such that .   "
},
{
  "id": "activity-51",
  "level": "2",
  "url": "sec-eigen-diag.html#activity-51",
  "type": "Activity",
  "number": "4.3.5",
  "title": "",
  "body": "  We begin by rewriting in terms of and and noting that   Explain why has the geometric effect of rotating vectors by and scaling them by a factor of .  Let's now consider the matrix whose eigenvalues are and . We will choose to focus on one of the eigenvalues   Form the matrix using these values of and . Then rewrite the point in polar coordinates by identifying the values of and . Explain the geometric effect of multiplying vectors by .   Suppose that . Verify that .    Explain why .  We formed the matrix by choosing the eigenvalue . Suppose we had instead chosen . Form the matrix and use polar coordinates to describe the geometric effect of .  Using the matrix , show that .      The matrix has the geometric effect of scaling vectors uniformly by a factor of while the matrix rotates vectors by .  We have so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .  Sage will verify this relationship.  As we saw earlier, we have and hence .  We have and so we form the matrix . This shows that will scale vectors by a factor of while rotating them by .    "
},
{
  "id": "proposition-34",
  "level": "2",
  "url": "sec-eigen-diag.html#proposition-34",
  "type": "Proposition",
  "number": "4.3.12",
  "title": "",
  "body": "  If is a matrix with a complex eigenvalue , then is similar to ; that is, there is a matrix such that .   "
},
{
  "id": "exercise-149",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-149",
  "type": "Exercise",
  "number": "4.3.5.1",
  "title": "",
  "body": " Determine whether the following matrices are diagonalizable. If so, find matrices and such that .   .   .   .   .   .       and    is not diagonalizable.   is not diagonalizable.   and    and      We find that is an eigenvector with associated eigenvalue as is with associated eigenvalue . Therefore, we have where   We see that is an eigenvalue having multiplicity two. However, so we cannot find a basis for consisting of eigenvectors of . Therefore, is not diagonalizable.  We see that has complex eigenvalues so it is not diagonalizable in the form for a diagonal matrix .  The matrix has three distinct eigenvalues , , and with associated eigenvectors , , and . This shows us that where   We have an eigenvalue having multiplicity two, but its eigenspace has dimension two so is diagonalizable. In particular, we choose and have .   "
},
{
  "id": "exercise-150",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-150",
  "type": "Exercise",
  "number": "4.3.5.2",
  "title": "",
  "body": " Determine whether the following matrices have complex eigenvalues. If so, find the matrix such that .   .   .   .     There are two real eigenvalues.  There is a single real eigenvalue.   or .     We have two real eigenvalues .  We have a single real eigenvalue .  We have complex eigenvalues so we can choose or .   "
},
{
  "id": "exercise-151",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-151",
  "type": "Exercise",
  "number": "4.3.5.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response.  If is invertible, then is diagonalizable.  If and are similar and is invertible, then is also invertible.  If is a diagonalizable matrix, then there is a basis of consisting of eigenvectors of .  If is diagonalizable, then is also diagonalizable.  If is diagonalizable, then is invertible.      False  True  True  True  False     False. A matrix can be invertible without us being able to form a basis consisting of eigenvectors. An example is .  True. If and are similar, then so that . If is invertible, we know that , which also tells us that and hence that is invertible.  True. If , then the columns of are eigenvectors of that form a basis for .  True. If , then .  False. It is possible that has eigenvalue , which would imply that is not invertible.   "
},
{
  "id": "exercise-152",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-152",
  "type": "Exercise",
  "number": "4.3.5.4",
  "title": "",
  "body": " Provide a justification for your response to the following questions.  If is a matrix having eigenvalues , can you guarantee that is diagonalizable?  If is a matrix with a complex eigenvalue, can you guarantee that is diagonalizable?  If is similar to the matrix , is diagonalizable?  What can you say about a matrix that is similar to the identity matrix?  If is a diagonalizable matrix with a single eigenvalue , what is ?      Yes  No  Yes  Only the identity   .     Yes. If has real and distinct eigenvalues, then there is a basis of consisting of eigenvectors of . Therefore, is diagonalizable.  No. We can write where , but is not diagonalizable.  Yes. Since is diagonal and , then is diagonalizable.  Only the identity because .   . If we denote , then .   "
},
{
  "id": "exercise-153",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-153",
  "type": "Exercise",
  "number": "4.3.5.5",
  "title": "",
  "body": " Describe geometric effect that the following matrices have on :                     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .     Multiplying by uniformly stretches vectors by a factor of .  This is a shear so that vectors are stretched and then pushed horizontally.  We write where and . Therefore, will stretch vectors by a factor of and rotate them by .  Vectors are stretched in the horizontal direction by a factor of and stretched vertically by a factor of before being reflected in the horizontal axis.  This matrix has eigenvectors with associated eigenvalue and with . Therefore, stretches vectors by a factor of in the direction of and a factor of in the direction of before reflecting in the line defined by .   "
},
{
  "id": "exercise-154",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-154",
  "type": "Exercise",
  "number": "4.3.5.6",
  "title": "",
  "body": " We say that is similar to if there is a matrix such that .  If is similar to , explain why is similar to .  If is similar to and is similar to , explain why is similar to .  If is similar to and is diagonalizable, explain why is diagonalizable.  If and are similar, explain why and have the same characteristic polynomial; that is, explain why .  If and are similar, explain why and have the same eigenvalues.      If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.     If , then . If we call , then , which shows that is similar to .  If and , then , which shows that is similar to .  If is diagonalizable, then it is similar to a diagonal matrix . Since is similar to and is similar to , we know that is also similar to so that is diagonalizable.  If , then   The eigenvalues of a matrix are the roots of its characteristic polynomial. If and are similar, then they have the same characteristic polynomial and hence the same eigenvalues.   "
},
{
  "id": "exercise-155",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-155",
  "type": "Exercise",
  "number": "4.3.5.7",
  "title": "",
  "body": " Suppose that where .  Explain the geometric effect that has on vectors in .  Explain the geometric effect that has on vectors in .  What can you say about and other powers of ?  Is invertible?      The matrix projects vectors onto the horizontal axis.   projects vectors onto the line defined by the eigenvector .   .  No     The matrix projects vectors onto the horizontal axis; that is, it produces the shadow of a vector on the horizontal axis from a flashlight shining down the vertical axis.   projects vectors onto the line defined by the eigenvector .  Since and , we have . That is, and .   is not invertible because is an eigenvalue.   "
},
{
  "id": "exercise-complex-eigenvector",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-complex-eigenvector",
  "type": "Exercise",
  "number": "4.3.5.8",
  "title": "",
  "body": " When is a matrix with a complex eigenvalue , we have said that there is a matrix such that where . In this exercise, we will learn how to find the matrix . As an example, we will consider the matrix .  Show that the eigenvalues of are complex.  Choose one of the complex eigenvalues and construct the usual matrix .  Using the same eigenvalue, we will find an eigenvector where the entries of are complex numbers. As always, we will describe by constructing the matrix and finding its reduced row echelon form. In doing so, we will necessarily need to use complex arithmetic.  We have now found a complex eigenvector . Write to identify vectors and having real entries.  Construct the matrix and verify that .       .  If , then .      and .       The eigenvalues are .  We will choose so that .  Construct the matrix so that a basis vector for the null space is .  We have giving and .  We let and verify that .   "
},
{
  "id": "exercise-157",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-157",
  "type": "Exercise",
  "number": "4.3.5.9",
  "title": "",
  "body": " For each of the following matrices, sketch the vector and powers for .     .        .        .     Consider a matrix of the form with . What happens when becomes very large when   .   .   .      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.      In each case, the vectors are rotated by . Note that the figures below are draw at different scales.    The vectors are stretched by a factor of 1.4.       The vectors are stretched by a factor of 0.8.       The length of the vectors is unchanged.     For different values of ,  The vectors are pulled into the origin as their lengths become increasingly small.  The length of the vectors is unchanged.  The vectors are pushed away from the origin as their lengths become increasingly large.     "
},
{
  "id": "exercise-158",
  "level": "2",
  "url": "sec-eigen-diag.html#exercise-158",
  "type": "Exercise",
  "number": "4.3.5.10",
  "title": "",
  "body": " For each of the following matrices and vectors, sketch the vector along with for .     .                        Find the eigenvalues and eigenvectors of to create your sketch.     If is a matrix with eigenvalues and and is any vector, what happens to when becomes very large?                          Vectors will be pulled into the origin.       Vectors are expanded horizontally and compressed vertically.       Vectors are compressed both horizontally and vertically.       Vectors are expanded both horizontally and vertically.       There are eigenvectors with and with . Vectors are expanded in the direction of and compressed in the direction of .     Because both eigenvalues have an absolute value smaller than , vectors will be compressed in the direction of both eigenvectors and hence be pulled into the origin.   "
},
{
  "id": "sec-dynamical",
  "level": "1",
  "url": "sec-dynamical.html",
  "type": "Section",
  "number": "4.4",
  "title": "Dynamical systems",
  "body": " Dynamical systems   The last section demonstrated ways in which we may relate a matrix, and the effect that multiplication has on vectors, to a simpler form. For instance, if there is a basis of consisting of eigenvectors of , we saw that is similar to a diagonal matrix . As a result, the effect of multiplying vectors by , when expressed using the basis of eigenvectors, is the same as multiplying by .  In this section, we will put these ideas to use as we explore discrete dynamical systems, first encountered in . Recall that we used a state vector to characterize the state of some system at a particular time, such as the distribution of delivery trucks between two locations. A matrix described the transition of the state vector with characterizing the state of the system at a later time. Since we would like to understand how the state vector evolves over time, we are interested in studying the sequence of vectors .  Our goal in this section is to describe the types of behaviors that dynamical systems exhibit and to develop a means of detecting these behaviors.    Suppose that we have a diagonalizable matrix where .  Find the eigenvalues of and find a basis for the associated eigenspaces.  Form a basis of consisting of eigenvectors of and write the vector as a linear combination of basis vectors.  Write as a linear combination of basis vectors.  For some power , write as a linear combination of basis vectors.  Find the vector .      Since has been diagonalized as , the eigenvalues of are the diagonal entries of and the eigenvectors are the columns of . Therefore, we know the eigenvalues are with associated eigenvector and with associated eigenvector .  The columns of , and , form a basis for . We find that .  Then .  We have .   .       A first example  We will begin with a dynamical system that illustrates how the ideas we've been developing can help us understand the populations of two interacting species. There are several possible ways in which two species may interact. For example, wolves on Isle Royale in northern Michigan prey on moose so this interaction is often called a predator-prey relationship. Other interactions between species, such as bees and flowering plants, are mutually beneficial for both species.    Suppose we have two species and that interact with each another and that we record the change in their populations from year to year. When we begin our study, the populations, measured in thousands, are and ; after years, the populations are and .  If we know the populations in one year, suppose that the populations in the following year are determined by the expressions This is an example of a mutually beneficial relationship between two species. If species is not present, then , which means that the population of species decreases every year. However, species benefits from the presence of species , which helps to grow by 80% of the population of species . In the same way, benefits from the presence of .  We will record the populations in a vector and note that where .  Verify that are eigenvectors of and find their respective eigenvalues.  Suppose that initially . Write as a linear combination of the eigenvectors and .  Write the vectors , , and as linear combinations of the eigenvectors and .  What happens to after a very long time?  When becomes very large, what happens to the ratio of the populations ?  After a very long time, by approximately what factor does the population of grow every year? By approximately what factor does the population of grow every year?  If we begin instead with , what eventually happens to the ratio as becomes very large?      We find that and . This shows that and are eigenvectors with associated eigenvalues and .  Solving for the weights of the linear combination of and that produces , we find that .  Each time we multiply by , the eigenvectors are multiplied by their associated eigenvalues. This gives   After a long time so we have .  More generally, we have . As grows large, becomes insignificantly small so that . This means that and so that the ratio .  We see that so that the populations both grow by a factor of approximately 1.3, which is a 30% growth rate.  Now we have . In the same way, when is very large, we have so that and . This gives the same ratio: .     This activity demonstrates the type of systems we will be considering. In particular, we will have vectors that describe the state of the system at time and a matrix that describes how the state evolves from one time to the next: . The eigenvalues and eigenvectors of provide the key that helps us understand how the vectors evolve and that enables us to make long-range predictions.  Let's look at the specific example in the previous activity more carefully. We see that and that the matrix has eigenvectors and with associated eigenvalues and .  With initial populations , we have   Let's shift our perspective slightly. The eigenvectors and form a basis of , which says that is diagonalizable; that is, where   The coordinate system defined by the basis can be used to express the state vectors. For instance, we can write the initial state vector , which means that . Moreover, so that In the same way,   More generally, we have which is a restatement of the fact that is similar to .   Thinking about this geometrically, we begin with the vector . Subsequent vectors are obtained by scaling horizontally by a factor of and scaling vertically by a factor . Notice how the points move along a curve away from the origin becoming ever closer to the horizontal axis. After a very long time, .     To recover the behavior of the sequence , we change coordinate systems using the basis defined by and . Here, the points move along a curve away from the origin becoming ever closer to the line defined by .    Eventually, the vectors become practically indistinguishable from a scalar multiple of since . This means that This shows that so that we expect the population of species to eventually be about twice that of species .  In addition, so that and , which tells us that both populations are multiplied by 1.3 every year meaning the annual growth rate for both populations is about 30%.  In the same way, we can consider other possible initial populations as shown in . Regardless of , the population vectors, in the coordinates defined by , are scaled horizontally by a factor of and vertically by a factor of . The sequence of points , called trajectories , move along the curves, as shown on the left. In the standard coordinate system, we see that the trajectories converge to the eigenspace .      The trajectories of the dynamical system formed by the matrix in the coordinate system defined by , on the left, and in the standard coordinate system, on the right.   We conclude that, regardless of the initial populations, the ratio of the populations will approach 2 to 1 and that the growth rate for both populations approaches 30%. This example demonstrates the power of using eigenvalues and eigenvectors to rewrite the problem in terms of a new coordinate system. By doing so, we are able to predict the long-term behavior of the populations independently of the initial populations.  Diagrams like those shown in are called phase portraits. On the left of is the phase portrait of the diagonal matrix while the right of that figure shows the phase portrait of . The phase portrait of is relatively easy to understand because it is determined only by the two eigenvalues. Once we have the phase portrait of , however, the phase portrait of has a similar appearance with the eigenvectors replacing the standard basis vectors .    Classifying dynamical systems  In the previous example, we were able to make predictions about the behavior of trajectories by considering the eigenvalues and eigenvectors of the matrix . The next activity looks at a collection of matrices that demonstrate the types of behavior a dynamical system can exhibit.    We will now look at several more examples of dynamical systems. If , we note that the columns of form a basis of . Given below are several matrices written in the form for some matrix . For each matrix, state the eigenvalues of and sketch a phase portrait for the matrix on the left and a phase portrait for on the right. Describe the behavior of as becomes very large for a typical initial vector .    where .        where .        where .        where .        where .        where .                                           This activity demonstrates six possible types of dynamical systems, which are determined by the eigenvalues of .  Suppose that has two real eigenvalues and and that both . In this case, any nonzero vector forms a trajectory that moves away from the origin so we say that the origin is a repellor . This is illustrated in .      The origin is a repellor when .    Suppose that has two real eigenvalues and and that . In this case, most nonzero vectors form trajectories that converge to the eigenspace . In this case, we say that the origin is a saddle as illustrated in .      The origin is a saddle when .    Suppose that has two real eigenvalues and and that both . In this case, any nonzero vector forms a trajectory that moves into the origin so we say that the origin is an attractor . This is illustrated in .      The origin is an attractor when .    Suppose that has a complex eigenvalue where . In this case, a nonzero vector forms a trajectory that spirals away from the origin. We say that the origin is a spiral repellor , as illustrated in .      The origin is a spiral repellor when has an eigenvalue with .    Suppose that has a complex eigenvalue where . In this case, a nonzero vector forms a trajectory that moves on a closed curve around the origin. We say that the origin is a center , as illustrated in .      The origin is a center when has an eigenvalue with .    Suppose that has a complex eigenvalue where . In this case, a nonzero vector forms a trajectory that spirals into the origin. We say that the origin is a spiral attractor , as illustrated in .      The origin is a spiral attractor when has an eigenvalue with .     This list includes many types of expected behavior, but there are other possibilities if, for instance, one of the eigenvalues is 0. The next section explores the situation when one of the eigenvalues is 1.    In this activity, we will consider several ways in which two species might interact with one another. Throughout, we will consider two species and whose populations in year form a vector and which evolve according to the rule .   Suppose that .  Explain why the species do not interact with one another. Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose now that .  Explain why is a beneficial species for . Which of the six types of dynamical systems do we have? What happens to both species after a long time?    If , explain why this describes a predator-prey system. Which of the species is the predator and which is the prey? Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose that . Compare this predator-prey system to the one in the previous part. Which of the six types of dynamical systems do we have? What happens to both species after a long time?        With this matrix , we have This shows that the population of one species does not depend on the other. This dynamical system is a saddle because we have the eigenvalues and . Eventually, species will become extinct while species grows by a factor of 1.6.  Now we have The population of species is not influenced by species . However, we see that the population of species grows in the presence of species . In other words, species helps species to grow so we say that is beneficial for .  Because the eigenvalues are and , this dynamical system is again a saddle. The associated eigenvectors are and . After a long time, the population vector so both populations grow by a factor of 1.6 and with a ratio   Here we have Species helps species to grow, while species inhibits the growth of species . One explanation for this is that species preys on species as a food source.  We have eigenvectors with associated eigenvalue and with . This dynamical system is a repellor so both species will grow arbitrarily large.  This example is similar to the previous one, but the coefficients are slightly different. We see that the growth rate of both species is smaller. For instance, in the previous problem, we had while we now have . This says that the reproduction rate of species has decreased from to . In the same way, that of species has decreased from to . Also, so the presence of species is less beneficial to species .  We now have the eigenvalues and , which means that this dynamical system is an attractor and that both species will become extinct.       A system  Up to this point, we have focused on systems. In fact, the general case is quite similar. As an example, consider a system where the matrix has eigenvalues , , and . Since the eigenvalues are real and distinct, there is a basis consisting of eigenvectors of so we can look at the trajectories in the coordinate system defined by . The phase portraits in show how some representative trajectories will evolve. We see that all the trajectories will converge into the eigenspace .       In a system with , , and , the trajectories move along the curves shown above.   In the same way, suppose we have a system with complex eigenvalues and . Since the complex eigenvalues satisfy , there is a two-dimensional subspace in which the trajectories spiral in toward the origin. The phase portraits in show some of the trajectories. Once again, we see that all the trajectories converge into the eigenspace .       In a system with complex eigenvalues with and , the trajectories move along the curves shown above.     The following type of analysis has been used to study the population of a bison herd. We will divide the population of female bison into three groups: juveniles who are less than one year old; yearlings between one and two years old; and adults who are older than two years.   Each year,  80% of the juveniles survive to become yearlings.  90% of the yearlings survive to become adults.  80% of the adults survive.  40% of the adults give birth to a juvenile.     By , , and , we denote the number of juveniles, yearlings, and adults in year . We have .  Find similar expressions for and in terms of , , and .  As is usual, we write the matrix . Write the matrix such that and find its eigenvalues.  We can write where the matrices and are approximately: Make a prediction about the long-term behavior of . For instance, at what rate does it grow? For every 100 adults, how many juveniles, and yearlings are there?  Suppose that the birth rate decreases so that only 30% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Suppose that the birth rate decreases further so that only 20% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Find the smallest birth rate that supports a stable population.      We have the relationships   The matrix .  There is a real eigenvalue , which is larger than . The other eigenvalues are complex and satisfy . Therefore, the complex eigenvalues will pull vectors in toward the line defined by the eigenvector . After a long time, the population is . All the populations grow annually by a factor of or 5.8%, and there are approximately 38 juveniles and 29 yearlings for every 100 adults.  We now have eigenvalues and . This shows that the growth rate is lowered to or % annually.  The first eigenvalue is so the growth rate is about %. In other words, the population decreases every year.  To be stable, we need the first eigenvalue . If we experiment with different birth rates, we see that a birth rate of about 0.278 gives this eigenvalue.       Summary  We have been exploring discrete dynamical systems in which an initial state vector evolves over time according to the rule . The eigenvalues and eigenvectors of help us understand the behavior of the state vectors. In the case, we saw that   produces an attractor so that trajectories are pulled in toward the origin.   and produces a saddle in which most trajectories are pushed away from the origin and in the direction of .   produces a repellor in which trajectories are pushed away from the origin.  The same kind of reasoning allows us to analyze systems as well.     For each of the matrices below, find the eigenvalues and, when appropriate, the eigenvectors to classify the dynamical system . Use this information to sketch the phase portraits.   .   .   .   .     Repellor  Spiral repellor  Saddle  Attractor     There are two real eigenvalues and with associated eigenvectors and . This dynamical system is a repellor so trajectories are pushed away from the origin.     This is a spiral repellor because there are complex eigenvalues whose length is greater than one. Trajectories will spiral away from the origin.     This is a saddle because there are two real eigenvalues and with associated eigenvectors and . Most trajectories will move away from the origin along the line defined by the eigenvector .     This is an attractor because there are two real eigenvalues and with associated eigenvectors and . Trajectories will be pulled in toward the origin.        We will consider matrices that have the form where where is a parameter that we will vary. Sketch phase portraits for and below when   .      .      .     For the different values of , determine which types of dynamical system results. For what range of values do we have an attractor? For what range of values do we have a saddle? For what value does the transition between the two types occur?    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .     Suppose that the populations of two species interact according to the relationships where is a parameter. As we saw in the text, this dynamical system represents a typical predator-prey relationship, and the parameter represents the rate at which species preys on . We will denote the matrix .  If , determine the eigenvectors and eigenvalues of the system and classify it as one of the six types. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     If , determine the eigenvectors and eigenvalues of the system. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     For what values of is the origin a saddle? What can you say about the populations when this happens?  Describe the evolution of the dynamical system as begins at and increases to .      and       and         As grows, there needs to be a sufficient population to ensure the survival of both species.     If , the eigenvectors are with associated eigenvalue and with eigenvalue . The phase portraits are as     With , we have eigenvectors with eigenvalue and with eigenvalue . The phase portraits appear as     We find the characteristic polynomial of to be which has roots So long as , one root will be real and larger than . We have a saddle when This shows that we have a saddle whenever .  When , we have a saddle. Notice that will go extinct if the initial population of is zero. As we increase , we still have a saddle, but will now go extinct when the initial population is positive. Eventually, when , we see that will go extinct as long as .  As we increase the predation rate, the population of is lowered by . However, is necessary to 's survival. When the predation rate grows too large, there needs to be a sufficiently large initial population to support both species.      Consider the matrices   Find the eigenvalues of . To which of the six types does the system belong?   Using the eigenvalues of , we can write for some matrices and . What is the matrix and what geometric effect does multiplication by have on vectors in the plane?  If we remember that , determine the smallest positive value of for which .  Find the eigenvalues of .  Find a matrix such that for some matrix . What geometric effect does multiplication by have on vectors in the plane?  Determine the smallest positive value of for which .     This dynamical system is a center.   .   .   .      .     The eigenvalues are so this dynamical system is a center.  The matrix is , which is a rotation.  If we rotate by four times, we obtain the identity. Therefore, .  Here, we find the eigenvalues .  The matrix is , which is a rotation.  If we rotate by six times, we obtain the identity. Therefore, .     Suppose we have the female population of a species is divided into juveniles, yearlings, and adults and that each year  90% of the juveniles live to be yearlings.  80% of the yearlings live to be adults.  60% of the adults survive to the next year.  50% of the adults give birth to a juvenile.      Set up a system of the form that describes this situation.  Find the eigenvalues of the matrix .   What prediction can you make about these populations after a very long time?  If the birth rate goes up to 80%, what prediction can you make about these populations after a very long time? For every 100 adults, how many juveniles, and yearlings are there?      If we write , we have    and .  The populations will eventually become extinct.  All the populations grow by %, and there are about juveniles and yearlings for every adults.     If we write , we have   We find the eigenvalues and .  All three eigenvalues have a length less than one. Therefore, the populations will eventually become extinct.  With a birth rate of %, the first eigenvalue goes up to , which means that, eventually, all the populations grow by %. An eigenvector is approximately so there are about juveniles and yearlings for every adults.     Determine whether the following statements are true or false and provide a justification for your response. In each case, we are considering a dynamical system of the form .  If the matrix has a complex eigenvalue, we cannot make a prediction about the behavior of the trajectories.  If has eigenvalues whose absolute value is smaller than 1, then all the trajectories are pulled in toward the origin.  If the origin is a repellor, then it is an attractor for the system .  If a matrix has complex eigenvalues , , , and , all of which satisfy , then all the trajectories are pushed away from the origin.  If the origin is a saddle, then all the trajectories are pushed away from the origin.     False  True  True  True  False     False. We know that the dynamical system is either a spiral repellor, center, or spiral attractor.  True. In this case, we have either an attractor or spiral attractor.  True. If is an attractor, then all the eigenvalues . The eigenvalues for are , which satisfy .  True. We can decompose into two two-dimensional subspaces on which acts as a spiral repellor.  False. There are trajectories along the line defined by the eigenvector associated to the smaller eigenvalue that are pulled in toward the origin.     The Fibonacci numbers form the sequence of numbers that begins . If we let denote the Fibonacci number, then . In general, a Fibonacci number is the sum of the previous two Fibonacci numbers; that is, so that we have     If we write , find the matrix such that .  Show that has eigenvalues with associated eigenvectors and .  Classify this dynamical system as one of the six types that we have seen in this section. What happens to as becomes very large?  Write the initial vector as a linear combination of eigenvectors and .  Write the vector as a linear combinations of and .  Explain why the Fibonacci number   Use this relationship to compute .  Explain why when is very large.    The number is called the golden ratio and is one of mathematics' special numbers.       Find the roots of the characteristic polynomial.   defines a saddle.   .  We have .  From the second component, we see that      For large values of , we have .     We have   The characteristic equation is Applying the quadratic formula, we see that the two eigenvalues are and .  We find the eigenvectors by row reducing the matrices .   Because and , this is a saddle.  We find .  We have   From the second component, we see that   Using this relationship, we compute that .  For large values of , we have , which says that and . This gives the ratio .     This exercise is a continuation of the previous one.  The Lucas numbers are defined by the same relationship as the Fibonacci numbers: . However, we begin with and , which leads to the sequence .  As before, form the vector so that . Express as a linear combination of and , eigenvectors of .  Explain why   Explain why is the closest integer to when is large, where is the golden ratio.  Use this observation to find .       .     Because , larger powers are very close to zero.   .     Constructing the augmented matrix and row reducing shows that .  Then we have . Since is the second component of , we have .  Because , larger powers are very close to zero. Therefore, .  We compute that so that .     Gil Strang defines the Gibonacci numbers  as follows. We begin with and . A subsequent Gibonacci number is the average of the two previous; that is, . We then have   If , find the matrix such that .  Find the eigenvalues and associated eigenvectors of .  Explain why this dynamical system does not neatly fit into one of the six types that we saw in this section.  Write as a linear combination of eigenvectors of .  Write as a linear combination of eigenvectors of .  What happens to as becomes very large?       The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.         stabilizes at .     If , then we have .  The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.  After constructing an augmented matrix and row reducing, we find .  We have .  As becomes large, the coefficient becomes very small. Therefore, . This shows that stabilizes at .     Consider a small rodent that lives for three years. Once again, we can separate a population of females into juveniles, yearlings, and adults. Suppose that, each year,  Half of the juveniles live to be yearlings.  One quarter of the yearlings live to be adults.  Adult females produce eight female offspring.  None of the adults survive to the next year.      Writing the populations of juveniles, yearlings, and adults in year using the vector , find the matrix such that .  Show that .  What are the eigenvalues of ? What does this say about the eigenvalues of ?  Verify your observation by finding the eigenvalues of .   What can you say about the trajectories of this dynamical system?  What does this mean about the population of rodents?  Find a population vector that is unchanged from year to year.       .  We can use Sage to compute that .  Any eigenvalue of must satisfy .   and   The trajectories lie on circles about the line defined by .  After every three years, the populations return to their initial values.       We have where .  We can use Sage to compute that .  The only eigenvalues of are so any eigenvalue of must satisfy .  Sage tells us that the eigenvalues are and .  The eigenvector satisfies so it is unchanged. The complex eigenvalues cause a rotation about the axis defined by . Therefore, the trajectories lie on circles about this line.  After every three years, the populations return to their initial values.  If , we know that so this population vector is unchanged.     "
},
{
  "id": "exploration-17",
  "level": "2",
  "url": "sec-dynamical.html#exploration-17",
  "type": "Preview Activity",
  "number": "4.4.1",
  "title": "",
  "body": "  Suppose that we have a diagonalizable matrix where .  Find the eigenvalues of and find a basis for the associated eigenspaces.  Form a basis of consisting of eigenvectors of and write the vector as a linear combination of basis vectors.  Write as a linear combination of basis vectors.  For some power , write as a linear combination of basis vectors.  Find the vector .      Since has been diagonalized as , the eigenvalues of are the diagonal entries of and the eigenvectors are the columns of . Therefore, we know the eigenvalues are with associated eigenvector and with associated eigenvector .  The columns of , and , form a basis for . We find that .  Then .  We have .   .    "
},
{
  "id": "activity-52",
  "level": "2",
  "url": "sec-dynamical.html#activity-52",
  "type": "Activity",
  "number": "4.4.2",
  "title": "",
  "body": "  Suppose we have two species and that interact with each another and that we record the change in their populations from year to year. When we begin our study, the populations, measured in thousands, are and ; after years, the populations are and .  If we know the populations in one year, suppose that the populations in the following year are determined by the expressions This is an example of a mutually beneficial relationship between two species. If species is not present, then , which means that the population of species decreases every year. However, species benefits from the presence of species , which helps to grow by 80% of the population of species . In the same way, benefits from the presence of .  We will record the populations in a vector and note that where .  Verify that are eigenvectors of and find their respective eigenvalues.  Suppose that initially . Write as a linear combination of the eigenvectors and .  Write the vectors , , and as linear combinations of the eigenvectors and .  What happens to after a very long time?  When becomes very large, what happens to the ratio of the populations ?  After a very long time, by approximately what factor does the population of grow every year? By approximately what factor does the population of grow every year?  If we begin instead with , what eventually happens to the ratio as becomes very large?      We find that and . This shows that and are eigenvectors with associated eigenvalues and .  Solving for the weights of the linear combination of and that produces , we find that .  Each time we multiply by , the eigenvectors are multiplied by their associated eigenvalues. This gives   After a long time so we have .  More generally, we have . As grows large, becomes insignificantly small so that . This means that and so that the ratio .  We see that so that the populations both grow by a factor of approximately 1.3, which is a 30% growth rate.  Now we have . In the same way, when is very large, we have so that and . This gives the same ratio: .    "
},
{
  "id": "fig-eigen-dynam-saddle",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-dynam-saddle",
  "type": "Figure",
  "number": "4.4.1",
  "title": "",
  "body": "    The trajectories of the dynamical system formed by the matrix in the coordinate system defined by , on the left, and in the standard coordinate system, on the right.  "
},
{
  "id": "activity-53",
  "level": "2",
  "url": "sec-dynamical.html#activity-53",
  "type": "Activity",
  "number": "4.4.3",
  "title": "",
  "body": "  We will now look at several more examples of dynamical systems. If , we note that the columns of form a basis of . Given below are several matrices written in the form for some matrix . For each matrix, state the eigenvalues of and sketch a phase portrait for the matrix on the left and a phase portrait for on the right. Describe the behavior of as becomes very large for a typical initial vector .    where .        where .        where .        where .        where .        where .                                          "
},
{
  "id": "fig-eigen-phase-repellor",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-repellor",
  "type": "Figure",
  "number": "4.4.2",
  "title": "",
  "body": "    The origin is a repellor when .  "
},
{
  "id": "fig-eigen-phase-saddle",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-saddle",
  "type": "Figure",
  "number": "4.4.3",
  "title": "",
  "body": "    The origin is a saddle when .  "
},
{
  "id": "fig-eigen-phase-attractor",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-attractor",
  "type": "Figure",
  "number": "4.4.4",
  "title": "",
  "body": "    The origin is an attractor when .  "
},
{
  "id": "fig-eigen-phase-spiralr",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-spiralr",
  "type": "Figure",
  "number": "4.4.5",
  "title": "",
  "body": "    The origin is a spiral repellor when has an eigenvalue with .  "
},
{
  "id": "fig-eigen-phase-center",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-center",
  "type": "Figure",
  "number": "4.4.6",
  "title": "",
  "body": "    The origin is a center when has an eigenvalue with .  "
},
{
  "id": "fig-eigen-phase-spirala",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-phase-spirala",
  "type": "Figure",
  "number": "4.4.7",
  "title": "",
  "body": "    The origin is a spiral attractor when has an eigenvalue with .  "
},
{
  "id": "activity-54",
  "level": "2",
  "url": "sec-dynamical.html#activity-54",
  "type": "Activity",
  "number": "4.4.4",
  "title": "",
  "body": "  In this activity, we will consider several ways in which two species might interact with one another. Throughout, we will consider two species and whose populations in year form a vector and which evolve according to the rule .   Suppose that .  Explain why the species do not interact with one another. Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose now that .  Explain why is a beneficial species for . Which of the six types of dynamical systems do we have? What happens to both species after a long time?    If , explain why this describes a predator-prey system. Which of the species is the predator and which is the prey? Which of the six types of dynamical systems do we have? What happens to both species after a long time?    Suppose that . Compare this predator-prey system to the one in the previous part. Which of the six types of dynamical systems do we have? What happens to both species after a long time?        With this matrix , we have This shows that the population of one species does not depend on the other. This dynamical system is a saddle because we have the eigenvalues and . Eventually, species will become extinct while species grows by a factor of 1.6.  Now we have The population of species is not influenced by species . However, we see that the population of species grows in the presence of species . In other words, species helps species to grow so we say that is beneficial for .  Because the eigenvalues are and , this dynamical system is again a saddle. The associated eigenvectors are and . After a long time, the population vector so both populations grow by a factor of 1.6 and with a ratio   Here we have Species helps species to grow, while species inhibits the growth of species . One explanation for this is that species preys on species as a food source.  We have eigenvectors with associated eigenvalue and with . This dynamical system is a repellor so both species will grow arbitrarily large.  This example is similar to the previous one, but the coefficients are slightly different. We see that the growth rate of both species is smaller. For instance, in the previous problem, we had while we now have . This says that the reproduction rate of species has decreased from to . In the same way, that of species has decreased from to . Also, so the presence of species is less beneficial to species .  We now have the eigenvalues and , which means that this dynamical system is an attractor and that both species will become extinct.    "
},
{
  "id": "fig-eigen-3d-saddle",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-3d-saddle",
  "type": "Figure",
  "number": "4.4.8",
  "title": "",
  "body": "     In a system with , , and , the trajectories move along the curves shown above.  "
},
{
  "id": "fig-eigen-3d-spiral",
  "level": "2",
  "url": "sec-dynamical.html#fig-eigen-3d-spiral",
  "type": "Figure",
  "number": "4.4.9",
  "title": "",
  "body": "     In a system with complex eigenvalues with and , the trajectories move along the curves shown above.  "
},
{
  "id": "activity-55",
  "level": "2",
  "url": "sec-dynamical.html#activity-55",
  "type": "Activity",
  "number": "4.4.5",
  "title": "",
  "body": "  The following type of analysis has been used to study the population of a bison herd. We will divide the population of female bison into three groups: juveniles who are less than one year old; yearlings between one and two years old; and adults who are older than two years.   Each year,  80% of the juveniles survive to become yearlings.  90% of the yearlings survive to become adults.  80% of the adults survive.  40% of the adults give birth to a juvenile.     By , , and , we denote the number of juveniles, yearlings, and adults in year . We have .  Find similar expressions for and in terms of , , and .  As is usual, we write the matrix . Write the matrix such that and find its eigenvalues.  We can write where the matrices and are approximately: Make a prediction about the long-term behavior of . For instance, at what rate does it grow? For every 100 adults, how many juveniles, and yearlings are there?  Suppose that the birth rate decreases so that only 30% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Suppose that the birth rate decreases further so that only 20% of adults give birth to a juvenile. How does this affect the long-term growth rate of the herd?   Find the smallest birth rate that supports a stable population.      We have the relationships   The matrix .  There is a real eigenvalue , which is larger than . The other eigenvalues are complex and satisfy . Therefore, the complex eigenvalues will pull vectors in toward the line defined by the eigenvector . After a long time, the population is . All the populations grow annually by a factor of or 5.8%, and there are approximately 38 juveniles and 29 yearlings for every 100 adults.  We now have eigenvalues and . This shows that the growth rate is lowered to or % annually.  The first eigenvalue is so the growth rate is about %. In other words, the population decreases every year.  To be stable, we need the first eigenvalue . If we experiment with different birth rates, we see that a birth rate of about 0.278 gives this eigenvalue.    "
},
{
  "id": "exercise-159",
  "level": "2",
  "url": "sec-dynamical.html#exercise-159",
  "type": "Exercise",
  "number": "4.4.5.1",
  "title": "",
  "body": " For each of the matrices below, find the eigenvalues and, when appropriate, the eigenvectors to classify the dynamical system . Use this information to sketch the phase portraits.   .   .   .   .     Repellor  Spiral repellor  Saddle  Attractor     There are two real eigenvalues and with associated eigenvectors and . This dynamical system is a repellor so trajectories are pushed away from the origin.     This is a spiral repellor because there are complex eigenvalues whose length is greater than one. Trajectories will spiral away from the origin.     This is a saddle because there are two real eigenvalues and with associated eigenvectors and . Most trajectories will move away from the origin along the line defined by the eigenvector .     This is an attractor because there are two real eigenvalues and with associated eigenvectors and . Trajectories will be pulled in toward the origin.      "
},
{
  "id": "exercise-160",
  "level": "2",
  "url": "sec-dynamical.html#exercise-160",
  "type": "Exercise",
  "number": "4.4.5.2",
  "title": "",
  "body": " We will consider matrices that have the form where where is a parameter that we will vary. Sketch phase portraits for and below when   .      .      .     For the different values of , determine which types of dynamical system results. For what range of values do we have an attractor? For what range of values do we have a saddle? For what value does the transition between the two types occur?    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .    The phase portraits are as shown.  When .     When .     When .     If , we have an attractor. If , we have a saddle. The transition between the two types occurs at .   "
},
{
  "id": "exercise-161",
  "level": "2",
  "url": "sec-dynamical.html#exercise-161",
  "type": "Exercise",
  "number": "4.4.5.3",
  "title": "",
  "body": " Suppose that the populations of two species interact according to the relationships where is a parameter. As we saw in the text, this dynamical system represents a typical predator-prey relationship, and the parameter represents the rate at which species preys on . We will denote the matrix .  If , determine the eigenvectors and eigenvalues of the system and classify it as one of the six types. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     If , determine the eigenvectors and eigenvalues of the system. Sketch the phase portraits for the diagonal matrix to which is similar as well as the phase portrait for .     For what values of is the origin a saddle? What can you say about the populations when this happens?  Describe the evolution of the dynamical system as begins at and increases to .      and       and         As grows, there needs to be a sufficient population to ensure the survival of both species.     If , the eigenvectors are with associated eigenvalue and with eigenvalue . The phase portraits are as     With , we have eigenvectors with eigenvalue and with eigenvalue . The phase portraits appear as     We find the characteristic polynomial of to be which has roots So long as , one root will be real and larger than . We have a saddle when This shows that we have a saddle whenever .  When , we have a saddle. Notice that will go extinct if the initial population of is zero. As we increase , we still have a saddle, but will now go extinct when the initial population is positive. Eventually, when , we see that will go extinct as long as .  As we increase the predation rate, the population of is lowered by . However, is necessary to 's survival. When the predation rate grows too large, there needs to be a sufficiently large initial population to support both species.    "
},
{
  "id": "exercise-162",
  "level": "2",
  "url": "sec-dynamical.html#exercise-162",
  "type": "Exercise",
  "number": "4.4.5.4",
  "title": "",
  "body": " Consider the matrices   Find the eigenvalues of . To which of the six types does the system belong?   Using the eigenvalues of , we can write for some matrices and . What is the matrix and what geometric effect does multiplication by have on vectors in the plane?  If we remember that , determine the smallest positive value of for which .  Find the eigenvalues of .  Find a matrix such that for some matrix . What geometric effect does multiplication by have on vectors in the plane?  Determine the smallest positive value of for which .     This dynamical system is a center.   .   .   .      .     The eigenvalues are so this dynamical system is a center.  The matrix is , which is a rotation.  If we rotate by four times, we obtain the identity. Therefore, .  Here, we find the eigenvalues .  The matrix is , which is a rotation.  If we rotate by six times, we obtain the identity. Therefore, .   "
},
{
  "id": "exercise-163",
  "level": "2",
  "url": "sec-dynamical.html#exercise-163",
  "type": "Exercise",
  "number": "4.4.5.5",
  "title": "",
  "body": " Suppose we have the female population of a species is divided into juveniles, yearlings, and adults and that each year  90% of the juveniles live to be yearlings.  80% of the yearlings live to be adults.  60% of the adults survive to the next year.  50% of the adults give birth to a juvenile.      Set up a system of the form that describes this situation.  Find the eigenvalues of the matrix .   What prediction can you make about these populations after a very long time?  If the birth rate goes up to 80%, what prediction can you make about these populations after a very long time? For every 100 adults, how many juveniles, and yearlings are there?      If we write , we have    and .  The populations will eventually become extinct.  All the populations grow by %, and there are about juveniles and yearlings for every adults.     If we write , we have   We find the eigenvalues and .  All three eigenvalues have a length less than one. Therefore, the populations will eventually become extinct.  With a birth rate of %, the first eigenvalue goes up to , which means that, eventually, all the populations grow by %. An eigenvector is approximately so there are about juveniles and yearlings for every adults.   "
},
{
  "id": "exercise-164",
  "level": "2",
  "url": "sec-dynamical.html#exercise-164",
  "type": "Exercise",
  "number": "4.4.5.6",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification for your response. In each case, we are considering a dynamical system of the form .  If the matrix has a complex eigenvalue, we cannot make a prediction about the behavior of the trajectories.  If has eigenvalues whose absolute value is smaller than 1, then all the trajectories are pulled in toward the origin.  If the origin is a repellor, then it is an attractor for the system .  If a matrix has complex eigenvalues , , , and , all of which satisfy , then all the trajectories are pushed away from the origin.  If the origin is a saddle, then all the trajectories are pushed away from the origin.     False  True  True  True  False     False. We know that the dynamical system is either a spiral repellor, center, or spiral attractor.  True. In this case, we have either an attractor or spiral attractor.  True. If is an attractor, then all the eigenvalues . The eigenvalues for are , which satisfy .  True. We can decompose into two two-dimensional subspaces on which acts as a spiral repellor.  False. There are trajectories along the line defined by the eigenvector associated to the smaller eigenvalue that are pulled in toward the origin.   "
},
{
  "id": "exercise-165",
  "level": "2",
  "url": "sec-dynamical.html#exercise-165",
  "type": "Exercise",
  "number": "4.4.5.7",
  "title": "",
  "body": " The Fibonacci numbers form the sequence of numbers that begins . If we let denote the Fibonacci number, then . In general, a Fibonacci number is the sum of the previous two Fibonacci numbers; that is, so that we have     If we write , find the matrix such that .  Show that has eigenvalues with associated eigenvectors and .  Classify this dynamical system as one of the six types that we have seen in this section. What happens to as becomes very large?  Write the initial vector as a linear combination of eigenvectors and .  Write the vector as a linear combinations of and .  Explain why the Fibonacci number   Use this relationship to compute .  Explain why when is very large.    The number is called the golden ratio and is one of mathematics' special numbers.       Find the roots of the characteristic polynomial.   defines a saddle.   .  We have .  From the second component, we see that      For large values of , we have .     We have   The characteristic equation is Applying the quadratic formula, we see that the two eigenvalues are and .  We find the eigenvectors by row reducing the matrices .   Because and , this is a saddle.  We find .  We have   From the second component, we see that   Using this relationship, we compute that .  For large values of , we have , which says that and . This gives the ratio .   "
},
{
  "id": "exercise-166",
  "level": "2",
  "url": "sec-dynamical.html#exercise-166",
  "type": "Exercise",
  "number": "4.4.5.8",
  "title": "",
  "body": " This exercise is a continuation of the previous one.  The Lucas numbers are defined by the same relationship as the Fibonacci numbers: . However, we begin with and , which leads to the sequence .  As before, form the vector so that . Express as a linear combination of and , eigenvectors of .  Explain why   Explain why is the closest integer to when is large, where is the golden ratio.  Use this observation to find .       .     Because , larger powers are very close to zero.   .     Constructing the augmented matrix and row reducing shows that .  Then we have . Since is the second component of , we have .  Because , larger powers are very close to zero. Therefore, .  We compute that so that .   "
},
{
  "id": "exercise-167",
  "level": "2",
  "url": "sec-dynamical.html#exercise-167",
  "type": "Exercise",
  "number": "4.4.5.9",
  "title": "",
  "body": " Gil Strang defines the Gibonacci numbers  as follows. We begin with and . A subsequent Gibonacci number is the average of the two previous; that is, . We then have   If , find the matrix such that .  Find the eigenvalues and associated eigenvectors of .  Explain why this dynamical system does not neatly fit into one of the six types that we saw in this section.  Write as a linear combination of eigenvectors of .  Write as a linear combination of eigenvectors of .  What happens to as becomes very large?       The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.         stabilizes at .     If , then we have .  The eigenvalues are and with associated eigenvectors and .  We have one eigenvalue , which means this system does not fall into any of our cases.  After constructing an augmented matrix and row reducing, we find .  We have .  As becomes large, the coefficient becomes very small. Therefore, . This shows that stabilizes at .   "
},
{
  "id": "exercise-168",
  "level": "2",
  "url": "sec-dynamical.html#exercise-168",
  "type": "Exercise",
  "number": "4.4.5.10",
  "title": "",
  "body": " Consider a small rodent that lives for three years. Once again, we can separate a population of females into juveniles, yearlings, and adults. Suppose that, each year,  Half of the juveniles live to be yearlings.  One quarter of the yearlings live to be adults.  Adult females produce eight female offspring.  None of the adults survive to the next year.      Writing the populations of juveniles, yearlings, and adults in year using the vector , find the matrix such that .  Show that .  What are the eigenvalues of ? What does this say about the eigenvalues of ?  Verify your observation by finding the eigenvalues of .   What can you say about the trajectories of this dynamical system?  What does this mean about the population of rodents?  Find a population vector that is unchanged from year to year.       .  We can use Sage to compute that .  Any eigenvalue of must satisfy .   and   The trajectories lie on circles about the line defined by .  After every three years, the populations return to their initial values.       We have where .  We can use Sage to compute that .  The only eigenvalues of are so any eigenvalue of must satisfy .  Sage tells us that the eigenvalues are and .  The eigenvector satisfies so it is unchanged. The complex eigenvalues cause a rotation about the axis defined by . Therefore, the trajectories lie on circles about this line.  After every three years, the populations return to their initial values.  If , we know that so this population vector is unchanged.   "
},
{
  "id": "sec-stochastic",
  "level": "1",
  "url": "sec-stochastic.html",
  "type": "Section",
  "number": "4.5",
  "title": "Markov chains and Google’s PageRank algorithm",
  "body": " Markov chains and Google's PageRank algorithm   In the last section, we used our understanding of eigenvalues and eigenvectors to describe the long-term behavior of some discrete dynamical systems. The state of the system, which could record, say, the populations of a few interacting species, at one time is described by a vector . The state vector then evolves according to a linear rule .  This section continues this exploration by looking at Markov chains , which form a specific type of discrete dynamical system. For instance, we could be interested in a rental car company that rents cars from several locations. From one day to the next, the number of cars at different locations can change, but the total number of cars stays the same. Once again, an understanding of eigenvalues and eigenvectors will help us make predictions about the long-term behavior of the system.    Suppose that our rental car company rents from two locations and . We find that 80% of the cars rented from location are returned to while the other 20% are returned to . For cars rented from location , 60% are returned to and 40% to .  We will use and to denote the number of cars at the two locations on day . The following day, the number of cars at equals 80% of and 40% of . This shows that   If we use the vector to represent the distribution of cars on day , find a matrix such that .  Find the eigenvalues and associated eigenvectors of .  Suppose that there are initially 1500 cars, all of which are at location . Write the vector as a linear combination of eigenvectors of .  Write the vectors as a linear combination of eigenvectors of .  What happens to the distribution of cars after a long time?      Expressing the set of equations in matrix form, we see that .  We have eigenvalues and with associated eigenvectors and .  We find that .  Multiplying by the matrix has the effect of multiplying the eigenvectors by their associated eigenvalues. Therefore, .  As becomes large, becomes very close to zero. Therefore . This tells us that cars are at location and are at .       A first example  In the preview activity, the distribution of rental cars was described by the discrete dynamical system . This matrix has some special properties. First, each entry represents the probability that a car rented at one location is returned to another. For instance, there is an 80% chance that a car rented at is returned to , which explains the entry of 0.8 in the upper left corner. Therefore, the entries of the matrix are between 0 and 1.  Second, a car rented at one location must be returned to one of the locations. For example, since 80% of the cars rented at are returned to , it follows that the other 20% of cars rented at are returned to . This implies that the entries in each column must add to 1. This will occur frequently in our discussion so we introduce the following definitions.   probability vector  stochastic matrix  A vector whose entries are nonnegative and add to 1 is called a probability vector . A square matrix whose columns are probability vectors is called a stochastic matrix.     Suppose you live in a country with three political parties , , and . We use , , and to denote the percentage of voters voting for that party in election .   Voters will change parties from one election to the next as shown in the figure. We see that 60% of voters stay with the same party. However, 40% of those who vote for party will vote for party in the next election.      Write expressions for , , and in terms of , , and .  If we write , find the matrix such that .  Explain why is a stochastic matrix.  Suppose that initially 40% of citizens vote for party , 30% vote for party , and 30% vote for party . Form the vector and explain why is a probability vector.  Find , the percentages who vote for the three parties in the next election. Verify that is also a probability vector and explain why will be a probability vector for every .   Find the eigenvalues of the matrix and explain why the eigenspace is a one-dimensional subspace of . Then verify that is a basis vector for .  As every vector in is a scalar multiple of , find a probability vector in and explain why it is the only probability vector in .  Describe what happens to after a very long time.      The solutions to this activity are given in the following text.    The previous activity illustrates some important points that we wish to emphasize.  First, to determine , we note that in election , party retains 60% of its voters from the previous election and adds 20% of those who voted for party . In this way, we see that We therefore define the matrix and note that .  If we consider the first column of , we see that the entries represent the percentages of party 's voters in the last election who vote for each of the three parties in the next election. Since everyone who voted for party previously votes for one of the three parties in the next election, the sum of these percentages must be 1. This is true for each of the columns of , which explains why is a stochastic matrix.  We begin with the vector , the entries of which represent the percentage of voters voting for each of the three parties. Since every voter votes for one of the three parties, the sum of these entries must be 1, which means that is a probability vector. We then find that . Notice that the vectors are also probability vectors and that the sequence seems to be converging to . It is this behavior that we would like to understand more fully by investigating the eigenvalues and eigenvectors of .  We find that the eigenvalues of are . Notice that if is an eigenvector of with associated eigenvalue , then . That is, is unchanged when we multiply it by .   Otherwise, we have where Notice that so the trajectories spiral into the eigenspace as indicated in the figure.    This tells us that the sequence converges to a vector in . In the usual way, we see that is a basis vector for because so we expect that will converge to a scalar multiple of . Indeed, since the vectors are probability vectors, we expect them to converge to a probability vector in .  We can find the probability vector in by finding the appropriate scalar multiple of . Notice that is a probability vector when , which implies that . Therefore, is the unique probability vector in . Since the sequence converges to a probability vector in , we see that converges to , which agrees with the computations we showed above.  The role of the eigenvalues is important in this example. Since , we can find a probability vector that is unchanged by multiplication by . Also, the other eigenvalues satisfy , which means that all the trajectories get pulled in to the eigenspace . Since is a sequence of probability vectors, these vectors converge to the probability vector as they are pulled into .    Markov chains  If we have a stochastic matrix and a probability vector , we can form the sequence where . We call this sequence of vectors a Markov chain . explains why we can guarantee that the vectors are probability vectors. Markov chain   In the example that studied voting patterns, we constructed a Markov chain that described how the percentages of voters choosing different parties changed from one election to the next. We saw that the Markov chain converges to , a probability vector in the eigenspace . In other words, is a probability vector that is unchanged under multiplication by ; that is, . This implies that, after a long time, 20% of voters choose party , 40% choose , and 40% choose .   steady-state vector  stationary vector  If is a stochastic matrix, we say that a probability vector is a steady-state or stationary vector if .   An important question that arises from our previous example is   If is a stochastic matrix and a Markov chain, does converge to a steady-state vector?     Consider the matrices .  Verify that both and are stochastic matrices.  Find the eigenvalues of and then find a steady-state vector for .  We will form the Markov chain beginning with the vector and defining . The Sage cell below constructs the first terms of the Markov chain with the command markov_chain(A, x0, N) . Define the matrix A and vector x0 and evaluate the cell to find the first 10 terms of the Markov chain. What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  Now find the eigenvalues of along with a steady-state vector for .  As before, find the first 10 terms in the Markov chain beginning with and . What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  What condition on the eigenvalues of a stochastic matrix will guarantee that a Markov chain will converge to a steady-state vector?      If we add the entries in each column of and each column of , we obtain . Also, all the entries in both matrices are nonnegative.  The matrix has the eigenvalues and with associated eigenvectors and . The steady-state vector is as this is the unique probability vector in .  The terms in the Markov chain are so the chain does not converge to any vector, much less the steady-state vector.  The matrix has eigenvalues and with associated eigenvectors and . The unique steady-state vector is since this is the only probability vector in .  Here we find which appears to be converging to the steady-state vector .  If there is one eigenvalue having multiplicity one with the other eigenvalues satisfying , we can guarantee that any Markov chain will converge to a unique steady-state vector.     As this activity implies, the eigenvalues of a stochastic matrix tell us whether a Markov chain will converge to a steady-state vector. Here are a few important facts about the eigenvalues of a stochastic matrix.  As is demonstrated in , is an eigenvalue of any stochastic matrix. We usually order the eigenvalues so it is the first eigenvalue meaning that .   All other eigenvalues satisfy the property that .  Any stochastic matrix has at least one steady-state vector .   As illustrated in the activity, a Markov chain could fail to converge to a steady-state vector if . This happens for the matrix , whose eigenvalues are and .  However, if all but the first eigenvalue satisfy , then there is a unique steady-state vector and any Markov chain will converge to . This was the case for the matrix , whose eigenvalues are and . In this case, any Markov chain will converge to the unique steady-state vector .  In this way, we see that the eigenvalues of a stochastic matrix tell us whether a Markov chain will converge to a steady-state vector. However, it is somewhat inconvenient to compute the eigenvalues to answer this question. Is there some way to conclude that every Markov chain will converge to a steady-state vector without actually computing the eigenvalues? It turns out that there is a simple condition on the matrix that guarantees this.   positive matrix  We say that a matrix is positive if either or some power has all positive entries.     The matrix is not positive. We can see this because some of the entries of are zero and therefore not positive. In addition, we see that , and so forth. Therefore, every power of also has some zero entries, which means that is not positive.  The matrix is positive because every entry of is positive.  Also, the matrix clearly has a zero entry. However, , which has all positive entries. Therefore, we see that is a positive matrix.    Positive matrices are important because of the following theorem.   Perron-Frobenius   If is a positive stochastic matrix, then the eigenvalues satisfy and for . This means that has a unique positive, steady-state vector and that every Markov chain defined by will converge to .      We will explore the meaning of the Perron-Frobenius theorem in this activity.  Consider the matrix . This is a positive matrix, as we saw in the previous example. Find the eigenvectors of and verify there is a unique steady-state vector.  Using the Sage cell below, construct the Markov chain with initial vector and describe what happens to as becomes large.   Construct another Markov chain with initial vector and describe what happens to as becomes large.  Consider the matrix and compute several powers of below. Determine whether is a positive matrix.  Find the eigenvalues of and then find the steady-state vectors. Is there a unique steady-state vector?  What happens to the Markov chain defined by with initial vector ? What happens to the Markov chain with initial vector .  Explain how the matrices and , which we have considered in this activity, relate to the Perron-Frobenius theorem.      We find that has eigenvalues and with eigenvectors and . Therefore, the unique steady-state vector is for this is the only probability vector in the eigenspace .  We see that the Markov chain converges to the steady-state vector as the Perron-Frobenius theorem tells us to expect.  Another Markov chain converges to the unique steady-state vector as the Perron-Frobenius theorem tells us to expect.  The matrix is not positive because the first two entries in the bottom row of any power are zero.  The eigenvalues are , which has multiplicity two, and . The eigenspace is two-dimensional and spanned by the probability vectors and . Both of these vectors are steady-state vectors so there is not a unique steady-state vector.  If , then the Markov chain converges to . If , then the Markov chain converges to .  Because is a positive matrix, the Perron-Frobenius theorem tells us that there is a unique steady-state vector to which any Markov chain will converge. Because is not a positive matrix, the Perron-Frobenius theorem does not tell us anything, and, indeed, we see that there is not a unique steady-state vector and different Markov chains can converge to different vectors.       Google's PageRank algorithm  Markov chains and the Perron-Frobenius theorem are the central ingredients in Google's PageRank algorithm, developed by Google to assess the quality of web pages.  Suppose we enter linear algebra into Google's search engine. Google responds by telling us there are 138 million web pages containing those terms. On the first page, however, there are links to ten web pages that Google judges to have the highest quality and to be the ones we are most likely to be interested in. How does Google assess the quality of web pages?  At the time this is being written, Google is tracking 35 trillion web pages. Clearly, this is too many for humans to evaluate. Plus, human evaluators may inject their own biases into their evaluations, perhaps even unintentionally. Google's idea is to use the structure of the Internet to assess the quality of web pages without any human intervention. For instance, if a web page has quality content, other web pages will link to it. This means that the number of links to a page reflect the quality of that page. In addition, we would expect a page to have even higher quality content if those links are coming from pages that are themselves assessed to have high quality. Simply said, if many quality pages link to a page, that page must itself be of high quality. This is the essence of the PageRank algorithm, which we introduce in the next activity.     We will consider a simple model of the Internet that has three pages and links between them as shown here. For instance, page 1 links to both pages 2 and 3, but page 2 only links to page 1.    Our first Internet.    We will measure the quality of the page with a number , which is called the PageRank of page . The PageRank is determined by the following rule: each page divides its PageRank into equal pieces, one for each outgoing link, and gives one piece to each of the pages it links to. A page's PageRank is the sum of all the PageRank it receives from pages linking to it.  For instance, page 3 has two outgoing links. It therefore divides its PageRank in half and gives half to page 1. Page 2 has only one outgoing link so it gives all of its PageRank to page 1. We therefore have .    Find similar expressions for and .  We now form the PageRank vector . Find a matrix such that the expressions for , , and can be written in the form . The matrix is called the Google matrix .  Explain why is a stochastic matrix.  Since is defined by the equation , any vector in the eigenspace satisfies this equation. So that we might work with a specific vector, we will define the PageRank vector to be the steady-state vector of the stochastic matrix . Find this steady state vector.   The PageRank vector is composed of the PageRanks for each of the three pages. Which page of the three is assessed to have the highest quality? By referring to the structure of this small model of the Internet, explain why this is a good choice.  If we begin with the initial vector and form the Markov chain , what does the Perron-Frobenius theorem tell us about the long-term behavior of the Markov chain?  Verify that this Markov chain converges to the steady-state PageRank vector.        Notice that page receives half of the PageRank from pages and . This means that .  Also, page receives half of the PageRank from page and none from page so we have .  This means that   From the equations, we see that .  All of the entries of are nonnegative and the columns each add to . This tells us that is stochastic.  We see that is an eigenvalue and that the eigenspace is spanned by . The unique steady-state vector is then .  Page is assessed to have the highest quality because its PageRank is the largest. This makes sense as a reflection of the structure of this Internet. Since Page only links to Page and not Page , it is not as useful as Page . Furthermore, Page only has one incoming link so it is not viewed by Page as being useful.  Since all the entries of are positive, we can conclude that is a positive matrix. The Perron-Frobenius theorem tells us there is a unique steady-state vector, which we have already seen, and that any Markov chain will converge to it.  We verify that the Markov chain converges to the unique steady-state vector .     This activity shows us two ways to find the PageRank vector. In the first, we determine a steady-state vector directly by finding a description of the eigenspace and then finding the appropriate scalar multiple of a basis vector that gives us the steady-state vector. To find a description of the eigenspace , however, we need to find the null space . Remember that the real Internet has 35 trillion pages so finding requires us to row reduce a matrix with 35 trillion rows and columns. As we saw in , that is not computationally feasible.  As suggested by the activity, the second way to find the PageRank vector is to use a Markov chain that converges to the PageRank vector. Since multiplying a vector by a matrix is significantly less work than row reducing the matrix, this approach is computationally feasible, and it is, in fact, how Google computes the PageRank vector.    Consider the Internet with eight web pages, shown in .      A simple model of the Internet with eight web pages.     Construct the Google matrix for this Internet. Then use a Markov chain to find the steady-state PageRank vector .   What does this vector tell us about the relative quality of the pages in this Internet? Which page has the highest quality and which the lowest?  Now consider the Internet with five pages, shown in .      A model of the Internet with five web pages.   What happens when you begin the Markov chain with the vector ? Explain why this behavior is consistent with the Perron-Frobenius theorem.  What do you think the PageRank vector for this Internet should be? Is any one page of a higher quality than another?  Now consider the Internet with eight web pages, shown in .      Another model of the Internet with eight web pages.   Notice that this version of the Internet is identical to the first one that we saw in this activity, except that a single link from page 7 to page 1 has been removed. We can therefore find its Google matrix by slightly modifying the earlier matrix.  What is the long-term behavior of a Markov chain defined by and why is this behavior not desirable? How is this behavior consistent with the Perron-Frobenius theorem?       After creating a Markov chain to find the steady-state vector, we find   This shows us that page is judged to be the most important and page the least important. The pages , , and , are the most important. This makes sense because there are a lot of links between these pages without many links going out from these pages to the others.  We have the matrix which leads to the Markov chain This Markov chain does not converge to a steady-state vector.  As all of the pages seem equally important, we should expect the PageRank vector to be   After generating some terms of a Markov chain, we see that These are not the components of a steady-state vector because some of the entries are zero. The Google matrix cannot be a positive matrix in this example. This is not desirable because we lose any information about the importance of the first four pages. All the PageRank drains out of the left side into the pages on the right side.     The Perron-Frobenius theorem tells us that a Markov chain converges to a unique steady-state vector when the matrix is positive. This means that or some power of should have only positive entries. Clearly, this is not the case for the matrix formed from the Internet in .  We can understand the problem with the Internet shown in by adding a box around some of the pages as shown in . Here we see that the pages outside of the box give up all of their PageRank to the pages inside the box. This is not desirable because the PageRanks of the pages outside of the box are found to be zero. Once again, the Google matrix is not a positive matrix.      The pages outside the box give up all of their PageRank to the pages inside the box.   Google solves this problem by slightly modifying the Google matrix to obtain a positive matrix . To understand this, think of the entries in the Google matrix as giving the probability that an Internet user follows a link from one page of another. To create a positive matrix, we will allow that user to randomly jump to any other page on the Internet with a small probability.  To make sense of this, suppose that there are pages on our internet. The matrix is a positive stochastic matrix describing a process where we can move from any page to another with equal probability. To form the modified Google matrix , we choose a parameter that is used to mix and together; that is, is the positive stochastic matrix . In practice, it is thought that Google uses a value of (Google doesn't publish this number as it is a trade secret) so that we have . Intuitively, this means that an Internet user will randomly follow a link from one page to another 85% of the time and will randomly jump to any other page on the Internet 15% of the time. Since the matrix is positive, the Perron-Frobenius theorem tells us that any Markov chain will converge to a unique steady-state vector that we call the PageRank vector.    The following Sage cell will generate the Markov chain for the modified Google matrix if you simply enter the original Google matrix in the appropriate line.   Consider the original Internet with three pages shown in and find the PageRank vector using the modified Google matrix in the Sage cell above. How does this modified PageRank vector compare to the vector we found using the original Google matrix ?  Find the modified PageRank vector for the Internet shown in . Explain why this vector seems to be the correct one.  Find the modified PageRank vector for the Internet shown in . Explain why this modified PageRank vector fixes the problem that appeared with the original PageRank vector.      The modified Google matrix has the steady-state vector , which compares to , the steady-state vector of the original Google matrix. These vectors are quite close so it appears that the modification does not change the PageRank significantly.  We see that the Markov chain generated by the modified Google matrix converges to the steady-state vector , which seems correct as all five pages should have the same PageRank.  We find that Now the entries are all positive so we can make assessments about the relative quality of all the pages. Due to the presence of the matrix in the modified Google matrix, the PageRank is allowed to flow back from the four pages on the right to the four pages on the left.     The ability to access almost anything we want to know through the Internet is something we take for granted in today's society. Without Google's PageRank algorithm, however, the Internet would be a chaotic place indeed; imagine trying to find a useful web page among the 30 trillion available pages without it. (There are, of course, other search algorithms, but Google's is the most widely used.) The fundamental role that Markov chains and the Perron-Frobenius theorem play in Google's algorithm demonstrates the vast power that mathematics has to shape our society.    Summary  This section explored stochastic matrices and Markov chains.  A probability vector is one whose entries are nonnegative and whose columns add to 1. A stochastic matrix is a square matrix whose columns are probability vectors.  A Markov chain is formed from a stochastic matrix and an initial probability vector using the rule . We may think of the sequence as describing the evolution of some conserved quantity, such as the number of rental cars or voters, among a number of possible states over time.  A steady-state vector for a stochastic matrix is a probability vector that satisfies .  The Perron-Frobenius theorem tells us that, if is a positive stochastic matrix, then every Markov chain defined by converges to a unique, positive steady-state vector.  Google's PageRank algorithm uses Markov chains and the Perron-Frobenius theorem to assess the relative quality of web pages on the Internet.      Consider the following stochastic matrices.   For each, make a copy of the diagram and label each edge to indicate the probability of that transition. Then find all the steady-state vectors and describe what happens to a Markov chain defined by that matrix.       .   .   .   .      Any Markov chain will converge to .  Any Markov chain will converge to .  The steady-state vectors are those for which for some satisfying .  Any Markov chain will converge to .     The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  Every two-dimensional vector is an eigenvector with eigenvalue so that for every . This shows that any Markov chain will remain constant. The steady-state vectors are those for which for some satisfying .  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.     Every year, people move between urban (U), suburban (S), and rural (R) populations with the probabilities given in .      The flow between urban, suburban, and rural populations.     Construct the stochastic matrix describing the movement of people.  Explain what the Perron-Frobenius theorem tells us about the existence of a steady-state vector and the behavior of a Markov chain.  Use the Sage cell below to find the some terms of a Markov chain.   Describe the long-term distribution of people among urban, suburban, and rural populations.       .  There is a unique steady-state vector and any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.     The diagram tells us that giving the stochastic matrix .  Since all the entries of are positive, we know that is a positive matrix. Therefore, the Perron-Frobenius theorem tells us that there is a unique steady-state vector and that any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.     Determine whether the following statements are true or false and provide a justification of your response.  Every stochastic matrix has a steady-state vector.  If is a stochastic matrix, then any Markov chain defined by converges to a steady-state vector.  If is a stochastic matrix, then is an eigenvalue and all the other eigenvalues satisfy .  A positive stochastic matrix has a unique steady-state vector.  If is an invertible stochastic matrix, then so is .      True  False  False  True  False     True. Every stochastic matrix has an eigenvalue , and we can find a steady-state vector inside the eigenspace .  False. We have seen examples, such as , for which this is not true. We need to know that is a positive matrix so that the Perron-Frobenius theorem applies if we want to reach this conclusion.   False. Again, we can only guarantee this if the matrix is positive so that the Perron-Frobenius theorem applies.  True. This follows from the Perron-Frobenius theorem.  False. If is an invertible stochastic matrix, the eigenvalues of must satisfy . The eigenvalues of are , which satisfy so is most likely not stochastic.     Consider the stochastic matrix .  Find the eigenvalues of .   Do the conditions of the Perron-Frobenius theorem apply to this matrix?  Find the steady-state vectors of .  What can we guarantee about the long-term behavior of a Markov chain defined by the matrix ?     The eigenvalues are , , and with associated eigenvectors , , and .  No     Any Markov chain will converge to .     The eigenvalues are , , and with associated eigenvectors , , and .  The conditions of the Perron-Frobenius theorem do not apply because the matrix is not positive. The first column of any power of will be .  Since has multiplicity , we know that is one-dimensional. There is, therefore, a unique steady-state vector .  Any Markov chain will converge to because the eigenvalues and are less than .      Explain your responses to the following.  Why does Google use a Markov chain to compute the PageRank vector?  Describe two problems that can happen when Google constructs a Markov chain using the Google matrix .  Describe how these problems are consistent with the Perron-Frobenius theorem.  Describe why the Perron-Frobenius theorem suggests creating a Markov chain using the modified Google matrix .     It is not computationally feasible.  A Markov chain does not converge or it may converge to a probability vector having some zero entries.  In both of these situations, the Google matrix is not positive.   is guaranteed to be positive.     The modified Google matrix is huge, roughly 30 trillion 30 trillion, so it is not computationally feasible to compute the steady-state vector by finding a basis for .  It may happen that a Markov chain does not converge. We saw this with the cyclic web where a Markov chain just moved PageRank from one page to the next around a loop. Also, it may happen that a Markov chain converges to a probability vector with some zero entries. This happens when PageRank drains out of one part of the web.  In both of these situations, the Google matrix is not positive so the Perron-Frobenius theorem does not apply.  When we create in this way, we are guaranteed of having a positive stochastic matrix because all the entries of are positive.    In the next few exercises, we will consider the matrix .   Suppose that is a stochastic matrix and that is a probability vector. We would like to explain why the product is a probability vector.  Explain why is a probability vector and then find the product .  More generally, if is any probability vector, what is the product ?  If is a stochastic matrix, explain why .  Explain why is a probability vector by considering the product .      .  If is a probability vector, .  The entries in are the sums of the columns of , which are all .       Since the components of are nonnegative and add to , is a probability vector. We see that .  Multiplying any vector by just adds the components of . Therefore, if is a probability vector, .  Because the columns of add to , we see that multiplying any column of gives . Consequently, the entries in are the sums of the columns of , which are all . This gives .  Because all the entries in both and are nonnegative, it follows that the components of are nonnegative. Then we have , which shows that the components of add to . Therefore, is a probability vector.     Using the results of the previous exercise, we would like to explain why is a stochastic matrix if is stochastic.  Suppose that and are stochastic matrices. Explain why the product is a stochastic matrix by considering the product .  Explain why is a stochastic matrix.  How do the steady-state vectors of compare to the steady-state vectors of ?       This follows from the previous part.  A steady-state vector for is a steady-state vector for but a steady-state vector for is not necessarily a steady-state vector for .     If both and are stochastic, all the entries of both and are nonnegative. Therefore, the entries of are also nonnegative. In addition, we have , which shows that is stochastic.  This follows from the previous part, which shows that the product of two stochastic matrices is stochastic.  If is a steady-state vector for , it will be a steady-state vector for as well because .  However, it is possible that there are steady-state vectors of that are not steady-state vectors of . If the eigenvalues of are , remember that the eigenvalues of are . Therefore, can have more steady-state vectors if is an eigenvalue of , which leads to . For instance, has a unique steady-state vector whereas every probability vector is a steady-state vector for .     This exercise explains why is an eigenvalue of a stochastic matrix . To conclude that is an eigenvalue, we need to know that is not invertible.  What is the product ?  What is the product ?  Consider the equation . Explain why this equation cannot be consistent by multiplying by to obtain .  What can you say about the span of the columns of ?  Explain why we can conclude that is not invertible and that is an eigenvalue of .         .   while .  The span of the columns of is not .  The span of the columns of is not , we know that is not invertible.     Notice that both and are stochastic. Therefore, .   .  We have This says that the original equation is not consistent.  The span of the columns of is not .  Because the span of the columns of is not , has a row without a pivot position, which says that is not invertible. Therefore, , which says that is an eigenvalue of .     We saw a couple of model Internets in which a Markov chain defined by the Google matrix did not converge to an appropriate PageRank vector. For this reason, Google defines the matrix , where is the number of web pages, and constructs a Markov chain from the modified Google matrix . Since is positive, the Markov chain is guaranteed to converge to a unique steady-state vector.  We said that Google chooses so we might wonder why this is a good choice. We will explore the role of in this exercise. Let's consider the model Internet described in and construct the Google matrix . In the Sage cell below, you can enter the matrix and choose a value for .   Let's begin with . With this choice, what is the matrix ? Construct a Markov chain using the Sage cell above. How many steps are required for the Markov chain to converge to the accuracy with which the vectors are displayed?  Now choose . How many steps are required for the Markov chain to converge to the accuracy at which the vectors are displayed?  Repeat this experiment with and .  What happens if ?   This experiment gives some insight into the choice of . The smaller is, the faster the Markov chain converges. This is important; since the matrix that Google works with is so large, we would like to minimize the number of terms in the Markov chain that we need to compute. On the other hand, as we lower , the matrix begins to resemble more and less. The value is chosen so that the matrix sufficiently resembles while having the Markov chain converge in a reasonable amount of steps.     If , then and we see that any Markov chain converges to the steady-state vector in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.    Remember that the steady-state vector for is .  If , then and we see that any Markov chain converges to in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.     This exercise will analyze the board game Chutes and Ladders , or at least a simplified version of it.   The board for this game consists of 100 squares arranged in a grid and numbered 1 to 100. There are pairs of squares joined by a ladder and pairs joined by a chute. All players begin in square 1 and take turns rolling a die. On their turn, a player will move ahead the number of squares indicated on the die. If they arrive at a square at the bottom of a ladder, they move to the square at the top of the ladder. If they arrive at a square at the top of a chute, they move down to the square at the bottom of the chute. The winner is the first player to reach square 100.      We begin by playing a simpler version of this game with only eight squares laid out in a row as shown in and containing neither chutes nor ladders. Rather than a six-sided die, we will toss a coin and move ahead one or two squares depending on the result of the coin toss. If we are on square 7, we move ahead to square 8 regardless of the coin flip, and if we are on square 8, we will stay there forever.      A simple version of Chutes and Ladders with neither chutes nor ladders.   Construct the matrix that records the probability that a player moves from one square to another on one move. For instance, if a player is on square 2, there is a 50% chance they move to square 3 and a 50% chance they move to square 4 on the next move.  Since we begin the game on square 1, the initial vector . Generate a few terms of the Markov chain .   What is the probability that we arrive at square 8 by the fourth move? By the sixth move? By the seventh move?   We will now modify the game by adding one chute and one ladder as shown in .      A version of Chutes and Ladders with one chute and one ladder.   Even though there are eight squares, we only need to consider six of them. For instance, if we arrive at the first white square, we move up to square 4. Similarly, if we arrive at the second white square, we move down to square 1.  Once again, construct the stochastic matrix that records the probability that we move from one square to another on a given turn and generate some terms in the Markov chain that begins with .     What is the smallest number of moves we can make and arrive at square 6? What is the probability that we arrive at square 6 using this number of moves?  What is the probability that we arrive at square 6 after five moves?  What is the probability that we are still on square 1 after five moves? After seven moves? After nine moves?  After how many moves do we have a 90% chance of having arrived at square 6?  Find the steady-state vector and discuss what this vector implies about the game.       One can analyze the full version of Chutes and Ladders having 100 squares in the same way. Without any chutes or ladders, one finds that the average number of moves required to reach square 100 is 29.0. Once we add the chutes and ladders back in, the average number of moves required to reach square 100 is 27.1. This shows that the average number of moves does not change significantly when we add the chutes and ladders. There is, however, much more variation in the possibilities because it is possible to reach square 100 much more quickly and much more slowly.    The chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  We find that  We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .          We have the matrix Starting a Markov chain with , we find that This says that the chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  Now we have the matrix which leads to the Markov chain with terms   We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .  The steady-state vector is , which means that we will eventually arrive at square .       "
},
{
  "id": "exploration-18",
  "level": "2",
  "url": "sec-stochastic.html#exploration-18",
  "type": "Preview Activity",
  "number": "4.5.1",
  "title": "",
  "body": "  Suppose that our rental car company rents from two locations and . We find that 80% of the cars rented from location are returned to while the other 20% are returned to . For cars rented from location , 60% are returned to and 40% to .  We will use and to denote the number of cars at the two locations on day . The following day, the number of cars at equals 80% of and 40% of . This shows that   If we use the vector to represent the distribution of cars on day , find a matrix such that .  Find the eigenvalues and associated eigenvectors of .  Suppose that there are initially 1500 cars, all of which are at location . Write the vector as a linear combination of eigenvectors of .  Write the vectors as a linear combination of eigenvectors of .  What happens to the distribution of cars after a long time?      Expressing the set of equations in matrix form, we see that .  We have eigenvalues and with associated eigenvectors and .  We find that .  Multiplying by the matrix has the effect of multiplying the eigenvectors by their associated eigenvalues. Therefore, .  As becomes large, becomes very close to zero. Therefore . This tells us that cars are at location and are at .    "
},
{
  "id": "definition-24",
  "level": "2",
  "url": "sec-stochastic.html#definition-24",
  "type": "Definition",
  "number": "4.5.1",
  "title": "",
  "body": " probability vector  stochastic matrix  A vector whose entries are nonnegative and add to 1 is called a probability vector . A square matrix whose columns are probability vectors is called a stochastic matrix.  "
},
{
  "id": "activity-56",
  "level": "2",
  "url": "sec-stochastic.html#activity-56",
  "type": "Activity",
  "number": "4.5.2",
  "title": "",
  "body": "  Suppose you live in a country with three political parties , , and . We use , , and to denote the percentage of voters voting for that party in election .   Voters will change parties from one election to the next as shown in the figure. We see that 60% of voters stay with the same party. However, 40% of those who vote for party will vote for party in the next election.      Write expressions for , , and in terms of , , and .  If we write , find the matrix such that .  Explain why is a stochastic matrix.  Suppose that initially 40% of citizens vote for party , 30% vote for party , and 30% vote for party . Form the vector and explain why is a probability vector.  Find , the percentages who vote for the three parties in the next election. Verify that is also a probability vector and explain why will be a probability vector for every .   Find the eigenvalues of the matrix and explain why the eigenspace is a one-dimensional subspace of . Then verify that is a basis vector for .  As every vector in is a scalar multiple of , find a probability vector in and explain why it is the only probability vector in .  Describe what happens to after a very long time.      The solutions to this activity are given in the following text.   "
},
{
  "id": "definition-25",
  "level": "2",
  "url": "sec-stochastic.html#definition-25",
  "type": "Definition",
  "number": "4.5.2",
  "title": "",
  "body": " steady-state vector  stationary vector  If is a stochastic matrix, we say that a probability vector is a steady-state or stationary vector if .  "
},
{
  "id": "question-3",
  "level": "2",
  "url": "sec-stochastic.html#question-3",
  "type": "Question",
  "number": "4.5.3",
  "title": "",
  "body": " If is a stochastic matrix and a Markov chain, does converge to a steady-state vector?  "
},
{
  "id": "activity-57",
  "level": "2",
  "url": "sec-stochastic.html#activity-57",
  "type": "Activity",
  "number": "4.5.3",
  "title": "",
  "body": "  Consider the matrices .  Verify that both and are stochastic matrices.  Find the eigenvalues of and then find a steady-state vector for .  We will form the Markov chain beginning with the vector and defining . The Sage cell below constructs the first terms of the Markov chain with the command markov_chain(A, x0, N) . Define the matrix A and vector x0 and evaluate the cell to find the first 10 terms of the Markov chain. What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  Now find the eigenvalues of along with a steady-state vector for .  As before, find the first 10 terms in the Markov chain beginning with and . What do you notice about the Markov chain? Does it converge to the steady-state vector for ?  What condition on the eigenvalues of a stochastic matrix will guarantee that a Markov chain will converge to a steady-state vector?      If we add the entries in each column of and each column of , we obtain . Also, all the entries in both matrices are nonnegative.  The matrix has the eigenvalues and with associated eigenvectors and . The steady-state vector is as this is the unique probability vector in .  The terms in the Markov chain are so the chain does not converge to any vector, much less the steady-state vector.  The matrix has eigenvalues and with associated eigenvectors and . The unique steady-state vector is since this is the only probability vector in .  Here we find which appears to be converging to the steady-state vector .  If there is one eigenvalue having multiplicity one with the other eigenvalues satisfying , we can guarantee that any Markov chain will converge to a unique steady-state vector.    "
},
{
  "id": "definition-26",
  "level": "2",
  "url": "sec-stochastic.html#definition-26",
  "type": "Definition",
  "number": "4.5.4",
  "title": "",
  "body": " positive matrix  We say that a matrix is positive if either or some power has all positive entries.  "
},
{
  "id": "example-48",
  "level": "2",
  "url": "sec-stochastic.html#example-48",
  "type": "Example",
  "number": "4.5.5",
  "title": "",
  "body": "  The matrix is not positive. We can see this because some of the entries of are zero and therefore not positive. In addition, we see that , and so forth. Therefore, every power of also has some zero entries, which means that is not positive.  The matrix is positive because every entry of is positive.  Also, the matrix clearly has a zero entry. However, , which has all positive entries. Therefore, we see that is a positive matrix.   "
},
{
  "id": "theorem-perron",
  "level": "2",
  "url": "sec-stochastic.html#theorem-perron",
  "type": "Theorem",
  "number": "4.5.6",
  "title": "Perron-Frobenius.",
  "body": " Perron-Frobenius   If is a positive stochastic matrix, then the eigenvalues satisfy and for . This means that has a unique positive, steady-state vector and that every Markov chain defined by will converge to .   "
},
{
  "id": "activity-58",
  "level": "2",
  "url": "sec-stochastic.html#activity-58",
  "type": "Activity",
  "number": "4.5.4",
  "title": "",
  "body": "  We will explore the meaning of the Perron-Frobenius theorem in this activity.  Consider the matrix . This is a positive matrix, as we saw in the previous example. Find the eigenvectors of and verify there is a unique steady-state vector.  Using the Sage cell below, construct the Markov chain with initial vector and describe what happens to as becomes large.   Construct another Markov chain with initial vector and describe what happens to as becomes large.  Consider the matrix and compute several powers of below. Determine whether is a positive matrix.  Find the eigenvalues of and then find the steady-state vectors. Is there a unique steady-state vector?  What happens to the Markov chain defined by with initial vector ? What happens to the Markov chain with initial vector .  Explain how the matrices and , which we have considered in this activity, relate to the Perron-Frobenius theorem.      We find that has eigenvalues and with eigenvectors and . Therefore, the unique steady-state vector is for this is the only probability vector in the eigenspace .  We see that the Markov chain converges to the steady-state vector as the Perron-Frobenius theorem tells us to expect.  Another Markov chain converges to the unique steady-state vector as the Perron-Frobenius theorem tells us to expect.  The matrix is not positive because the first two entries in the bottom row of any power are zero.  The eigenvalues are , which has multiplicity two, and . The eigenspace is two-dimensional and spanned by the probability vectors and . Both of these vectors are steady-state vectors so there is not a unique steady-state vector.  If , then the Markov chain converges to . If , then the Markov chain converges to .  Because is a positive matrix, the Perron-Frobenius theorem tells us that there is a unique steady-state vector to which any Markov chain will converge. Because is not a positive matrix, the Perron-Frobenius theorem does not tell us anything, and, indeed, we see that there is not a unique steady-state vector and different Markov chains can converge to different vectors.    "
},
{
  "id": "activity-59",
  "level": "2",
  "url": "sec-stochastic.html#activity-59",
  "type": "Activity",
  "number": "4.5.5",
  "title": "",
  "body": "   We will consider a simple model of the Internet that has three pages and links between them as shown here. For instance, page 1 links to both pages 2 and 3, but page 2 only links to page 1.    Our first Internet.    We will measure the quality of the page with a number , which is called the PageRank of page . The PageRank is determined by the following rule: each page divides its PageRank into equal pieces, one for each outgoing link, and gives one piece to each of the pages it links to. A page's PageRank is the sum of all the PageRank it receives from pages linking to it.  For instance, page 3 has two outgoing links. It therefore divides its PageRank in half and gives half to page 1. Page 2 has only one outgoing link so it gives all of its PageRank to page 1. We therefore have .    Find similar expressions for and .  We now form the PageRank vector . Find a matrix such that the expressions for , , and can be written in the form . The matrix is called the Google matrix .  Explain why is a stochastic matrix.  Since is defined by the equation , any vector in the eigenspace satisfies this equation. So that we might work with a specific vector, we will define the PageRank vector to be the steady-state vector of the stochastic matrix . Find this steady state vector.   The PageRank vector is composed of the PageRanks for each of the three pages. Which page of the three is assessed to have the highest quality? By referring to the structure of this small model of the Internet, explain why this is a good choice.  If we begin with the initial vector and form the Markov chain , what does the Perron-Frobenius theorem tell us about the long-term behavior of the Markov chain?  Verify that this Markov chain converges to the steady-state PageRank vector.        Notice that page receives half of the PageRank from pages and . This means that .  Also, page receives half of the PageRank from page and none from page so we have .  This means that   From the equations, we see that .  All of the entries of are nonnegative and the columns each add to . This tells us that is stochastic.  We see that is an eigenvalue and that the eigenspace is spanned by . The unique steady-state vector is then .  Page is assessed to have the highest quality because its PageRank is the largest. This makes sense as a reflection of the structure of this Internet. Since Page only links to Page and not Page , it is not as useful as Page . Furthermore, Page only has one incoming link so it is not viewed by Page as being useful.  Since all the entries of are positive, we can conclude that is a positive matrix. The Perron-Frobenius theorem tells us there is a unique steady-state vector, which we have already seen, and that any Markov chain will converge to it.  We verify that the Markov chain converges to the unique steady-state vector .    "
},
{
  "id": "activity-60",
  "level": "2",
  "url": "sec-stochastic.html#activity-60",
  "type": "Activity",
  "number": "4.5.6",
  "title": "",
  "body": "  Consider the Internet with eight web pages, shown in .      A simple model of the Internet with eight web pages.     Construct the Google matrix for this Internet. Then use a Markov chain to find the steady-state PageRank vector .   What does this vector tell us about the relative quality of the pages in this Internet? Which page has the highest quality and which the lowest?  Now consider the Internet with five pages, shown in .      A model of the Internet with five web pages.   What happens when you begin the Markov chain with the vector ? Explain why this behavior is consistent with the Perron-Frobenius theorem.  What do you think the PageRank vector for this Internet should be? Is any one page of a higher quality than another?  Now consider the Internet with eight web pages, shown in .      Another model of the Internet with eight web pages.   Notice that this version of the Internet is identical to the first one that we saw in this activity, except that a single link from page 7 to page 1 has been removed. We can therefore find its Google matrix by slightly modifying the earlier matrix.  What is the long-term behavior of a Markov chain defined by and why is this behavior not desirable? How is this behavior consistent with the Perron-Frobenius theorem?       After creating a Markov chain to find the steady-state vector, we find   This shows us that page is judged to be the most important and page the least important. The pages , , and , are the most important. This makes sense because there are a lot of links between these pages without many links going out from these pages to the others.  We have the matrix which leads to the Markov chain This Markov chain does not converge to a steady-state vector.  As all of the pages seem equally important, we should expect the PageRank vector to be   After generating some terms of a Markov chain, we see that These are not the components of a steady-state vector because some of the entries are zero. The Google matrix cannot be a positive matrix in this example. This is not desirable because we lose any information about the importance of the first four pages. All the PageRank drains out of the left side into the pages on the right side.    "
},
{
  "id": "fig-google-reducible-box",
  "level": "2",
  "url": "sec-stochastic.html#fig-google-reducible-box",
  "type": "Figure",
  "number": "4.5.11",
  "title": "",
  "body": "    The pages outside the box give up all of their PageRank to the pages inside the box.  "
},
{
  "id": "activity-61",
  "level": "2",
  "url": "sec-stochastic.html#activity-61",
  "type": "Activity",
  "number": "4.5.7",
  "title": "",
  "body": "  The following Sage cell will generate the Markov chain for the modified Google matrix if you simply enter the original Google matrix in the appropriate line.   Consider the original Internet with three pages shown in and find the PageRank vector using the modified Google matrix in the Sage cell above. How does this modified PageRank vector compare to the vector we found using the original Google matrix ?  Find the modified PageRank vector for the Internet shown in . Explain why this vector seems to be the correct one.  Find the modified PageRank vector for the Internet shown in . Explain why this modified PageRank vector fixes the problem that appeared with the original PageRank vector.      The modified Google matrix has the steady-state vector , which compares to , the steady-state vector of the original Google matrix. These vectors are quite close so it appears that the modification does not change the PageRank significantly.  We see that the Markov chain generated by the modified Google matrix converges to the steady-state vector , which seems correct as all five pages should have the same PageRank.  We find that Now the entries are all positive so we can make assessments about the relative quality of all the pages. Due to the presence of the matrix in the modified Google matrix, the PageRank is allowed to flow back from the four pages on the right to the four pages on the left.    "
},
{
  "id": "exercise-169",
  "level": "2",
  "url": "sec-stochastic.html#exercise-169",
  "type": "Exercise",
  "number": "4.5.5.1",
  "title": "",
  "body": " Consider the following stochastic matrices.   For each, make a copy of the diagram and label each edge to indicate the probability of that transition. Then find all the steady-state vectors and describe what happens to a Markov chain defined by that matrix.       .   .   .   .      Any Markov chain will converge to .  Any Markov chain will converge to .  The steady-state vectors are those for which for some satisfying .  Any Markov chain will converge to .     The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.  Every two-dimensional vector is an eigenvector with eigenvalue so that for every . This shows that any Markov chain will remain constant. The steady-state vectors are those for which for some satisfying .  The eigenvalues are and so there is a unique steady-state vector to which any Markov chain will converge.   "
},
{
  "id": "exercise-170",
  "level": "2",
  "url": "sec-stochastic.html#exercise-170",
  "type": "Exercise",
  "number": "4.5.5.2",
  "title": "",
  "body": " Every year, people move between urban (U), suburban (S), and rural (R) populations with the probabilities given in .      The flow between urban, suburban, and rural populations.     Construct the stochastic matrix describing the movement of people.  Explain what the Perron-Frobenius theorem tells us about the existence of a steady-state vector and the behavior of a Markov chain.  Use the Sage cell below to find the some terms of a Markov chain.   Describe the long-term distribution of people among urban, suburban, and rural populations.       .  There is a unique steady-state vector and any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.     The diagram tells us that giving the stochastic matrix .  Since all the entries of are positive, we know that is a positive matrix. Therefore, the Perron-Frobenius theorem tells us that there is a unique steady-state vector and that any Markov chain will converge to .  Any Markov chain we create will converge to .  Eventually, about % of the population will be rural, % will be suburban, and % will be urban.   "
},
{
  "id": "exercise-171",
  "level": "2",
  "url": "sec-stochastic.html#exercise-171",
  "type": "Exercise",
  "number": "4.5.5.3",
  "title": "",
  "body": " Determine whether the following statements are true or false and provide a justification of your response.  Every stochastic matrix has a steady-state vector.  If is a stochastic matrix, then any Markov chain defined by converges to a steady-state vector.  If is a stochastic matrix, then is an eigenvalue and all the other eigenvalues satisfy .  A positive stochastic matrix has a unique steady-state vector.  If is an invertible stochastic matrix, then so is .      True  False  False  True  False     True. Every stochastic matrix has an eigenvalue , and we can find a steady-state vector inside the eigenspace .  False. We have seen examples, such as , for which this is not true. We need to know that is a positive matrix so that the Perron-Frobenius theorem applies if we want to reach this conclusion.   False. Again, we can only guarantee this if the matrix is positive so that the Perron-Frobenius theorem applies.  True. This follows from the Perron-Frobenius theorem.  False. If is an invertible stochastic matrix, the eigenvalues of must satisfy . The eigenvalues of are , which satisfy so is most likely not stochastic.   "
},
{
  "id": "exercise-172",
  "level": "2",
  "url": "sec-stochastic.html#exercise-172",
  "type": "Exercise",
  "number": "4.5.5.4",
  "title": "",
  "body": " Consider the stochastic matrix .  Find the eigenvalues of .   Do the conditions of the Perron-Frobenius theorem apply to this matrix?  Find the steady-state vectors of .  What can we guarantee about the long-term behavior of a Markov chain defined by the matrix ?     The eigenvalues are , , and with associated eigenvectors , , and .  No     Any Markov chain will converge to .     The eigenvalues are , , and with associated eigenvectors , , and .  The conditions of the Perron-Frobenius theorem do not apply because the matrix is not positive. The first column of any power of will be .  Since has multiplicity , we know that is one-dimensional. There is, therefore, a unique steady-state vector .  Any Markov chain will converge to because the eigenvalues and are less than .   "
},
{
  "id": "exercise-173",
  "level": "2",
  "url": "sec-stochastic.html#exercise-173",
  "type": "Exercise",
  "number": "4.5.5.5",
  "title": "",
  "body": " Explain your responses to the following.  Why does Google use a Markov chain to compute the PageRank vector?  Describe two problems that can happen when Google constructs a Markov chain using the Google matrix .  Describe how these problems are consistent with the Perron-Frobenius theorem.  Describe why the Perron-Frobenius theorem suggests creating a Markov chain using the modified Google matrix .     It is not computationally feasible.  A Markov chain does not converge or it may converge to a probability vector having some zero entries.  In both of these situations, the Google matrix is not positive.   is guaranteed to be positive.     The modified Google matrix is huge, roughly 30 trillion 30 trillion, so it is not computationally feasible to compute the steady-state vector by finding a basis for .  It may happen that a Markov chain does not converge. We saw this with the cyclic web where a Markov chain just moved PageRank from one page to the next around a loop. Also, it may happen that a Markov chain converges to a probability vector with some zero entries. This happens when PageRank drains out of one part of the web.  In both of these situations, the Google matrix is not positive so the Perron-Frobenius theorem does not apply.  When we create in this way, we are guaranteed of having a positive stochastic matrix because all the entries of are positive.   "
},
{
  "id": "exercise-stochastic-probability",
  "level": "2",
  "url": "sec-stochastic.html#exercise-stochastic-probability",
  "type": "Exercise",
  "number": "4.5.5.6",
  "title": "",
  "body": " Suppose that is a stochastic matrix and that is a probability vector. We would like to explain why the product is a probability vector.  Explain why is a probability vector and then find the product .  More generally, if is any probability vector, what is the product ?  If is a stochastic matrix, explain why .  Explain why is a probability vector by considering the product .      .  If is a probability vector, .  The entries in are the sums of the columns of , which are all .       Since the components of are nonnegative and add to , is a probability vector. We see that .  Multiplying any vector by just adds the components of . Therefore, if is a probability vector, .  Because the columns of add to , we see that multiplying any column of gives . Consequently, the entries in are the sums of the columns of , which are all . This gives .  Because all the entries in both and are nonnegative, it follows that the components of are nonnegative. Then we have , which shows that the components of add to . Therefore, is a probability vector.   "
},
{
  "id": "exercise-175",
  "level": "2",
  "url": "sec-stochastic.html#exercise-175",
  "type": "Exercise",
  "number": "4.5.5.7",
  "title": "",
  "body": " Using the results of the previous exercise, we would like to explain why is a stochastic matrix if is stochastic.  Suppose that and are stochastic matrices. Explain why the product is a stochastic matrix by considering the product .  Explain why is a stochastic matrix.  How do the steady-state vectors of compare to the steady-state vectors of ?       This follows from the previous part.  A steady-state vector for is a steady-state vector for but a steady-state vector for is not necessarily a steady-state vector for .     If both and are stochastic, all the entries of both and are nonnegative. Therefore, the entries of are also nonnegative. In addition, we have , which shows that is stochastic.  This follows from the previous part, which shows that the product of two stochastic matrices is stochastic.  If is a steady-state vector for , it will be a steady-state vector for as well because .  However, it is possible that there are steady-state vectors of that are not steady-state vectors of . If the eigenvalues of are , remember that the eigenvalues of are . Therefore, can have more steady-state vectors if is an eigenvalue of , which leads to . For instance, has a unique steady-state vector whereas every probability vector is a steady-state vector for .   "
},
{
  "id": "exercise-stochastic-eigenvalue",
  "level": "2",
  "url": "sec-stochastic.html#exercise-stochastic-eigenvalue",
  "type": "Exercise",
  "number": "4.5.5.8",
  "title": "",
  "body": " This exercise explains why is an eigenvalue of a stochastic matrix . To conclude that is an eigenvalue, we need to know that is not invertible.  What is the product ?  What is the product ?  Consider the equation . Explain why this equation cannot be consistent by multiplying by to obtain .  What can you say about the span of the columns of ?  Explain why we can conclude that is not invertible and that is an eigenvalue of .         .   while .  The span of the columns of is not .  The span of the columns of is not , we know that is not invertible.     Notice that both and are stochastic. Therefore, .   .  We have This says that the original equation is not consistent.  The span of the columns of is not .  Because the span of the columns of is not , has a row without a pivot position, which says that is not invertible. Therefore, , which says that is an eigenvalue of .   "
},
{
  "id": "exercise-177",
  "level": "2",
  "url": "sec-stochastic.html#exercise-177",
  "type": "Exercise",
  "number": "4.5.5.9",
  "title": "",
  "body": " We saw a couple of model Internets in which a Markov chain defined by the Google matrix did not converge to an appropriate PageRank vector. For this reason, Google defines the matrix , where is the number of web pages, and constructs a Markov chain from the modified Google matrix . Since is positive, the Markov chain is guaranteed to converge to a unique steady-state vector.  We said that Google chooses so we might wonder why this is a good choice. We will explore the role of in this exercise. Let's consider the model Internet described in and construct the Google matrix . In the Sage cell below, you can enter the matrix and choose a value for .   Let's begin with . With this choice, what is the matrix ? Construct a Markov chain using the Sage cell above. How many steps are required for the Markov chain to converge to the accuracy with which the vectors are displayed?  Now choose . How many steps are required for the Markov chain to converge to the accuracy at which the vectors are displayed?  Repeat this experiment with and .  What happens if ?   This experiment gives some insight into the choice of . The smaller is, the faster the Markov chain converges. This is important; since the matrix that Google works with is so large, we would like to minimize the number of terms in the Markov chain that we need to compute. On the other hand, as we lower , the matrix begins to resemble more and less. The value is chosen so that the matrix sufficiently resembles while having the Markov chain converge in a reasonable amount of steps.     If , then and we see that any Markov chain converges to the steady-state vector in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.    Remember that the steady-state vector for is .  If , then and we see that any Markov chain converges to in one term. That is, .  After six terms, we have to three digits.  With , it is not until to three digits. With , it is not until .  With , we have so the Markov chain will never converge.   "
},
{
  "id": "exercise-178",
  "level": "2",
  "url": "sec-stochastic.html#exercise-178",
  "type": "Exercise",
  "number": "4.5.5.10",
  "title": "",
  "body": " This exercise will analyze the board game Chutes and Ladders , or at least a simplified version of it.   The board for this game consists of 100 squares arranged in a grid and numbered 1 to 100. There are pairs of squares joined by a ladder and pairs joined by a chute. All players begin in square 1 and take turns rolling a die. On their turn, a player will move ahead the number of squares indicated on the die. If they arrive at a square at the bottom of a ladder, they move to the square at the top of the ladder. If they arrive at a square at the top of a chute, they move down to the square at the bottom of the chute. The winner is the first player to reach square 100.      We begin by playing a simpler version of this game with only eight squares laid out in a row as shown in and containing neither chutes nor ladders. Rather than a six-sided die, we will toss a coin and move ahead one or two squares depending on the result of the coin toss. If we are on square 7, we move ahead to square 8 regardless of the coin flip, and if we are on square 8, we will stay there forever.      A simple version of Chutes and Ladders with neither chutes nor ladders.   Construct the matrix that records the probability that a player moves from one square to another on one move. For instance, if a player is on square 2, there is a 50% chance they move to square 3 and a 50% chance they move to square 4 on the next move.  Since we begin the game on square 1, the initial vector . Generate a few terms of the Markov chain .   What is the probability that we arrive at square 8 by the fourth move? By the sixth move? By the seventh move?   We will now modify the game by adding one chute and one ladder as shown in .      A version of Chutes and Ladders with one chute and one ladder.   Even though there are eight squares, we only need to consider six of them. For instance, if we arrive at the first white square, we move up to square 4. Similarly, if we arrive at the second white square, we move down to square 1.  Once again, construct the stochastic matrix that records the probability that we move from one square to another on a given turn and generate some terms in the Markov chain that begins with .     What is the smallest number of moves we can make and arrive at square 6? What is the probability that we arrive at square 6 using this number of moves?  What is the probability that we arrive at square 6 after five moves?  What is the probability that we are still on square 1 after five moves? After seven moves? After nine moves?  After how many moves do we have a 90% chance of having arrived at square 6?  Find the steady-state vector and discuss what this vector implies about the game.       One can analyze the full version of Chutes and Ladders having 100 squares in the same way. Without any chutes or ladders, one finds that the average number of moves required to reach square 100 is 29.0. Once we add the chutes and ladders back in, the average number of moves required to reach square 100 is 27.1. This shows that the average number of moves does not change significantly when we add the chutes and ladders. There is, however, much more variation in the possibilities because it is possible to reach square 100 much more quickly and much more slowly.    The chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  We find that  We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .          We have the matrix Starting a Markov chain with , we find that This says that the chances are % that we arrive at square after four moves and % that we arrive after five moves. We are guaranteed to arrive after seven moves.  Now we have the matrix which leads to the Markov chain with terms   We can first arrive at square after three moves. The chances of doing so are %.  After five moves, there is a % chance that we have arrived at square .  The chances of being on square after five moves is %. After seven moves, it is still %. After nine moves, however, it is %.  After 23 moves, there is a % chance that we have arrived at square .  The steady-state vector is , which means that we will eventually arrive at square .     "
},
{
  "id": "sec-gaussian-revisited",
  "level": "1",
  "url": "sec-gaussian-revisited.html",
  "type": "Section",
  "number": "5.1",
  "title": "Gaussian elimination revisited",
  "body": " Gaussian elimination revisited   In this section, we revisit Gaussian elimination and explore some problems with implementing it in the straightforward way that we described back in . In particular, we will see how the fact that computers only approximate arithmetic operations can lead us to find solutions that are far from the actual solutions. Second, we will explore how much work is required to implement Gaussian elimination and devise a more efficient means of implementing it when we want to solve equations for several different vectors .    To begin, let's recall how we implemented Gaussian elimination by considering the matrix   What is the first row operation we perform? If the resulting matrix is , find a matrix such that .  What is the matrix inverse ? You can find this using your favorite technique for finding a matrix inverse. However, it may be easier to think about the effect that the row operation has and how it can be undone.  Perform the next two steps in the Gaussian elimination algorithm to obtain . Represent these steps using multiplication by matrices and so that .  Suppose we need to scale the second row by . What is the matrix that perfoms this row operation by left multiplication?  Suppose that we need to interchange the first and second rows. What is the matrix that performs this row operation by left multiplication?      We first multiply the first row by and add to the second row. This can be represented by multiplying where   To undo the row operation, we multiply the first row by and add to the second row. This shows that .  We have   The matrix performing this scaling is .  The matrix performing this interchange is .       Partial pivoting  The first issue that we address is the fact that computers do not perform arithemtic operations exactly. For instance, Python will evaluate 0.1 + 0.2 and report 0.30000000000000004 even though we know that the true value is 0.3. There are a couple of reasons for this.  First, computers perform arithmetic using base 2 numbers, which means that numbers we enter in decimal form, such as , must be converted to base 2. Even though 0.1 has a simple decimal form, its representation in base 2 is the repeating decimal , To accurately represent this number inside a computer would require infinitely many digits. Since a computer can only hold a finite number of digits, we are necessarily using an approximation just by representing this number in a computer.  In addition, arithmetic operations, such as addition, are prone to error. To keep things simple, suppose we have a computer that represents numbers using only three decimal digits. For instance, the number 1.023 would be represented as 1.02 while 0.023421 would be 0.0234 . If we add these numbers, we have 1.023 + 0.023421 = 1.046421; the computer reports this sum as 1.02 + 0.0234 = 1.04 , whose last digit is not correctly rounded. Generally speaking, we will see this problem, which is called round off error , whenever we add numbers of signficantly different magnitudes. round off error   Remember that Gaussian elimination, when applied to an matrix, requires approximately operations. If we have a matrix, performing Gaussian elimination requires roughly a billion operations, and the errors introduced in each operation could accumulate. How can we have confidence in the final result? We can never completely avoid these errors, but we can take steps to mitigate them. The next activity will introduce one such technique.    Suppose we have a hypothetical computer that represents numbers using only three decimal digits. We will consider the linear system   Show that this system has the unique solution   If we represent this solution inside our computer that only holds 3 decimal digits, what do we find for the solution? This is the best that we can hope to find using our computer.  Let's imagine that we use our computer to find the solution using Gaussian elimination; that is, after every arithmetic operation, we keep only three decimal digits. Our first step is to multiply the first equation by 10000 and subtract it from the second equation. If we represent numbers using only three decimal digits, what does this give for the value of ?  By substituting our value for into the first equation, what do we find for ?  Compare the solution we find on our computer with the actual solution and assess the quality of the approximation.  Let's now modify the linear system by simplying interchanging the equations: Of course, this doesn't change the actual solution. Let's imagine we use our computer to find the solution using Gaussian elimination. Perform the first step where we multiply the first equation by 0.0001 and subtract from the second equation. What does this give for if we represent numbers using only three decimal digits?  Substitute the value you found for into the first equation and solve for . Then compare the approximate solution found with our hypothetical computer to the exact solution.  Which approach produces the most accurate approximation?      We may find this result by forming the augmented matrix and row reducing.  The solution would be rounded to and .  We first multiply the first row by and add to the second row. This gives This tells us that .  The next steps in the Gaussian elimination algorithm give us so have the approximation solution and .  This compares to the actual solution, as represented in our computer, as and . Notice that the value of differs considerably.  Now we have which gives .  Performing one more row operation gives us which shows the approximate solution as and .  The second approach gives an approximate solution that is as accurate as possible given the computer's limited ability to store digits.     This activity demonstrates how the practical aspects of computing differ from the theoretical. We know that the order in which we write the equations has no effect on the solution space; row interchange is one of our three allowed row operations in the Gaussian elimination algorithm. However, when we are only able to perform arithmetic operations approximately, applying row interchanges can dramatically improve the accuracy of our approximations.  If we could compute the solution exactly, we find Since our hypothetical computer represents numbers using only three decimal digits, our computer finds This is the best we can hope to do with our computer since it is impossible to represent the solution exactly.  When the equations are written in their original order and we multiply the first equation by 10000 and subtract from the second, we find   In fact, we find the same value for when we interchange the equations. Here we multiply the first equation by 0.0001 and subtract from the second equation. We then find   The difference occurs when we substitute into the first equation. When the equations are written in their original order, we have When the equations are written in their original order, we find the solution .  When we write the equation in the opposite order, however, substituting into the first equation gives In this case, we find the approximate solution , which is the most accurate solution that our hypothetical computer can find. Simply interchanging the order of the equation produces a much more accurate solution.   We can understand why this works graphically. Each equation represents a line in the plane, and the solution is the intersection point. Notice that the slopes of these lines differ considerably.    When the equations are written in their original order, we substitute into the equation , which is a nearly horizontal line. Along this line, a small change in leads to a large change in . The slight difference in our approximation from the exact value leads to a large difference in the approximation from the exact value .  If we exchange the order in which the equations are written, we substitute our approximation into the equation . Notice that the slope of the associated line is . On this line, a small change in leads to a relatively small change in as well. Therefore, the difference in our approximation from the exact value leads to only a small difference in the approximation from the exact value.  This example motivates the technique that computers usually use to perform Gaussian elimation. We only need to perform a row interchange when a zero occurs in a pivot position, such as . However, we will perform a row interchange to put the entry having the largest possible absolute value into the pivot position. For instance, when performing Gaussian elimination on the following matrix, we begin by interchanging the first and third rows so that the upper left entry has the largest possible absolute value.  partial pivoting This technique is called partial pivoting , and it means that, in practice, we will perform many more row interchange operations than we typically do when computing exactly by hand.     factorizations  In , we saw that the number of arithmetic operations needed to perform Gaussian elimination on an matrix is about . This means that a matrix, requires about two thirds of a billion operations.  Suppose that we have two equations, and , that we would like to solve. Usually, we would form augmented matrices and and apply Gaussian elimination. Of course, the steps we perform in these two computations are nearly identical. Is there a way to store some of the computation we perform in reducing and reuse it in solving subsequent equations? The next activity will point us in the right direction.    We will consider the matrix and begin performing Gaussian elimination without using partial pivoting.   Perform two row replacement operations to find the row equivalent matrix Find elementary matrices and that perform these two operations so that .  Perform a third row replacement to find the upper triangular matrix Find the elementary matrix such that .  We can write . Find the inverse matrices , , and and the product . Then verify that .   Suppose that we want to solve the equation . We will write and introduce an unknown vector such that . Find by noting that and solving this equation.  Now that we have found , find by solving .  Using the factorization and this two-step process, solve the equation .      We have   The third row replacement is performed by   The inverse matrices are found to be giving so that .  Noting that , we find .  To find , we solve the equation to obtain .  We solve to find and to find .     This activity introduces a method for factoring a matrix as a product of two triangular matrices, , where is lower triangular and is upper triangular. The key to finding this factorization is to represent the row operations that we apply in the Gaussian elimination algorithm through multiplication by elementary matrices.   Suppose we have the equation which we write in the form . We begin by applying the Gaussian elimination algorithm to find an factorization of .  The first step is to multiply the first row of by and add it to the second row. The elementary matrix performs this operation so that .  We next apply matrices to obtain the upper triangular matrix .  We can write , which tells us that That is, we have Notice that the matrix is lower triangular, a result of the fact that the elementary matrices , , and are lower triangular.  Now that we have factored into two triangular matrices, we can solve the equation by solving two triangular systems. We write and define the unknown vector , which is determined by the equation . Because is lower triangular, we find the solution using forward substitution, . Finally, we find , the solution to our original system , by applying back substitution to solve . This gives .  If we want to solve for a different right-hand side , we can simply repeat this two-step process.   An factorization allow us to trade in one equation for two simpler equations For instance, the equation in our example has the form Because is a lower-triangular matrix, we can read off the first component of directly from the equations: . We then have , which gives , and , which gives . Solving a triangular system is simplified because we only need to perform a sequence of substitutions.  In fact, solving an equation with an triangular matrix requires approximately operations. Once we have the factorization , we solve the equation by solving two equations involving triangular matrices, which requires about operations. For example, if is a matrix, we solve the equation using about one million steps. The compares with roughly a billion operations needed to perform Gaussian elimination, which represents a significant savings. Of course, we have to first find the factorization of and this requires roughly the same amount of work as performing Gaussian elimination. However, once we have the factorization, we can use it to solve for different right hand sides .  Our discussion so far has ignored one issue, however. Remember that we sometimes have to perform row interchange operations in addition to row replacement. A typical row interchange is represented by multiplication by a matrix such as which has the effect of interchanging the first and third rows. Notice that this matrix is not triangular so performing a row interchange will disrupt the structure of the factorization we seek. Without giving the details, we simply note that linear algebra software packages provide a matrix that describes how the rows are permuted in the Gaussian elimination process. In particular, we will write , where is a permutation matrix, is lower triangular, and is upper triangular.  Therefore, to solve the equation , we first multiply both sides by to obtain . That is, we multiply by and then find using the factorization: and .    Sage will create factorizations; once we have a matrix A , we write P, L, U = A.LU() to obtain the matrices , , and such that .   In , we found the factorization Using Sage, define the matrix , and then ask Sage for the factorization. What are the matrices , , and ?  Notice that Sage finds a different factorization than we found in the previous activity because Sage uses partial pivoting, as described in the previous section, when it performs Gaussian elimination.   Define the vector in Sage and compute .  Use the matrices L and U to solve and . You should find the same solution that you found in the previous activity.  Use the factorization to solve the equation .  How does the factorization show us that is invertible and that, therefore, every equation has a unique solution?  Suppose that we have the matrix . Use Sage to find the factorization. Explain how the factorization shows that is not invertible.  Consider the matrix and find its factorization. Explain why and have the same null space and use this observation to find a basis for .      Sage gives us the matrices   We find .  Solving , we have . Finally, solving , we obtain .  We find   Because , , and are invertible, it follows that , which means that is invertible.  Sage tells us that Because is not invertible, we see that so is not invertible.  We have We may rewrite as so if , then because and are invertible.  Notice that is upper triangular, which means that it is straightforward to find its reduced row echelon form: This shows that a basis for is .       Summary  We returned to Gaussian elimination, which we have used as a primary tool for finding solutions to linear systems, and explored its practicality, both in terms of numerical accuracy and computational effort.  We saw that the accuracy of computations implemented on a computer could be improved using partial pivoting , a technique that performs row interchanges so that the entry in a pivot position has the largest possible magnitude.  Beginning with a matrix , we used the Gaussian elimination algorithm to write , where is a permutation matrix, is lower triangular, and is upper triangular.  Finding this factorization involves roughly as much work as performing Gaussian elimination. However, once we have the factorization, we are able to quickly solve equations of the form by first solving and then .      In this section, we saw that errors made in computer arithmetic can produce approximate solutions that are far from the exact solutions. Here is another example in which this can happen. Consider the matrix   Find the exact solution to the equation .  Suppose that this linear system arises in the midst of a larger computation except that, due to some error in the computation of the right hand side of the equation, our computer thinks we want to solve . Find the solution to this equation and compare it to the solution of the equation in the previous part of this exericse.    Notice how a small change in the right hand side of the equation leads to a large change in the solution. In this case, we say that the matrix is ill-conditioned because the solutions are extremely sensitive to small changes in the right hand side of the equation. Though we will not do so here, it is possible to create a measure of the matrix that tells us when a matrix is ill-conditioned. Regrettably, there is not much we can do to remedy this problem.     .   .     We find the solution .  Here we find the solution .     In this section, we found the factorization of the matrix in one of the activities, without using partial pivoting. Apply a sequence of row operations, now using partial pivoting, to find an upper triangular matrix that is row equivalent to .       Using partial pivoting, we find This agrees with the result that Sage returns using the LU command.    In the following exercises, use the given factorizations to solve the equations .  Solve the equation   Solve the equation               We solve to find and to find .  We solve to find and to find .     Use Sage to solve the following equation by finding an factorization: .        We obtain the decomposition To solve the equation , we first find , then solve to find , and finally solve to find .    Here is another problem with approximate computer arithmetic that we will encounter in the next section. Consider the matrix   Notice that this is a positive stochastic matrix. What do we know about the eigenvalues of this matrix?  Use Sage to define the matrix using decimals such as 0.2 and the identity matrix . Ask Sage to compute and find the reduced row echelon form of .   Why is the computation that Sage performed incorrect?  Explain why using a computer to find the eigenvectors of a matrix by finding a basis for is problematic.      is an eigenvalue.   .  The computation tells us that is invertible, which is not true.  The approximate arithmetic creates a nonexistent third pivot position.     Since is a positive stochastic matrix, we know that is an eigenvalue. Therefore, should not be invertible.  Sage tells us that the reduced row echelon form of is .  The computation tells us that is invertible. We know that this can't be the case, however, since is an eigenvalue.  Let's trace through the work that goes into finding the reduced row echelon form of : The next step is to multiply the second row by and add it to the third row. The last entry in the third row would be , which we identify as being . The problem is that the computer only approximates this result so it finds something close to but not quite equal to . It therefore, thinks this is a pivot position and scales the row to have in that position.     In practice, one rarely finds the inverse of a matrix . It requires considerable effort to compute, and we can solve any equation of the form using an factorization, which means that the inverse isn't necessary. In any case, the best way to compute an inverse is using an factorization, as this exericse demonstrates.  Suppose that . Explain why .  Since and are triangular, finding their inverses is relatively efficient. That makes this an effective means of finding .   Consider the matrix . Find the factorization of and use it to find .      .    Notice that a row interchange is undone by the same row interchange. Therefore, the permutation matrix is its own inverse so that . If , it then follows that so that .  We find that which gives      Consider the matrix   Find the factorization of .  What conditions on , , , and guarantee that is invertible?     We find    , , , and .     We begin with where Next we have where Finally, we have , an upper triangular matrix where This gives   Notice that is invertible if and only if is invertible and is invertible if and only all the diagonal terms are not equal. This means that is invertible provided that , , , and .     In the factorization of a matrix, the diagonal entries of are all while the diagonal entries of are not necessarily . This exercise will explore that observation by considering the matrix .  Perform Gaussian elimination without partial pivoting to find , an upper triangular matrix that is row equivalent to .  The diagonal entries of are called pivots . Explain why equals the product of the pivots.  What is for our matrix ?  More generally, if we have , explain why equals plus or minus the product of the pivots.      .   .      .     We have This leads to where   Because the diagonal entries of are , we have . Therefore, , which equals the product of the pivots.     More generally, if we have , we can rewrite as so that , which is plus or minus the product of the pivots.     Please provide a justification to your responses to these questions.  In this section, our hypothetical computer could only store numbers using 3 decimal places. Most computers can store numbers using 15 or more decimal places. Why do we still need to be concerned about the accuracy of our computations when solving systems of linear equations?  Finding the factorization of a matrix is roughly the same amount of work as finding its reduced row echelon form. Why is the factorization useful then?  How can we detect whether a matrix is invertible from its factorization?     When working with a large matrix, the errors from individual calculations may accumulate.  We can solve for a couple of different without performing Gaussian elimination again.   is invertible if and only if is.     Since a typical computer has 15 or more decimal places of accuracy, the error in each individual calculation is relatively small. However, if we are working with a large matrix, we are performing many calculations and the errors from each of them may accumulate to become significant.  If we need to solve for a couple of different vectors , we do not have to perform Gaussian elimination again. Some other quantities, like the inverse and determinant of , are more easily computed using an factorization.  The matrix is invertible if and only if is, and we can easily see if is invertible by noting if every row has a pivot.     Consider the matrix   Find the factorization of .  Use the factorization to find a basis for .  We have seen that . Is it true that ?     We see that    .  No     We see that   Because , we see that with a basis vector .  It is not true that . Since there is a nonzero vector such that , we know that is not invertible and hence its column space is not . However, is invertible so we have .     "
},
{
  "id": "exploration-19",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exploration-19",
  "type": "Preview Activity",
  "number": "5.1.1",
  "title": "",
  "body": "  To begin, let's recall how we implemented Gaussian elimination by considering the matrix   What is the first row operation we perform? If the resulting matrix is , find a matrix such that .  What is the matrix inverse ? You can find this using your favorite technique for finding a matrix inverse. However, it may be easier to think about the effect that the row operation has and how it can be undone.  Perform the next two steps in the Gaussian elimination algorithm to obtain . Represent these steps using multiplication by matrices and so that .  Suppose we need to scale the second row by . What is the matrix that perfoms this row operation by left multiplication?  Suppose that we need to interchange the first and second rows. What is the matrix that performs this row operation by left multiplication?      We first multiply the first row by and add to the second row. This can be represented by multiplying where   To undo the row operation, we multiply the first row by and add to the second row. This shows that .  We have   The matrix performing this scaling is .  The matrix performing this interchange is .    "
},
{
  "id": "activity-62",
  "level": "2",
  "url": "sec-gaussian-revisited.html#activity-62",
  "type": "Activity",
  "number": "5.1.2",
  "title": "",
  "body": "  Suppose we have a hypothetical computer that represents numbers using only three decimal digits. We will consider the linear system   Show that this system has the unique solution   If we represent this solution inside our computer that only holds 3 decimal digits, what do we find for the solution? This is the best that we can hope to find using our computer.  Let's imagine that we use our computer to find the solution using Gaussian elimination; that is, after every arithmetic operation, we keep only three decimal digits. Our first step is to multiply the first equation by 10000 and subtract it from the second equation. If we represent numbers using only three decimal digits, what does this give for the value of ?  By substituting our value for into the first equation, what do we find for ?  Compare the solution we find on our computer with the actual solution and assess the quality of the approximation.  Let's now modify the linear system by simplying interchanging the equations: Of course, this doesn't change the actual solution. Let's imagine we use our computer to find the solution using Gaussian elimination. Perform the first step where we multiply the first equation by 0.0001 and subtract from the second equation. What does this give for if we represent numbers using only three decimal digits?  Substitute the value you found for into the first equation and solve for . Then compare the approximate solution found with our hypothetical computer to the exact solution.  Which approach produces the most accurate approximation?      We may find this result by forming the augmented matrix and row reducing.  The solution would be rounded to and .  We first multiply the first row by and add to the second row. This gives This tells us that .  The next steps in the Gaussian elimination algorithm give us so have the approximation solution and .  This compares to the actual solution, as represented in our computer, as and . Notice that the value of differs considerably.  Now we have which gives .  Performing one more row operation gives us which shows the approximate solution as and .  The second approach gives an approximate solution that is as accurate as possible given the computer's limited ability to store digits.    "
},
{
  "id": "activity-63",
  "level": "2",
  "url": "sec-gaussian-revisited.html#activity-63",
  "type": "Activity",
  "number": "5.1.3",
  "title": "",
  "body": "  We will consider the matrix and begin performing Gaussian elimination without using partial pivoting.   Perform two row replacement operations to find the row equivalent matrix Find elementary matrices and that perform these two operations so that .  Perform a third row replacement to find the upper triangular matrix Find the elementary matrix such that .  We can write . Find the inverse matrices , , and and the product . Then verify that .   Suppose that we want to solve the equation . We will write and introduce an unknown vector such that . Find by noting that and solving this equation.  Now that we have found , find by solving .  Using the factorization and this two-step process, solve the equation .      We have   The third row replacement is performed by   The inverse matrices are found to be giving so that .  Noting that , we find .  To find , we solve the equation to obtain .  We solve to find and to find .    "
},
{
  "id": "example-LU",
  "level": "2",
  "url": "sec-gaussian-revisited.html#example-LU",
  "type": "Example",
  "number": "5.1.1",
  "title": "",
  "body": " Suppose we have the equation which we write in the form . We begin by applying the Gaussian elimination algorithm to find an factorization of .  The first step is to multiply the first row of by and add it to the second row. The elementary matrix performs this operation so that .  We next apply matrices to obtain the upper triangular matrix .  We can write , which tells us that That is, we have Notice that the matrix is lower triangular, a result of the fact that the elementary matrices , , and are lower triangular.  Now that we have factored into two triangular matrices, we can solve the equation by solving two triangular systems. We write and define the unknown vector , which is determined by the equation . Because is lower triangular, we find the solution using forward substitution, . Finally, we find , the solution to our original system , by applying back substitution to solve . This gives .  If we want to solve for a different right-hand side , we can simply repeat this two-step process.  "
},
{
  "id": "activity-64",
  "level": "2",
  "url": "sec-gaussian-revisited.html#activity-64",
  "type": "Activity",
  "number": "5.1.4",
  "title": "",
  "body": "  Sage will create factorizations; once we have a matrix A , we write P, L, U = A.LU() to obtain the matrices , , and such that .   In , we found the factorization Using Sage, define the matrix , and then ask Sage for the factorization. What are the matrices , , and ?  Notice that Sage finds a different factorization than we found in the previous activity because Sage uses partial pivoting, as described in the previous section, when it performs Gaussian elimination.   Define the vector in Sage and compute .  Use the matrices L and U to solve and . You should find the same solution that you found in the previous activity.  Use the factorization to solve the equation .  How does the factorization show us that is invertible and that, therefore, every equation has a unique solution?  Suppose that we have the matrix . Use Sage to find the factorization. Explain how the factorization shows that is not invertible.  Consider the matrix and find its factorization. Explain why and have the same null space and use this observation to find a basis for .      Sage gives us the matrices   We find .  Solving , we have . Finally, solving , we obtain .  We find   Because , , and are invertible, it follows that , which means that is invertible.  Sage tells us that Because is not invertible, we see that so is not invertible.  We have We may rewrite as so if , then because and are invertible.  Notice that is upper triangular, which means that it is straightforward to find its reduced row echelon form: This shows that a basis for is .    "
},
{
  "id": "exercise-179",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-179",
  "type": "Exercise",
  "number": "5.1.4.1",
  "title": "",
  "body": " In this section, we saw that errors made in computer arithmetic can produce approximate solutions that are far from the exact solutions. Here is another example in which this can happen. Consider the matrix   Find the exact solution to the equation .  Suppose that this linear system arises in the midst of a larger computation except that, due to some error in the computation of the right hand side of the equation, our computer thinks we want to solve . Find the solution to this equation and compare it to the solution of the equation in the previous part of this exericse.    Notice how a small change in the right hand side of the equation leads to a large change in the solution. In this case, we say that the matrix is ill-conditioned because the solutions are extremely sensitive to small changes in the right hand side of the equation. Though we will not do so here, it is possible to create a measure of the matrix that tells us when a matrix is ill-conditioned. Regrettably, there is not much we can do to remedy this problem.     .   .     We find the solution .  Here we find the solution .   "
},
{
  "id": "exercise-180",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-180",
  "type": "Exercise",
  "number": "5.1.4.2",
  "title": "",
  "body": " In this section, we found the factorization of the matrix in one of the activities, without using partial pivoting. Apply a sequence of row operations, now using partial pivoting, to find an upper triangular matrix that is row equivalent to .       Using partial pivoting, we find This agrees with the result that Sage returns using the LU command.  "
},
{
  "id": "exercise-181",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-181",
  "type": "Exercise",
  "number": "5.1.4.3",
  "title": "",
  "body": " In the following exercises, use the given factorizations to solve the equations .  Solve the equation   Solve the equation               We solve to find and to find .  We solve to find and to find .   "
},
{
  "id": "exercise-182",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-182",
  "type": "Exercise",
  "number": "5.1.4.4",
  "title": "",
  "body": " Use Sage to solve the following equation by finding an factorization: .        We obtain the decomposition To solve the equation , we first find , then solve to find , and finally solve to find .  "
},
{
  "id": "exercise-eigenvector-approx",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-eigenvector-approx",
  "type": "Exercise",
  "number": "5.1.4.5",
  "title": "",
  "body": " Here is another problem with approximate computer arithmetic that we will encounter in the next section. Consider the matrix   Notice that this is a positive stochastic matrix. What do we know about the eigenvalues of this matrix?  Use Sage to define the matrix using decimals such as 0.2 and the identity matrix . Ask Sage to compute and find the reduced row echelon form of .   Why is the computation that Sage performed incorrect?  Explain why using a computer to find the eigenvectors of a matrix by finding a basis for is problematic.      is an eigenvalue.   .  The computation tells us that is invertible, which is not true.  The approximate arithmetic creates a nonexistent third pivot position.     Since is a positive stochastic matrix, we know that is an eigenvalue. Therefore, should not be invertible.  Sage tells us that the reduced row echelon form of is .  The computation tells us that is invertible. We know that this can't be the case, however, since is an eigenvalue.  Let's trace through the work that goes into finding the reduced row echelon form of : The next step is to multiply the second row by and add it to the third row. The last entry in the third row would be , which we identify as being . The problem is that the computer only approximates this result so it finds something close to but not quite equal to . It therefore, thinks this is a pivot position and scales the row to have in that position.   "
},
{
  "id": "exercise-184",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-184",
  "type": "Exercise",
  "number": "5.1.4.6",
  "title": "",
  "body": " In practice, one rarely finds the inverse of a matrix . It requires considerable effort to compute, and we can solve any equation of the form using an factorization, which means that the inverse isn't necessary. In any case, the best way to compute an inverse is using an factorization, as this exericse demonstrates.  Suppose that . Explain why .  Since and are triangular, finding their inverses is relatively efficient. That makes this an effective means of finding .   Consider the matrix . Find the factorization of and use it to find .      .    Notice that a row interchange is undone by the same row interchange. Therefore, the permutation matrix is its own inverse so that . If , it then follows that so that .  We find that which gives    "
},
{
  "id": "exercise-185",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-185",
  "type": "Exercise",
  "number": "5.1.4.7",
  "title": "",
  "body": " Consider the matrix   Find the factorization of .  What conditions on , , , and guarantee that is invertible?     We find    , , , and .     We begin with where Next we have where Finally, we have , an upper triangular matrix where This gives   Notice that is invertible if and only if is invertible and is invertible if and only all the diagonal terms are not equal. This means that is invertible provided that , , , and .   "
},
{
  "id": "exercise-186",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-186",
  "type": "Exercise",
  "number": "5.1.4.8",
  "title": "",
  "body": " In the factorization of a matrix, the diagonal entries of are all while the diagonal entries of are not necessarily . This exercise will explore that observation by considering the matrix .  Perform Gaussian elimination without partial pivoting to find , an upper triangular matrix that is row equivalent to .  The diagonal entries of are called pivots . Explain why equals the product of the pivots.  What is for our matrix ?  More generally, if we have , explain why equals plus or minus the product of the pivots.      .   .      .     We have This leads to where   Because the diagonal entries of are , we have . Therefore, , which equals the product of the pivots.     More generally, if we have , we can rewrite as so that , which is plus or minus the product of the pivots.   "
},
{
  "id": "exercise-187",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-187",
  "type": "Exercise",
  "number": "5.1.4.9",
  "title": "",
  "body": " Please provide a justification to your responses to these questions.  In this section, our hypothetical computer could only store numbers using 3 decimal places. Most computers can store numbers using 15 or more decimal places. Why do we still need to be concerned about the accuracy of our computations when solving systems of linear equations?  Finding the factorization of a matrix is roughly the same amount of work as finding its reduced row echelon form. Why is the factorization useful then?  How can we detect whether a matrix is invertible from its factorization?     When working with a large matrix, the errors from individual calculations may accumulate.  We can solve for a couple of different without performing Gaussian elimination again.   is invertible if and only if is.     Since a typical computer has 15 or more decimal places of accuracy, the error in each individual calculation is relatively small. However, if we are working with a large matrix, we are performing many calculations and the errors from each of them may accumulate to become significant.  If we need to solve for a couple of different vectors , we do not have to perform Gaussian elimination again. Some other quantities, like the inverse and determinant of , are more easily computed using an factorization.  The matrix is invertible if and only if is, and we can easily see if is invertible by noting if every row has a pivot.   "
},
{
  "id": "exercise-188",
  "level": "2",
  "url": "sec-gaussian-revisited.html#exercise-188",
  "type": "Exercise",
  "number": "5.1.4.10",
  "title": "",
  "body": " Consider the matrix   Find the factorization of .  Use the factorization to find a basis for .  We have seen that . Is it true that ?     We see that    .  No     We see that   Because , we see that with a basis vector .  It is not true that . Since there is a nonzero vector such that , we know that is not invertible and hence its column space is not . However, is invertible so we have .   "
},
{
  "id": "sec-power-method",
  "level": "1",
  "url": "sec-power-method.html",
  "type": "Section",
  "number": "5.2",
  "title": "Finding eigenvectors numerically",
  "body": " Finding eigenvectors numerically   We have typically found eigenvalues of a square matrix as the roots of the characteristic polynomial and the associated eigenvectors as the null space . Unfortunately, this approach is not practical when we are working with large matrices. First, finding the charactertic polynomial of a large matrix requires considerable computation, as does finding the roots of that polynomial. Second, finding the null space of a singular matrix is plagued by numerical problems, as we will see in the preview activity.  For this reason, we will explore a technique called the power method that finds numerical approximations to the eigenvalues and eigenvectors of a square matrix.    Let's recall some earlier observations about eigenvalues and eigenvectors.  How are the eigenvalues and associated eigenvectors of related to those of ?  How are the eigenvalues and associated eigenvectors of related to those of ?  If is an eigenvalue of , what can we say about the pivot positions of ?  Suppose that . Explain how we know that is an eigenvalue of and then explain why the following Sage computation is incorrect.   Suppose that , and we define a sequence ; in other words, . What happens to as grows increasingly large?  Explain how the eigenvalues of are responsible for the behavior noted in the previous question.      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  In the same way, if is an eigenvalue of , then for an associated eigenvector . This means that so that is an eigenvalue of .  If is an eigenvalue of , then is not invertible and so has a row without a pivot position.  Since is a positive stochastic matrix, we know that is an eigenvalue and hence that is not invertible. Sage, however, tells us that , which cannot be true since is not invertible.  The vectors form a Markov chain, which must converge to the steady-state vector .  We have eigenvalues and . If we begin with and successively multiply by , we have . When becomes large, the coefficient of becomes insignificantly small so we are left with an eigenvector in .       The power method  Our goal is to find a technique that produces numerical approximations to the eigenvalues and associated eigenvectors of a matrix . We begin by searching for the eigenvalue having the largest absolute value, which is called the dominant eigenvalue. The next two examples demonstrate this technique. eigenvalue, dominant    Let's begin with the positive stochastic matrix . We spent quite a bit of time studying this type of matrix in ; in particular, we saw that any Markov chain will converge to the unique steady-state vector. Let's rephrase this statement in terms of the eigenvectors of .  This matrix has eigenvalues and so the dominant eigenvalue is . The associated eigenvectors are and . Suppose we begin with the vector and find and so forth. Notice that the powers become increasingly small as grows so that when is large. Therefore, the vectors become increasingly close to a vector in the eigenspace , the eigenspace associated to the dominant eigenvalue. If we did not know the eigenvector , we could use a Markov chain in this way to find a basis vector for , which, as seen in , is essentially how the Google PageRank algorithm works.    Let's now look at the matrix , which has eigenvalues and . The dominant eigenvalue is , and the associated eigenvectors are and . Once again, begin with the vector so that    As the figure shows, the vectors are stretched by a factor of in the direction and not at all in the direction. Consequently, the vectors become increasingly long, but their direction becomes closer to the direction of the eigenvector associated to the dominant eigenvalue.    To find an eigenvector associated to the dominant eigenvalue, we will prevent the length of the vectors from growing arbitrarily large by multiplying by an appropriate scaling constant. Here is one way to do this. Given the vector , we identify its component having the largest absolute value and call it . We then define , which means that the component of having the largest absolute value is .  For example, beginning with , we find . The component of having the largest absolute value is so we multiply by to obtain . Then . Now the component having the largest absolute value is so we multiply by to obtain .   The resulting sequence of vectors is shown in the figure. Notice how the vectors now approach the eigenvector , which gives us a way to find the eigenvector . This is the power method for finding an eigenvector associated to the dominant eigenvalue of a matrix. power method        Let's begin by considering the matrix and the initial vector .   Compute the vector .  Find , the component of that has the largest absolute value. Then form . Notice that the component having the largest absolute value of is .  Find the vector . Identify the component of having the largest absolute value. Then form to obtain a vector in which the component with the largest absolute value is .  The Sage cell below defines a function that implements the power method. Define the matrix and initial vector below. The command power(A, x0, N) will print out the multiplier and the vectors for steps of the power method.   How does this computation identify an eigenvector of the matrix ?  What is the corresponding eigenvalue of this eigenvector?  How do the values of the multipliers tell us the eigenvalue associated to the eigenvector we have found?  Consider now the matrix . Use the power method to find the dominant eigenvalue of and an associated eigenvector.      We find .  The first component has the largest absolute value so . Therefore, .  In the same way, we obtain . We see that so we have .  We see that the vectors are getting closer and closer to , which we therefore identify as an eigenvector associated to the dominant eigenvalue.  We see that . Therefore, the dominant eigenvalue is .  More generally, we see that the multiplier will converge to the dominant eigenvalue.  The power method constructs a sequence of vectors converging to an eigenvector . The multipliers converge to , the dominant eigenvalue.     Notice that the power method gives us not only an eigenvector but also its associated eigenvalue. As in the activity, consider the matrix , which has eigenvector . The first component has the largest absolute value so we multiply by to obtain . When we multiply by , we have . Notice that the first component still has the largest absolute value so that the multiplier is the eigenvalue corresponding to the eigenvector. This demonstrates the fact that the multipliers approach the eigenvalue having the largest absolute value.  Notice that the power method requires us to choose an initial vector . For most choices, this method will find the eigenvalue having the largest absolute value. However, an unfortunate choice of may not. For instance, if we had chosen in our example above, the vectors in the sequence will not detect the eigenvector . However, it usually happens that our initial guess has some contribution from that enables us to find it.  The power method, as presented here, will fail for certain unlucky matrices. This is examined in along with a means to improve the power method to work for all matrices.    Finding other eigenvalues  The power method gives a technique for finding the dominant eigenvalue of a matrix. We can modify the method to find the other eigenvalues as well.    The key to finding the eigenvalue of having the smallest absolute value is to note that the eigenvectors of are the same as those of .  If is an eigenvector of with associated eigenvector , explain why is an eigenvector of with associated eigenvalue .  Explain why the eigenvalue of having the smallest absolute value is the reciprocal of the dominant eigenvalue of .  Explain how to use the power method applied to to find the eigenvalue of having the smallest absolute value.  If we apply the power method to , we begin with an intial vector and generate the sequence . It is not computationally efficient to compute , however, so instead we solve the equation . Explain why an factorization of is useful for implementing the power method applied to .  The following Sage cell defines a command called inverse_power that applies the power method to . That is, inverse_power(A, x0, N) prints the vectors , where , and multipliers , which approximate the eigenvalue of . Use it to find the eigenvalue of having the smallest absolute value.   The inverse power method only works if is invertible. If is not invertible, what is its eigenvalue having the smallest absolute value?  Use the power method and the inverse power method to find the eigenvalues and associated eigenvectors of the matrix .      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  If , then . Therefore, the reciprocal of the smallest eigenvalue of is the dominant eigenvalue of .  If we apply the power method to the matrix , we will find the dominant eigenvalue and an associated eigenvector of . We know, however, that will be the eigenvalue of having the smallest absolute value and will be an associated eigenvector.  We would like to solve equations of the form for many different vectors . Using an factorization allows us to recycle for subsequent equations the effort we expend performing Gaussian elimination to solve the first equation.  We obtain the eigenvector and associated eigenvalue .  If is not invertible, then is the eigenvalue having the smallest absolute value.  We find the dominant eigenvalue to be with associated eigenvector . The smallest eigenvalue is with associated eigenvector .     With the power method and the inverse power method, we can now find the eigenvalues of a matrix having the largest and smallest absolute values. With one more modification, we can find all the eigenvalues of .    Remember that the absolute value of a number tells us how far that number is from on the real number line. We may therefore think of the inverse power method as telling us the eigenvalue closest to .  If is an eigenvector of with associated eigenvalue , explain why is an eigenvector of where is some scalar.  What is the eigenvalue of associated to the eigenvector ?  Explain why the eigenvalue of closest to is the eigenvalue of closest to .  Explain why applying the inverse power method to gives the eigenvalue of closest to .  Consider the matrix . If we use the power method and inverse power method, we find two eigenvalues, and . Viewing these eigenvalues on a number line, we know that the other eigenvalues lie in the range between and , as shaded in .      The range of eigenvalues of .   The Sage cell below has a function find_closest_eigenvalue(A, s, x, N) that implements steps of the inverse power method using the matrix and an initial vector . This function prints approximations to the eigenvalue of closest to and its associated eigenvector. By trying different values of in the shaded regions of the number line shown in , find the other two eigenvalues of .   Write a list of the four eigenvalues of in increasing order.      If , then , which shows that is also an eigenvector of .  From the previous part, we see that the associated eigenvalue is .  If is the eigenvalue of closest to , then is an eigenvalue of that must be the closest to .  The inverse power method applied to tells us the eigenvalue of having the smallest absolute value and an associated eigenvector . Therefore, is the eigenvalue of closest to and is an associated eigenvector.  We begin by trying to find the closest eigenvalue to, say, . The power method tells us that this eigenvalue is . If we then try to find the eigenvalue closest to , we find the fourth eigenvalue . It may require some experimentation to find all of the eigenvalues.  The eigenvalues are .     There are some restrictions on the matrices to which this technique applies as we have assumed that the eigenvalues of are real and distinct. If has repeated or complex eigenvalues, this technique will need to be modified, as explored in some of the exercises.    Summary  We have explored the power method as a tool for numerically approximating the eigenvalues and eigenvectors of a matrix.  After choosing an initial vector , we define the sequence . As grows larger, the direction of the vectors closely approximates the direction of the eigenspace corresponding to the eigenvalue having the largest absolute value.  We normalize the vectors by multiplying by , where is the component having the largest absolute value. In this way, the vectors approach an eigenvector associated to , and the multipliers approach the eigenvalue .  To find the eigenvalue having the smallest absolute value, we apply the power method using the matrix .  To find the eigenvalue closest to some number , we apply the power method using the matrix .     This Sage cell has the commands power , inverse_power , and find_closest_eigenvalue that we have developed in this section. After evaluating this cell, these commands will be available in any other cell on this page.    Suppose that is a matrix having eigenvalues , , , and .  What are the eigenvalues of ?  What are the eigenvalues of ?      , , , and .   , , , and .     The eigenvalues of are the reciprocals of the eigenvalues of . They are, therefore, , , , and .  If is an eigenvalue of , then is an eigenvalue of . Therefore, the eigenvalues of are , , , and .     Use the commands power , inverse_power , and find_closest_eigenvalue to approximate the eigenvalues and associated eigenvectors of the following matrices.    .   .   .      and .   and .   , , , and .     The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is . The inverse power method tells us that the eigenvalue having the smallest absolute value is . If we now look for the closest eigenvalue to , we see that it is , which we have already found. Let's try again, this time looking to find the closest eigenvalue to . Here, we find . If we next try to find the eigenvalue closest to , we find it to be .  The four eigenvalues are then , , , and .     Use the techniques we have seen in this section to find the eigenvalues of the matrix .     , , , , and .   The power method shows us that is the dominant eigenvalue. The inverse power method tells us that is the eigenvalue having the smallest absolute value. We then probe in between these values to find eigenvalues , , and .    Consider the matrix .   Describe what happens if we apply the power method and the inverse power method using the initial vector .  Find the eigenvalues of this matrix and explain this observed behavior.  How can we apply the techniques of this section to find the eigenvalues of ?     The methods do not converge.  There is not a unique dominant eigenvalue.  Try finding an eigenvalue closest to, say, .     We see that neither the power method nor the inverse power method converge.  The eigenvalues are and . This means that there is not a unique dominant eigenvalue and there is not a unique eigenvalue with the smallest absolute value. The methods try to find first one of them and then the other.  To break the symmetry, we can look for an eigenvalue closest to, say, . When we do this, we find the eigenvalue . Then look for another eigenvalue closest to to find .     We have seen that the matrix has eigenvalues and and associated eigenvectors and .  Describe what happens when we apply the inverse power method using the initial vector .  Explain why this is happening and provide a contrast with how the power method usually works.  How can we modify the power method to give the dominant eigenvalue in this case?     We see that the vectors do not converge and the multipliers converge to the wrong value.  The multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier .     We see that the vectors do not converge but instead flip between approximations to and . Also, the multipliers are converging to rather than .  When applying the power method, the multipliers are usually formed from the same component of the vectors in every iteration. Here, we see that the multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier . The problem is that you must make sure that this component is not approaching zero.     Suppose that is a matrix with eigenvalues and and that is a matrix with eigenvalues and . If we apply the power method to find the dominant eigenvalue of these matrices to the same degree of accuracy, which matrix will require more steps in the algorithm? Explain your response.   We will need more steps when finding the dominant eigenvalue of .   For both matrices, is the dominant eigenvalue and is the eigenvalue closest to . We will construct the initial vector as a linear combination of eigenvectors: . Then . For the matrix , is relatively small compared to so we expect the contribution from to become smaller more quickly. Therefore, we will need more steps in the power method to find the dominant eigenvalue of .    Suppose that we apply the power method to the matrix with an initial vector and find the eigenvalue and eigenvector . Suppose that we then apply the power method again with a different initial vector and find the same eigenvalue but a different eigenvector . What can we conclude about the matrix in this case?    is the dominant eigenvalue with a multiplicity greater than one.   The dominant eigenvector is but it has a multiplicity greater than one and the associated eigenspace has .    The power method we have developed only works if the matrix has real eigenvalues. Suppose that is a matrix that has a complex eigenvalue . What would happen if we apply the power method to ?   The vectors will not converge.   The power method relies on the fact that the vectors attempt to line up in the direction of a vector in the dominant eigenspace. If the eigenvalues are complex, however, the vectors will be rotated with each iteration so they will not converge.    Consider the matrix .  Find the eigenvalues and associated eigenvectors of .  Make a prediction about what happens if we apply the power method and the inverse power method to find eigenvalues of .  Verify your prediction using Sage.      There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .     There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .     "
},
{
  "id": "exploration-20",
  "level": "2",
  "url": "sec-power-method.html#exploration-20",
  "type": "Preview Activity",
  "number": "5.2.1",
  "title": "",
  "body": "  Let's recall some earlier observations about eigenvalues and eigenvectors.  How are the eigenvalues and associated eigenvectors of related to those of ?  How are the eigenvalues and associated eigenvectors of related to those of ?  If is an eigenvalue of , what can we say about the pivot positions of ?  Suppose that . Explain how we know that is an eigenvalue of and then explain why the following Sage computation is incorrect.   Suppose that , and we define a sequence ; in other words, . What happens to as grows increasingly large?  Explain how the eigenvalues of are responsible for the behavior noted in the previous question.      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  In the same way, if is an eigenvalue of , then for an associated eigenvector . This means that so that is an eigenvalue of .  If is an eigenvalue of , then is not invertible and so has a row without a pivot position.  Since is a positive stochastic matrix, we know that is an eigenvalue and hence that is not invertible. Sage, however, tells us that , which cannot be true since is not invertible.  The vectors form a Markov chain, which must converge to the steady-state vector .  We have eigenvalues and . If we begin with and successively multiply by , we have . When becomes large, the coefficient of becomes insignificantly small so we are left with an eigenvector in .    "
},
{
  "id": "example-50",
  "level": "2",
  "url": "sec-power-method.html#example-50",
  "type": "Example",
  "number": "5.2.1",
  "title": "",
  "body": " Let's begin with the positive stochastic matrix . We spent quite a bit of time studying this type of matrix in ; in particular, we saw that any Markov chain will converge to the unique steady-state vector. Let's rephrase this statement in terms of the eigenvectors of .  This matrix has eigenvalues and so the dominant eigenvalue is . The associated eigenvectors are and . Suppose we begin with the vector and find and so forth. Notice that the powers become increasingly small as grows so that when is large. Therefore, the vectors become increasingly close to a vector in the eigenspace , the eigenspace associated to the dominant eigenvalue. If we did not know the eigenvector , we could use a Markov chain in this way to find a basis vector for , which, as seen in , is essentially how the Google PageRank algorithm works.  "
},
{
  "id": "example-51",
  "level": "2",
  "url": "sec-power-method.html#example-51",
  "type": "Example",
  "number": "5.2.2",
  "title": "",
  "body": " Let's now look at the matrix , which has eigenvalues and . The dominant eigenvalue is , and the associated eigenvectors are and . Once again, begin with the vector so that    As the figure shows, the vectors are stretched by a factor of in the direction and not at all in the direction. Consequently, the vectors become increasingly long, but their direction becomes closer to the direction of the eigenvector associated to the dominant eigenvalue.    To find an eigenvector associated to the dominant eigenvalue, we will prevent the length of the vectors from growing arbitrarily large by multiplying by an appropriate scaling constant. Here is one way to do this. Given the vector , we identify its component having the largest absolute value and call it . We then define , which means that the component of having the largest absolute value is .  For example, beginning with , we find . The component of having the largest absolute value is so we multiply by to obtain . Then . Now the component having the largest absolute value is so we multiply by to obtain .   The resulting sequence of vectors is shown in the figure. Notice how the vectors now approach the eigenvector , which gives us a way to find the eigenvector . This is the power method for finding an eigenvector associated to the dominant eigenvalue of a matrix. power method     "
},
{
  "id": "activity-65",
  "level": "2",
  "url": "sec-power-method.html#activity-65",
  "type": "Activity",
  "number": "5.2.2",
  "title": "",
  "body": "  Let's begin by considering the matrix and the initial vector .   Compute the vector .  Find , the component of that has the largest absolute value. Then form . Notice that the component having the largest absolute value of is .  Find the vector . Identify the component of having the largest absolute value. Then form to obtain a vector in which the component with the largest absolute value is .  The Sage cell below defines a function that implements the power method. Define the matrix and initial vector below. The command power(A, x0, N) will print out the multiplier and the vectors for steps of the power method.   How does this computation identify an eigenvector of the matrix ?  What is the corresponding eigenvalue of this eigenvector?  How do the values of the multipliers tell us the eigenvalue associated to the eigenvector we have found?  Consider now the matrix . Use the power method to find the dominant eigenvalue of and an associated eigenvector.      We find .  The first component has the largest absolute value so . Therefore, .  In the same way, we obtain . We see that so we have .  We see that the vectors are getting closer and closer to , which we therefore identify as an eigenvector associated to the dominant eigenvalue.  We see that . Therefore, the dominant eigenvalue is .  More generally, we see that the multiplier will converge to the dominant eigenvalue.  The power method constructs a sequence of vectors converging to an eigenvector . The multipliers converge to , the dominant eigenvalue.    "
},
{
  "id": "activity-66",
  "level": "2",
  "url": "sec-power-method.html#activity-66",
  "type": "Activity",
  "number": "5.2.3",
  "title": "",
  "body": "  The key to finding the eigenvalue of having the smallest absolute value is to note that the eigenvectors of are the same as those of .  If is an eigenvector of with associated eigenvector , explain why is an eigenvector of with associated eigenvalue .  Explain why the eigenvalue of having the smallest absolute value is the reciprocal of the dominant eigenvalue of .  Explain how to use the power method applied to to find the eigenvalue of having the smallest absolute value.  If we apply the power method to , we begin with an intial vector and generate the sequence . It is not computationally efficient to compute , however, so instead we solve the equation . Explain why an factorization of is useful for implementing the power method applied to .  The following Sage cell defines a command called inverse_power that applies the power method to . That is, inverse_power(A, x0, N) prints the vectors , where , and multipliers , which approximate the eigenvalue of . Use it to find the eigenvalue of having the smallest absolute value.   The inverse power method only works if is invertible. If is not invertible, what is its eigenvalue having the smallest absolute value?  Use the power method and the inverse power method to find the eigenvalues and associated eigenvectors of the matrix .      If is an eigenvalue of , then for an associated eigenvector . Multiplying by and , we obtain , which shows that is an eigenvalue of .  If , then . Therefore, the reciprocal of the smallest eigenvalue of is the dominant eigenvalue of .  If we apply the power method to the matrix , we will find the dominant eigenvalue and an associated eigenvector of . We know, however, that will be the eigenvalue of having the smallest absolute value and will be an associated eigenvector.  We would like to solve equations of the form for many different vectors . Using an factorization allows us to recycle for subsequent equations the effort we expend performing Gaussian elimination to solve the first equation.  We obtain the eigenvector and associated eigenvalue .  If is not invertible, then is the eigenvalue having the smallest absolute value.  We find the dominant eigenvalue to be with associated eigenvector . The smallest eigenvalue is with associated eigenvector .    "
},
{
  "id": "activity-67",
  "level": "2",
  "url": "sec-power-method.html#activity-67",
  "type": "Activity",
  "number": "5.2.4",
  "title": "",
  "body": "  Remember that the absolute value of a number tells us how far that number is from on the real number line. We may therefore think of the inverse power method as telling us the eigenvalue closest to .  If is an eigenvector of with associated eigenvalue , explain why is an eigenvector of where is some scalar.  What is the eigenvalue of associated to the eigenvector ?  Explain why the eigenvalue of closest to is the eigenvalue of closest to .  Explain why applying the inverse power method to gives the eigenvalue of closest to .  Consider the matrix . If we use the power method and inverse power method, we find two eigenvalues, and . Viewing these eigenvalues on a number line, we know that the other eigenvalues lie in the range between and , as shaded in .      The range of eigenvalues of .   The Sage cell below has a function find_closest_eigenvalue(A, s, x, N) that implements steps of the inverse power method using the matrix and an initial vector . This function prints approximations to the eigenvalue of closest to and its associated eigenvector. By trying different values of in the shaded regions of the number line shown in , find the other two eigenvalues of .   Write a list of the four eigenvalues of in increasing order.      If , then , which shows that is also an eigenvector of .  From the previous part, we see that the associated eigenvalue is .  If is the eigenvalue of closest to , then is an eigenvalue of that must be the closest to .  The inverse power method applied to tells us the eigenvalue of having the smallest absolute value and an associated eigenvector . Therefore, is the eigenvalue of closest to and is an associated eigenvector.  We begin by trying to find the closest eigenvalue to, say, . The power method tells us that this eigenvalue is . If we then try to find the eigenvalue closest to , we find the fourth eigenvalue . It may require some experimentation to find all of the eigenvalues.  The eigenvalues are .    "
},
{
  "id": "exercise-189",
  "level": "2",
  "url": "sec-power-method.html#exercise-189",
  "type": "Exercise",
  "number": "5.2.4.1",
  "title": "",
  "body": " Suppose that is a matrix having eigenvalues , , , and .  What are the eigenvalues of ?  What are the eigenvalues of ?      , , , and .   , , , and .     The eigenvalues of are the reciprocals of the eigenvalues of . They are, therefore, , , , and .  If is an eigenvalue of , then is an eigenvalue of . Therefore, the eigenvalues of are , , , and .   "
},
{
  "id": "exercise-190",
  "level": "2",
  "url": "sec-power-method.html#exercise-190",
  "type": "Exercise",
  "number": "5.2.4.2",
  "title": "",
  "body": " Use the commands power , inverse_power , and find_closest_eigenvalue to approximate the eigenvalues and associated eigenvectors of the following matrices.    .   .   .      and .   and .   , , , and .     The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is with associated eigenvector . The inverse power method tells us that the eigenvalue having the smallest absolute value is with associated eigenvector .  The power method tells us that the dominant eigenvalue is . The inverse power method tells us that the eigenvalue having the smallest absolute value is . If we now look for the closest eigenvalue to , we see that it is , which we have already found. Let's try again, this time looking to find the closest eigenvalue to . Here, we find . If we next try to find the eigenvalue closest to , we find it to be .  The four eigenvalues are then , , , and .   "
},
{
  "id": "exercise-191",
  "level": "2",
  "url": "sec-power-method.html#exercise-191",
  "type": "Exercise",
  "number": "5.2.4.3",
  "title": "",
  "body": " Use the techniques we have seen in this section to find the eigenvalues of the matrix .     , , , , and .   The power method shows us that is the dominant eigenvalue. The inverse power method tells us that is the eigenvalue having the smallest absolute value. We then probe in between these values to find eigenvalues , , and .  "
},
{
  "id": "exercise-192",
  "level": "2",
  "url": "sec-power-method.html#exercise-192",
  "type": "Exercise",
  "number": "5.2.4.4",
  "title": "",
  "body": " Consider the matrix .   Describe what happens if we apply the power method and the inverse power method using the initial vector .  Find the eigenvalues of this matrix and explain this observed behavior.  How can we apply the techniques of this section to find the eigenvalues of ?     The methods do not converge.  There is not a unique dominant eigenvalue.  Try finding an eigenvalue closest to, say, .     We see that neither the power method nor the inverse power method converge.  The eigenvalues are and . This means that there is not a unique dominant eigenvalue and there is not a unique eigenvalue with the smallest absolute value. The methods try to find first one of them and then the other.  To break the symmetry, we can look for an eigenvalue closest to, say, . When we do this, we find the eigenvalue . Then look for another eigenvalue closest to to find .   "
},
{
  "id": "exercise-power-method",
  "level": "2",
  "url": "sec-power-method.html#exercise-power-method",
  "type": "Exercise",
  "number": "5.2.4.5",
  "title": "",
  "body": " We have seen that the matrix has eigenvalues and and associated eigenvectors and .  Describe what happens when we apply the inverse power method using the initial vector .  Explain why this is happening and provide a contrast with how the power method usually works.  How can we modify the power method to give the dominant eigenvalue in this case?     We see that the vectors do not converge and the multipliers converge to the wrong value.  The multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier .     We see that the vectors do not converge but instead flip between approximations to and . Also, the multipliers are converging to rather than .  When applying the power method, the multipliers are usually formed from the same component of the vectors in every iteration. Here, we see that the multipliers are obtained from the first component, then the second component, then the first, and so on.  Choose one of the components, say, the first one and consider the ratio between that component of and in place of the multiplier . The problem is that you must make sure that this component is not approaching zero.   "
},
{
  "id": "exercise-194",
  "level": "2",
  "url": "sec-power-method.html#exercise-194",
  "type": "Exercise",
  "number": "5.2.4.6",
  "title": "",
  "body": " Suppose that is a matrix with eigenvalues and and that is a matrix with eigenvalues and . If we apply the power method to find the dominant eigenvalue of these matrices to the same degree of accuracy, which matrix will require more steps in the algorithm? Explain your response.   We will need more steps when finding the dominant eigenvalue of .   For both matrices, is the dominant eigenvalue and is the eigenvalue closest to . We will construct the initial vector as a linear combination of eigenvectors: . Then . For the matrix , is relatively small compared to so we expect the contribution from to become smaller more quickly. Therefore, we will need more steps in the power method to find the dominant eigenvalue of .  "
},
{
  "id": "exercise-195",
  "level": "2",
  "url": "sec-power-method.html#exercise-195",
  "type": "Exercise",
  "number": "5.2.4.7",
  "title": "",
  "body": " Suppose that we apply the power method to the matrix with an initial vector and find the eigenvalue and eigenvector . Suppose that we then apply the power method again with a different initial vector and find the same eigenvalue but a different eigenvector . What can we conclude about the matrix in this case?    is the dominant eigenvalue with a multiplicity greater than one.   The dominant eigenvector is but it has a multiplicity greater than one and the associated eigenspace has .  "
},
{
  "id": "exercise-196",
  "level": "2",
  "url": "sec-power-method.html#exercise-196",
  "type": "Exercise",
  "number": "5.2.4.8",
  "title": "",
  "body": " The power method we have developed only works if the matrix has real eigenvalues. Suppose that is a matrix that has a complex eigenvalue . What would happen if we apply the power method to ?   The vectors will not converge.   The power method relies on the fact that the vectors attempt to line up in the direction of a vector in the dominant eigenspace. If the eigenvalues are complex, however, the vectors will be rotated with each iteration so they will not converge.  "
},
{
  "id": "exercise-197",
  "level": "2",
  "url": "sec-power-method.html#exercise-197",
  "type": "Exercise",
  "number": "5.2.4.9",
  "title": "",
  "body": " Consider the matrix .  Find the eigenvalues and associated eigenvectors of .  Make a prediction about what happens if we apply the power method and the inverse power method to find eigenvalues of .  Verify your prediction using Sage.      There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .     There is a single eigenvalue having multiplicity two with its associated eigenspace being one-dimensional with basis vector .  Applying either the power or inverse power method will find the eigenvalue and a scalar multiple of .   "
},
{
  "id": "sec-dot-product",
  "level": "1",
  "url": "sec-dot-product.html",
  "type": "Section",
  "number": "6.1",
  "title": "The dot product",
  "body": " The dot product   In this section, we introduce a simple algebraic operation, known as the dot product , that helps us measure the length of vectors and the angle formed by a pair of vectors. For two-dimensional vectors and , their dot product is the scalar defined to be For instance,       Compute the dot product   Sketch the vector below. Then use the Pythagorean theorem to find the length of .     Sketch the vector and find its length.    Compute the dot product . How is the dot product related to the length of ?  Remember that the matrix represents the matrix transformation that rotates vectors counterclockwise by . Beginning with the vector , find , the result of rotating by , and sketch it above.  What is the dot product ?  Suppose that . Find the vector that results from rotating by and find the dot product .  Suppose that and are two perpendicular vectors. What do you think their dot product is?          .    The length of is 5.     , which is the square of the length of .          .     .    The dot product should be zero.         The geometry of the dot product   dot product The dot product is defined, more generally, for any two -dimensional vectors: The important thing to remember is that the dot product will produce a scalar. In other words, the two vectors are combined in such a way as to create a number, and, as we'll see, this number conveys useful geometric information.    We compute the dot product between two four-dimensional vectors as      Properties of dot products  As with ordinary multiplication, the dot product enjoys some familiar algebraic properties, such as commutativity and distributivity. More specifically, it doesn't matter in which order we compute the dot product of two vectors: If is a scalar, we have We may also distribute the dot product across linear combinations:      Suppose that and . Then     The most important property of the dot product, and the real reason for our interest in it, is that it gives us geometric information about vectors and their relationship to one another. Let's first think about the length of a vector by looking at the vector as shown in       The vector .   We may find the length of this vector using the Pythagorean theorem since the vector forms the hypotenuse of a right triangle having a horizontal leg of length 3 and a vertical leg of length 2. The length of , which we denote as , is therefore . Now notice that the dot product of with itself is . This is true in general; that is, we have   More than that, the dot product of two vectors records information about the angle between them. Consider .      The dot product measures the angle .   To see this, we will apply the Law of Cosines, which says that The upshot of this reasoning is that   To summarize:   Geometric properties of the dot product  The dot product gives us the following geometric information: where is the angle between and .        Sketch the vectors and using .      Sketch the vectors and here.    Find the lengths and using the dot product.  Find the dot product and use it to find the angle between and .  Consider the vector . Include it in your sketch in and find the angle between and .  If two vectors are perpendicular, what can you say about their dot product? Explain your thinking.  For what value of is the vector perpendicular to ?  Sage can be used to find lengths of vectors and their dot products. For instance, if v and w are vectors, then v.norm() gives the length of v and v * w gives .  Suppose that Use the Sage cell below to find , , , and the angle between and . You may use arccos to find the angle's measure expressed in radians.              We find that so that and .     so that      so that     If two vectors are perpendicular, then the angle between them is . Since , their dot product must be zero.    The dot product is so .    We find that , , , and the angle between these vectors is .                and .              Their dot product must be zero.     .     .       As we move forward, it will be important for us to recognize when vectors are perpendicular to one another. For instance, when vectors and are perpendicular, the angle between them and we have Therefore, the dot product between perpendicular vectors must be zero. This leads to the following definition.    orthogonal We say that vectors and are orthogonal if .   In practical terms, two perpendicular vectors are orthogonal. However, the concept of orthogonality is somewhat more general because it allows one or both of the vectors to be the zero vector .  We've now seen that the dot product gives us geometric information about vectors. It also provides a way to compare vectors. For example, consider the vectors , , and , shown in . The vectors and seem somewhat similar as the directions they define are nearly the same. By comparison, appears rather dissimilar to both and . We will measure the similarity of vectors by finding the angle between them; the smaller the angle, the more similar the vectors.      Which of the vectors are most similar?     This activity explores two further uses of the dot product beginning with the similarity of vectors.   Our first task is to assess the similarity between various Wikipedia articles by forming vectors from each of five articles. In particular, one may download the text from a Wikipedia article, remove common words, such as the and and , count the number of times the remaining words appear in the article, and represent these counts in a vector.  For example, evaluate the following cell that loads some special commands along with the vectors constructed from the Wikipedia articles on Veteran's Day, Memorial Day, Labor Day, the Golden Globe Awards, and the Super Bowl. For each of the five articles, you will see a list of the number of times 10 words appear in these articles. For instance, the word act appears 3 times in the Veteran's Day article and 0 times in the Labor Day article. For each of the five articles, we obtain 604-dimensional vectors, which are named veterans , memorial , labor , golden , and super .     Suppose that two articles have no words in common. What is the value of the dot product between their corresponding vectors? What does this say about the angle between these vectors?    Suppose there are two articles on the same subject, yet one article is twice as long. What approximate relationship would you expect to hold between the two vectors? What does this say about the angle between them?    Use the Sage cell below to find the angle between the vector veterans and the other four vectors. To express the angle in degrees, use the degrees(x) command, which gives the number of degrees in x radians.     Compare the four angles you have found and discuss what they mean about the similarity between the Veteran's Day article and the other four. How do your findings reflect the nature of these five events?       Vectors are often used to represent how a quantity changes over time. For instance, the vector might represent the value of a company's stock on four consecutive days. When interpreted in this way, we call the vector a time series. Evaluate the Sage cell below to see a representation of two time series , in blue, and , in orange, which we imagine represent the value of two stocks over a period of time. (This cell relies on some data loaded by the first cell in this activity.) Even though one stock has a higher value than the other, the two appear to be related since they seem to rise and fall at roughly similar ways. We often say that they are correlated , and we would like to measure the degree to which they are correlated.   In order to compare the ways in which they rise and fall, we will first demean the time series; that is, for each time series, we will subtract its average value to obtain a new time series. There is a command, demean(s) , that returns the demeaned time series of s . Use the Sage cell below to demean the series and and plot.     If the demeaned series are and , then the correlation between and is defined to be Given the geometric interpretation of the dot product, the correlation equals the cosine of the angle between the demeaned time series, and therefore is between -1 and 1.  Find the correlation between and .     Suppose that two time series are such that their demeaned time series are scalar multiples of one another, as in        On the left, the demeaned time series are positive scalar multiples of one another. On the right, they are negative scalar multiples.   For instance, suppose we have time series and whose demeaned time series and are positive scalar multiples of one another. What is the angle between the demeaned vectors? What does this say about the correlation ?    Suppose the demeaned time series and are negative scalar multiples of one another, what is the angle between the demeaned vectors? What does this say about the correlation ?    Use the Sage cell below to plot the time series and and find their correlation.     Use the Sage cell below to plot the time series and and find their correlation.                 If there are no words in common, then the dot product between the two vectors will be zero. This means that they are perpendicular to one another.    The vectors should be, at least approximately, scalar multiples of one another, which means that the angle between them is zero.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    It appears that the articles on Veteran's Day and Memorial Day are most similar. This makes sense because both are U.S. national holidays that honor military service. The second most similar article is Labor Day, which is also a national holiday. The other two are quite dissimilar as they are entertainment events.          The graphs are now lowered so that their averages are zero.    The correlation is , which is quite close to 1.    The angle should be zero, which means that the correlation will be .    The angle should be , which means that the correlation should be .    The correlation is .    The correlation is .                They are perpendicular.    The angle should be close to 0.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    The articles on Veteran's Day and Memorial Day are most similar.          The graphs are now lowered so that their averages are zero.                                      -means clustering  A typical problem in data science is to find some underlying patterns in a dataset. Suppose, for instance, that we have the set of 177 data points plotted in . Notice that the points are not scattered around haphazardly; instead, they seem to form clusters. Our goal here is to develop a strategy for detecting the clusters.      A set of 177 data points.   To see how this could be useful, suppose we have medical data describing a group of patients, some of whom have been diagnosed with a specific condition, such as diabetes. Perhaps we have a record of age, weight, blood sugar, cholesterol, and other attributes for each patient. It could be that the data points for the group diagnosed as having the condition form a cluster that is somewhat distinct from the rest of the data. Suppose that we are able to identify that cluster and that we are then presented with a new patient that has not been tested for the condition. If the attributes for that patient place them in that cluster, we might identify them as being at risk for the condition and prioritize them for appropriate screenings.  If there are many attributes for each patient, the data may be high-dimensional and not easily visualized. We would therefore like to develop an algorithm that separates the data points into clusters without human intervention. We call the result a clustering .  The next activity introduces a technique, called -means clustering, that helps us find clusterings. To do so, we will view the data points as vectors so that the distance between two data points equals the length of the vector joining them. That is, if two points are represented by the vectors and , then the distance between the points is .    To begin, we identify the centroid , or the average, of a set of vectors as    Find the centroid of the vectors and sketch the vectors and the centroid using . You may wish to simply plot the points represented by the tips of the vectors rather than drawing the vectors themselves.      The vectors , , and their centroid.   Notice that the centroid lies in the center of the points defined by the vectors.    Now we'll illustrate an algorithm that forms clusterings. To begin, consider the following points, represented as vectors, which are shown in .      We will group this set of four points into two clusters.   Suppose that we would like to group these points into clusters. (Later on, we'll see how to choose an appropriate value for , the number of clusters.) We begin by choosing two points and at random and declaring them to be the centers ' of the two clusters.  For example, suppose we randomly choose and as the center of two clusters. The cluster centered on will be the set of points that are closer to than to . Determine which of the four data points are in this cluster, which we denote by , and circle them in .    The second cluster will consist of the data points that are closer to than . Determine which of the four points are in this cluster, which we denote by , and circle them in .    We now have a clustering with two clusters, but we will try to improve upon it in the following way. First, find the centroids of the two clusters; that is, redefine to be the centroid of cluster and to be the centroid of . Find those centroids and indicate them in       Indicate the new centroids and clusters.   Now update the cluster to be the set of points closer to than . Update the cluster in a similar way and indicate the clusters in .    Let's perform this last step again. That is, update the centroids and from the new clusters and then update the clusters and . Indicate your centroids and clusters in .      Indicate the new centroids and clusters.   Notice that this last step produces the same set of clusters so there is no point in repeating it. We declare this to be our final clustering.          The centroid is .       The first cluster is .    The second cluster is .    We redefine and . This leads to new clusters and .    We have new centroids and , and the clusters and are unchanged.          The centroid is .        .     .     and    and .     and . The clusters and are unchanged.       This activity demonstrates our algorithm for finding a clustering. We first choose a value and seek to break the data points into clusters. The algorithm proceeds in the following way:   Choose points at random from our dataset.    Construct the cluster as the set of data points closest to , as the set of data points closest to , and so forth.    Repeat the following until the clusters no longer change:   Find the centroids of the current clusters.    Update the clusters .        The clusterings we find depend on the initial random choice of points . For instance, in the previous activity, we arrived, with the initial choice and , at the clustering:   If we instead choose the initial points to be and , we eventually find the clustering:   Is there a way that we can determine which clustering is the better of the two? It seems like a better clustering will be one for which the points in a cluster are, on average, closer to the centroid of their cluster. If we have a clustering, we therefore define a function, called the objective , which measures the average of the square of the distance from each point to the centroid of the cluster to which that point belongs. A clustering with a smaller objective will have clusters more tightly centered around their centroids, which should result in a better clustering.  For example, when we obtain the clustering: with centroids and , we find the objective to be     We'll now use the objective to compare clusterings and to choose an appropriate value of .   In the previous activity, one initial choice of and led to the clustering: with centroids and . Find the objective of this clustering.    We have now seen two clusterings and computed their objectives. Recall that our dataset is shown in . Which of the two clusterings feels like the better fit? How is this fit reflected in the values of the objectives?    Evaluating the following cell will load and display a dataset consisting of 177 data points. This dataset has the name data . Given this plot of the data, what would seem like a reasonable number of clusters?    In the following cell, you may choose a value of and then run the algorithm to determine and display a clustering and its objective. If you run the algorithm a few times with the same value of , you will likely see different clusterings having different objectives. This is natural since our algorithm starts by making a random choice of points , and a different choices may lead to different clusterings. Choose a value of and run the algorithm a few times. Notice that clusterings having lower objectives seem to fit the data better. Repeat this experiment with a few different values of .     For a given value of , our strategy is to run the algorithm several times and choose the clustering with the smallest objective. After choosing a value of , the following cell will run the algorithm 10 times and display the clustering having the smallest objective.   For each value of between 2 and 9, find the clustering having the smallest objective and plot your findings in .      Construct a plot of the minimal objective as it depends on the choice of .   This plot is called an elbow plot due to its shape. Notice how the objective decreases sharply when is small and then flattens out. This leads to a location, called the elbow, where the objective transitions from being sharply decreasing to relatively flat. This means that increasing beyond the elbow does not significantly decrease the objective, which makes the elbow a good choice for .  Where does the elbow occur in your plot above? How does this compare to the best value of that you estimated by simply looking at the data in .     Of course, we could increase until each data point is its own cluster. However, this defeats the point of the technique, which is to group together nearby data points in the hope that they share common features, thus providing insight into the structure of the data.       The objective is     The clustering with and appears to be a tighter clustering and has a smaller objective.    It appears that the best clustering is either or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or , which are the values that we felt led to the best clusterings.             The objective is .    The clustering and has a smaller objective.     or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or .          We have now seen how our algorithm and the objective identify a reasonable value for , the number of the clusters, and produce a good clustering having clusters. Notice that we don't claim to have found the best clustering as the true test of any clustering will be in how it helps us understand the dataset and helps us make predictions about any new data that we may encounter.    Summary  This section introduced the dot product and the ability to investigate geometric relationships between vectors.   The dot product of two vectors and satisfies these properties: where is the angle between and .    The vectors and are orthogonal when .    We explored some applications of the dot product to the similarity of vectors, correlation of time series, and -means clustering.        Consider the vectors    Find the lengths of the vectors, and .    Find the dot product and use it to find the angle between and .          , , , and .     and .         We have and so that and .    Since , we find that .       Consider the three vectors    Find the dot products , , and .    Use the dot products you just found to evaluate:    .     .     .     .       For what value of is orthogonal to ?          , , and .                                    , , and .                                 Suppose that and are vectors where    What is ?    What is the angle between and ?    Suppose that is a scalar. Find the value of for which is orthogonal to ?                    .                    so .       Suppose that .   What is the relationship between and ?    What is the relationship between and ?    If for some scalar , what is the relationship between and ? What is the relationship between and ?    Suppose that . Find a scalar so that has length 1.                                             so that     We know so        Given vectors and , explain why Sketch two vectors and and explain why this fact is called the parallelogram law .   Use the relationship .              Consider the vectors and a general vector .   Write an equation in terms of , , and that describes all the vectors orthogonal to .    Write a linear system that describes all the vectors orthogonal to both and .    Write the solution set to this linear system in parametric form. What type of geometric object does this solution set represent? Indicate with a rough sketch why this makes sense.    Give a parametric description of all vectors orthogonal to . What type of geometric object does this represent? Indicate with a rough sketch why this makes sense.                                             , which describes a line     , which describes a plane.       Explain your responses to these questions.   Suppose that is orthogonal to both and . Can you guarantee that is also orthogonal to any linear combination ?    Suppose that is orthogonal to itself. What can you say about ?         Yes              Yes, because      so        Suppose that , , and form a basis for and that each vector is orthogonal to the other two. Suppose also that is another vector in .   Explain why for some scalars , , and .    Beginning with the expression apply the distributive property of dot products to explain why Find similar expressions for and .    Verify that form a basis for and that each vector is orthogonal to the other two. Use what you've discovered in this problem to write the vector as a linear combination of , , and .         Since , , and form a basis for , any vector in can be written as a linear combination of them.    Apply the distributive property.   and               Since , , and form a basis for , any vector in can be written as a linear combination of them.     so that .  In the same way, and     Check that the vectors are orthogonal by computing their dot products. Then by computing the ratios of dot products.       Suppose that , , and are three nonzero vectors that are pairwise orthogonal; that is, each vector is orthogonal to the other two.   Explain why cannot be a linear combination of and .    Explain why this set of three vectors is linearly independent.         If , then and .    None of the vectors is a linear combination of the others.         If , then . In the same way, so must be the zero vector. We're told, however, that is nonzero.    We've seen that is not a linear combination of and . The same thinking shows that none of the vectors is a linear combination of the others so they form a linearly independent set.       In the next chapter, we will consider certain matrices and define a function where is a vector in .   Suppose that and . Evaluate .    For a general vector , evaluate as an expression involving and .    Suppose that is an eigenvector of a matrix with associated eigenvalue and that has length 1. What is the value of the function ?                                        .       Back in , we saw that equations of the form represent lines in the plane. In this exercise, we will see how this expression arises geometrically.     A line, a point on the line, and a vector perpendicular to the line.     Find the slope and vertical intercept of the line shown in . Then write an equation for the line in the form .    Suppose that is a point on the line, that is a vector perpendicular to the line, and that is a general point on the line. Sketch the vector and describe the angle between this vector and the vector .    What is the value of the dot product ?    Explain why the equation of the line can be written in the form .    Identify the vectors and for the line illustrated in and use them to write the equation of the line in terms of and . Verify that this expression is algebraically equivalent to the equation that you earlier found for this line.    Explain why any line in the plane can be described by an equation having the form . What is the significance of the vector ?          .     is orthogonal to .         Apply the distributive property          is perpendicular to the line.         The slope and the intercept so .    The vector is in the direction of the line so it is orthogonal to .    Since these vectors are orthogonal, their dot product is .    Since , we have , which implies that .     and . This gives . We can rearrange this to have the form .    If we choose a vector that is perpendicular to the line and a point on the line, we have .       "
},
{
  "id": "exploration-21",
  "level": "2",
  "url": "sec-dot-product.html#exploration-21",
  "type": "Preview Activity",
  "number": "6.1.1",
  "title": "",
  "body": "    Compute the dot product   Sketch the vector below. Then use the Pythagorean theorem to find the length of .     Sketch the vector and find its length.    Compute the dot product . How is the dot product related to the length of ?  Remember that the matrix represents the matrix transformation that rotates vectors counterclockwise by . Beginning with the vector , find , the result of rotating by , and sketch it above.  What is the dot product ?  Suppose that . Find the vector that results from rotating by and find the dot product .  Suppose that and are two perpendicular vectors. What do you think their dot product is?          .    The length of is 5.     , which is the square of the length of .          .     .    The dot product should be zero.      "
},
{
  "id": "example-52",
  "level": "2",
  "url": "sec-dot-product.html#example-52",
  "type": "Example",
  "number": "6.1.2",
  "title": "",
  "body": "  We compute the dot product between two four-dimensional vectors as    "
},
{
  "id": "example-53",
  "level": "2",
  "url": "sec-dot-product.html#example-53",
  "type": "Example",
  "number": "6.1.3",
  "title": "",
  "body": "  Suppose that and . Then    "
},
{
  "id": "fig-dot-length",
  "level": "2",
  "url": "sec-dot-product.html#fig-dot-length",
  "type": "Figure",
  "number": "6.1.4",
  "title": "",
  "body": "    The vector .  "
},
{
  "id": "fig-dot-angle",
  "level": "2",
  "url": "sec-dot-product.html#fig-dot-angle",
  "type": "Figure",
  "number": "6.1.5",
  "title": "",
  "body": "    The dot product measures the angle .  "
},
{
  "id": "activity-68",
  "level": "2",
  "url": "sec-dot-product.html#activity-68",
  "type": "Activity",
  "number": "6.1.2",
  "title": "",
  "body": "     Sketch the vectors and using .      Sketch the vectors and here.    Find the lengths and using the dot product.  Find the dot product and use it to find the angle between and .  Consider the vector . Include it in your sketch in and find the angle between and .  If two vectors are perpendicular, what can you say about their dot product? Explain your thinking.  For what value of is the vector perpendicular to ?  Sage can be used to find lengths of vectors and their dot products. For instance, if v and w are vectors, then v.norm() gives the length of v and v * w gives .  Suppose that Use the Sage cell below to find , , , and the angle between and . You may use arccos to find the angle's measure expressed in radians.              We find that so that and .     so that      so that     If two vectors are perpendicular, then the angle between them is . Since , their dot product must be zero.    The dot product is so .    We find that , , , and the angle between these vectors is .                and .              Their dot product must be zero.     .     .      "
},
{
  "id": "definition-27",
  "level": "2",
  "url": "sec-dot-product.html#definition-27",
  "type": "Definition",
  "number": "6.1.7",
  "title": "",
  "body": "  orthogonal We say that vectors and are orthogonal if .  "
},
{
  "id": "fig-similar-vectors",
  "level": "2",
  "url": "sec-dot-product.html#fig-similar-vectors",
  "type": "Figure",
  "number": "6.1.8",
  "title": "",
  "body": "    Which of the vectors are most similar?  "
},
{
  "id": "activity-69",
  "level": "2",
  "url": "sec-dot-product.html#activity-69",
  "type": "Activity",
  "number": "6.1.3",
  "title": "",
  "body": "  This activity explores two further uses of the dot product beginning with the similarity of vectors.   Our first task is to assess the similarity between various Wikipedia articles by forming vectors from each of five articles. In particular, one may download the text from a Wikipedia article, remove common words, such as the and and , count the number of times the remaining words appear in the article, and represent these counts in a vector.  For example, evaluate the following cell that loads some special commands along with the vectors constructed from the Wikipedia articles on Veteran's Day, Memorial Day, Labor Day, the Golden Globe Awards, and the Super Bowl. For each of the five articles, you will see a list of the number of times 10 words appear in these articles. For instance, the word act appears 3 times in the Veteran's Day article and 0 times in the Labor Day article. For each of the five articles, we obtain 604-dimensional vectors, which are named veterans , memorial , labor , golden , and super .     Suppose that two articles have no words in common. What is the value of the dot product between their corresponding vectors? What does this say about the angle between these vectors?    Suppose there are two articles on the same subject, yet one article is twice as long. What approximate relationship would you expect to hold between the two vectors? What does this say about the angle between them?    Use the Sage cell below to find the angle between the vector veterans and the other four vectors. To express the angle in degrees, use the degrees(x) command, which gives the number of degrees in x radians.     Compare the four angles you have found and discuss what they mean about the similarity between the Veteran's Day article and the other four. How do your findings reflect the nature of these five events?       Vectors are often used to represent how a quantity changes over time. For instance, the vector might represent the value of a company's stock on four consecutive days. When interpreted in this way, we call the vector a time series. Evaluate the Sage cell below to see a representation of two time series , in blue, and , in orange, which we imagine represent the value of two stocks over a period of time. (This cell relies on some data loaded by the first cell in this activity.) Even though one stock has a higher value than the other, the two appear to be related since they seem to rise and fall at roughly similar ways. We often say that they are correlated , and we would like to measure the degree to which they are correlated.   In order to compare the ways in which they rise and fall, we will first demean the time series; that is, for each time series, we will subtract its average value to obtain a new time series. There is a command, demean(s) , that returns the demeaned time series of s . Use the Sage cell below to demean the series and and plot.     If the demeaned series are and , then the correlation between and is defined to be Given the geometric interpretation of the dot product, the correlation equals the cosine of the angle between the demeaned time series, and therefore is between -1 and 1.  Find the correlation between and .     Suppose that two time series are such that their demeaned time series are scalar multiples of one another, as in        On the left, the demeaned time series are positive scalar multiples of one another. On the right, they are negative scalar multiples.   For instance, suppose we have time series and whose demeaned time series and are positive scalar multiples of one another. What is the angle between the demeaned vectors? What does this say about the correlation ?    Suppose the demeaned time series and are negative scalar multiples of one another, what is the angle between the demeaned vectors? What does this say about the correlation ?    Use the Sage cell below to plot the time series and and find their correlation.     Use the Sage cell below to plot the time series and and find their correlation.                 If there are no words in common, then the dot product between the two vectors will be zero. This means that they are perpendicular to one another.    The vectors should be, at least approximately, scalar multiples of one another, which means that the angle between them is zero.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    It appears that the articles on Veteran's Day and Memorial Day are most similar. This makes sense because both are U.S. national holidays that honor military service. The second most similar article is Labor Day, which is also a national holiday. The other two are quite dissimilar as they are entertainment events.          The graphs are now lowered so that their averages are zero.    The correlation is , which is quite close to 1.    The angle should be zero, which means that the correlation will be .    The angle should be , which means that the correlation should be .    The correlation is .    The correlation is .                They are perpendicular.    The angle should be close to 0.    The angle veterans makes with memorial is , with labor is , with golden is , and with super is .    The articles on Veteran's Day and Memorial Day are most similar.          The graphs are now lowered so that their averages are zero.                                  "
},
{
  "id": "fig-clusters",
  "level": "2",
  "url": "sec-dot-product.html#fig-clusters",
  "type": "Figure",
  "number": "6.1.10",
  "title": "",
  "body": "    A set of 177 data points.  "
},
{
  "id": "activity-70",
  "level": "2",
  "url": "sec-dot-product.html#activity-70",
  "type": "Activity",
  "number": "6.1.4",
  "title": "",
  "body": "  To begin, we identify the centroid , or the average, of a set of vectors as    Find the centroid of the vectors and sketch the vectors and the centroid using . You may wish to simply plot the points represented by the tips of the vectors rather than drawing the vectors themselves.      The vectors , , and their centroid.   Notice that the centroid lies in the center of the points defined by the vectors.    Now we'll illustrate an algorithm that forms clusterings. To begin, consider the following points, represented as vectors, which are shown in .      We will group this set of four points into two clusters.   Suppose that we would like to group these points into clusters. (Later on, we'll see how to choose an appropriate value for , the number of clusters.) We begin by choosing two points and at random and declaring them to be the centers ' of the two clusters.  For example, suppose we randomly choose and as the center of two clusters. The cluster centered on will be the set of points that are closer to than to . Determine which of the four data points are in this cluster, which we denote by , and circle them in .    The second cluster will consist of the data points that are closer to than . Determine which of the four points are in this cluster, which we denote by , and circle them in .    We now have a clustering with two clusters, but we will try to improve upon it in the following way. First, find the centroids of the two clusters; that is, redefine to be the centroid of cluster and to be the centroid of . Find those centroids and indicate them in       Indicate the new centroids and clusters.   Now update the cluster to be the set of points closer to than . Update the cluster in a similar way and indicate the clusters in .    Let's perform this last step again. That is, update the centroids and from the new clusters and then update the clusters and . Indicate your centroids and clusters in .      Indicate the new centroids and clusters.   Notice that this last step produces the same set of clusters so there is no point in repeating it. We declare this to be our final clustering.          The centroid is .       The first cluster is .    The second cluster is .    We redefine and . This leads to new clusters and .    We have new centroids and , and the clusters and are unchanged.          The centroid is .        .     .     and    and .     and . The clusters and are unchanged.      "
},
{
  "id": "activity-71",
  "level": "2",
  "url": "sec-dot-product.html#activity-71",
  "type": "Activity",
  "number": "6.1.5",
  "title": "",
  "body": "  We'll now use the objective to compare clusterings and to choose an appropriate value of .   In the previous activity, one initial choice of and led to the clustering: with centroids and . Find the objective of this clustering.    We have now seen two clusterings and computed their objectives. Recall that our dataset is shown in . Which of the two clusterings feels like the better fit? How is this fit reflected in the values of the objectives?    Evaluating the following cell will load and display a dataset consisting of 177 data points. This dataset has the name data . Given this plot of the data, what would seem like a reasonable number of clusters?    In the following cell, you may choose a value of and then run the algorithm to determine and display a clustering and its objective. If you run the algorithm a few times with the same value of , you will likely see different clusterings having different objectives. This is natural since our algorithm starts by making a random choice of points , and a different choices may lead to different clusterings. Choose a value of and run the algorithm a few times. Notice that clusterings having lower objectives seem to fit the data better. Repeat this experiment with a few different values of .     For a given value of , our strategy is to run the algorithm several times and choose the clustering with the smallest objective. After choosing a value of , the following cell will run the algorithm 10 times and display the clustering having the smallest objective.   For each value of between 2 and 9, find the clustering having the smallest objective and plot your findings in .      Construct a plot of the minimal objective as it depends on the choice of .   This plot is called an elbow plot due to its shape. Notice how the objective decreases sharply when is small and then flattens out. This leads to a location, called the elbow, where the objective transitions from being sharply decreasing to relatively flat. This means that increasing beyond the elbow does not significantly decrease the objective, which makes the elbow a good choice for .  Where does the elbow occur in your plot above? How does this compare to the best value of that you estimated by simply looking at the data in .     Of course, we could increase until each data point is its own cluster. However, this defeats the point of the technique, which is to group together nearby data points in the hope that they share common features, thus providing insight into the structure of the data.       The objective is     The clustering with and appears to be a tighter clustering and has a smaller objective.    It appears that the best clustering is either or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or , which are the values that we felt led to the best clusterings.             The objective is .    The clustering and has a smaller objective.     or .    With a fixed value of , running the algorithm several times leads to different clusterings with different objectives. If we increase , the objective generally decreases.    The elbow occurs around or .         "
},
{
  "id": "exercise-198",
  "level": "2",
  "url": "sec-dot-product.html#exercise-198",
  "type": "Exercise",
  "number": "6.1.4.1",
  "title": "",
  "body": " Consider the vectors    Find the lengths of the vectors, and .    Find the dot product and use it to find the angle between and .          , , , and .     and .         We have and so that and .    Since , we find that .     "
},
{
  "id": "exercise-199",
  "level": "2",
  "url": "sec-dot-product.html#exercise-199",
  "type": "Exercise",
  "number": "6.1.4.2",
  "title": "",
  "body": " Consider the three vectors    Find the dot products , , and .    Use the dot products you just found to evaluate:    .     .     .     .       For what value of is orthogonal to ?          , , and .                                    , , and .                               "
},
{
  "id": "exercise-200",
  "level": "2",
  "url": "sec-dot-product.html#exercise-200",
  "type": "Exercise",
  "number": "6.1.4.3",
  "title": "",
  "body": " Suppose that and are vectors where    What is ?    What is the angle between and ?    Suppose that is a scalar. Find the value of for which is orthogonal to ?                    .                    so .     "
},
{
  "id": "exercise-201",
  "level": "2",
  "url": "sec-dot-product.html#exercise-201",
  "type": "Exercise",
  "number": "6.1.4.4",
  "title": "",
  "body": " Suppose that .   What is the relationship between and ?    What is the relationship between and ?    If for some scalar , what is the relationship between and ? What is the relationship between and ?    Suppose that . Find a scalar so that has length 1.                                             so that     We know so      "
},
{
  "id": "exercise-202",
  "level": "2",
  "url": "sec-dot-product.html#exercise-202",
  "type": "Exercise",
  "number": "6.1.4.5",
  "title": "",
  "body": " Given vectors and , explain why Sketch two vectors and and explain why this fact is called the parallelogram law .   Use the relationship .            "
},
{
  "id": "exercise-203",
  "level": "2",
  "url": "sec-dot-product.html#exercise-203",
  "type": "Exercise",
  "number": "6.1.4.6",
  "title": "",
  "body": " Consider the vectors and a general vector .   Write an equation in terms of , , and that describes all the vectors orthogonal to .    Write a linear system that describes all the vectors orthogonal to both and .    Write the solution set to this linear system in parametric form. What type of geometric object does this solution set represent? Indicate with a rough sketch why this makes sense.    Give a parametric description of all vectors orthogonal to . What type of geometric object does this represent? Indicate with a rough sketch why this makes sense.                                             , which describes a line     , which describes a plane.     "
},
{
  "id": "exercise-204",
  "level": "2",
  "url": "sec-dot-product.html#exercise-204",
  "type": "Exercise",
  "number": "6.1.4.7",
  "title": "",
  "body": " Explain your responses to these questions.   Suppose that is orthogonal to both and . Can you guarantee that is also orthogonal to any linear combination ?    Suppose that is orthogonal to itself. What can you say about ?         Yes              Yes, because      so      "
},
{
  "id": "exercise-205",
  "level": "2",
  "url": "sec-dot-product.html#exercise-205",
  "type": "Exercise",
  "number": "6.1.4.8",
  "title": "",
  "body": " Suppose that , , and form a basis for and that each vector is orthogonal to the other two. Suppose also that is another vector in .   Explain why for some scalars , , and .    Beginning with the expression apply the distributive property of dot products to explain why Find similar expressions for and .    Verify that form a basis for and that each vector is orthogonal to the other two. Use what you've discovered in this problem to write the vector as a linear combination of , , and .         Since , , and form a basis for , any vector in can be written as a linear combination of them.    Apply the distributive property.   and               Since , , and form a basis for , any vector in can be written as a linear combination of them.     so that .  In the same way, and     Check that the vectors are orthogonal by computing their dot products. Then by computing the ratios of dot products.     "
},
{
  "id": "exercise-206",
  "level": "2",
  "url": "sec-dot-product.html#exercise-206",
  "type": "Exercise",
  "number": "6.1.4.9",
  "title": "",
  "body": " Suppose that , , and are three nonzero vectors that are pairwise orthogonal; that is, each vector is orthogonal to the other two.   Explain why cannot be a linear combination of and .    Explain why this set of three vectors is linearly independent.         If , then and .    None of the vectors is a linear combination of the others.         If , then . In the same way, so must be the zero vector. We're told, however, that is nonzero.    We've seen that is not a linear combination of and . The same thinking shows that none of the vectors is a linear combination of the others so they form a linearly independent set.     "
},
{
  "id": "exercise-207",
  "level": "2",
  "url": "sec-dot-product.html#exercise-207",
  "type": "Exercise",
  "number": "6.1.4.10",
  "title": "",
  "body": " In the next chapter, we will consider certain matrices and define a function where is a vector in .   Suppose that and . Evaluate .    For a general vector , evaluate as an expression involving and .    Suppose that is an eigenvector of a matrix with associated eigenvalue and that has length 1. What is the value of the function ?                                        .     "
},
{
  "id": "exercise-208",
  "level": "2",
  "url": "sec-dot-product.html#exercise-208",
  "type": "Exercise",
  "number": "6.1.4.11",
  "title": "",
  "body": " Back in , we saw that equations of the form represent lines in the plane. In this exercise, we will see how this expression arises geometrically.     A line, a point on the line, and a vector perpendicular to the line.     Find the slope and vertical intercept of the line shown in . Then write an equation for the line in the form .    Suppose that is a point on the line, that is a vector perpendicular to the line, and that is a general point on the line. Sketch the vector and describe the angle between this vector and the vector .    What is the value of the dot product ?    Explain why the equation of the line can be written in the form .    Identify the vectors and for the line illustrated in and use them to write the equation of the line in terms of and . Verify that this expression is algebraically equivalent to the equation that you earlier found for this line.    Explain why any line in the plane can be described by an equation having the form . What is the significance of the vector ?          .     is orthogonal to .         Apply the distributive property          is perpendicular to the line.         The slope and the intercept so .    The vector is in the direction of the line so it is orthogonal to .    Since these vectors are orthogonal, their dot product is .    Since , we have , which implies that .     and . This gives . We can rearrange this to have the form .    If we choose a vector that is perpendicular to the line and a point on the line, we have .     "
},
{
  "id": "sec-transpose",
  "level": "1",
  "url": "sec-transpose.html",
  "type": "Section",
  "number": "6.2",
  "title": "Orthogonal complements and the matrix transpose",
  "body": " Orthogonal complements and the matrix transpose   We've now seen how the dot product enables us to determine the angle between two vectors and, more specifically, when two vectors are orthogonal. Moving forward, we will explore how the orthogonality condition simplifies many common tasks, such as expressing a vector as a linear combination of a given set of vectors.  This section introduces the notion of an orthogonal complement, the set of vectors each of which is orthogonal to a prescribed subspace. We'll also find a way to describe dot products using matrix products, which allows us to study orthogonality using many of the tools for understanding linear systems that we developed earlier.     Sketch the vector on and one vector that is orthogonal to it.     Sketch the vector and one vector orthogonal to it.    If a vector is orthogonal to , what do we know about the dot product ?  If we write , use the dot product to write an equation for the vectors orthogonal to in terms of and .  Use this equation to sketch the set of all vectors orthogonal to in .   introduced the column space and null space of a matrix . If is a matrix, what is the meaning of the null space ?  What is the meaning of the column space ?        The vector is an example of a vector orthogonal to .    The dot product must be zero.     .    This is the line .    It is the set of vectors for which .    It is the set of vector for which the equation is consistent.         Orthogonal complements  The preview activity presented us with a vector and led us through the process of describing all the vectors orthogonal to . Notice that the set of scalar multiples of describes a line , a 1-dimensional subspace of . We then described a second line consisting of all the vectors orthogonal to . Notice that every vector on this line is orthogonal to every vector on the line . We call this new line the orthogonal complement of and denote it by . The lines and are illustrated on the left of .       On the left is a line and its orthogonal complement . On the right is a plane and its orthogonal complement in .   The next definition places this example into a more general context.   orthogonal complement   Given a subspace of , the orthogonal complement of is the set of vectors in each of which is orthogonal to every vector in . We denote the orthogonal complement by .    A typical example appears on the right of . Here we see a plane , a two-dimensional subspace of , and its orthogonal complement , which is a line in .  As the next activity demonstrates, the orthogonal complement of a subspace is itself a subspace of .    Suppose that and form a basis for , a two-dimensional subspace of . We will find a description of the orthogonal complement .   Suppose that the vector is orthogonal to . If we write , use the fact that to write a linear equation for , , and .    Suppose that is also orthogonal to . In the same way, write a linear equation for , , and that arises from the fact that .    If is orthogonal to both and , these two equations give us a linear system for some matrix . Identify the matrix and write a parametric description of the solution space to the equation .    Since and form a basis for the two-dimensional subspace , any vector in can be written as a linear combination If is orthogonal to both and , use the distributive property of dot products to explain why is orthogonal to .    Give a basis for the orthogonal complement and state the dimension .    Describe , the orthogonal complement of .          We have the equation .    We have the equation .    These two equations give where whose solutions have the parametric form .    By distributivity, .     is the solution space to the equation . Therefore, a basis consists of the single vector , and is one-dimensional.    Since every vector in is orthogonal to every vector in , the orthogonal complement of is .           .     .     so .    By distributivity, .    A basis consists of , and is one-dimensional.    The orthogonal complement of is .         If is the line defined by in , we will describe the orthogonal complement , the set of vectors orthogonal to .  If is orthogonal to , it must be orthogonal to so we have   We can describe the solutions to this equation parametrically as Therefore, the orthogonal complement is a plane, a two-dimensional subspace of , spanned by the vectors and .      Suppose that is the -dimensional subspace of with basis We will give a description of the orthogonal complement .  If is in , we know that is orthogonal to both and . Therefore, In other words, where The solutions may be described parametrically as The distributive property of dot products implies that any vector that is orthogonal to both and is also orthogonal to any linear combination of and since Therefore, is a -dimensional subspace of with basis One may check that the vectors , , and are each orthogonal to both and .      The matrix transpose  The previous activity and examples show how we can describe the orthogonal complement of a subspace as the solution set of a particular linear system. We will make this connection more explicit by defining a new matrix operation called the transpose .   transpose   The transpose of the matrix is the matrix whose rows are the columns of .      If , then      This activity illustrates how multiplying a vector by is related to computing dot products with the columns of . You'll develop a better understanding of this relationship if you compute the dot products and matrix products in this activity without using technology.  If , write the matrix .  Suppose that Find the dot products and .  Now write the matrix and its transpose . Find the product and describe how this product computes both dot products and .  Suppose that is a vector that is orthogonal to both and . What does this say about the dot products and ? What does this say about the product ?  Use the matrix to give a parametric description of all the vectors that are orthogonal to and .   Remember that , the null space of , is the solution set of the equation . If is a vector in , explain why must be orthogonal to both and .    Remember that , the column space of , is the set of linear combinations of the columns of . Therefore, any vector in can be written as . If is a vector in , explain why is orthogonal to every vector in .                         Both dot products are 0 so we have .    We need to solve the equation so we find the reduced row echelon form The vectors orthogonal to both and have the form .     tells us that and .    Since is orthogonal to both and , we have                 , .          .     .     .    Apply the distributive property of dot products.       The previous activity demonstrates an important connection between the matrix transpose and dot products. More specifically, the components of the product are simply the dot products of the columns of with . We will make frequent use of this observation so let's record it as a proposition.    If is the matrix whose columns are , then       Suppose that is a subspace of having basis and that we wish to describe the orthogonal complement .  If is the matrix and is in , we have Describing vectors that are orthogonal to both and is therefore equivalent to the more familiar task of describing the solution set . To do so, we find the reduced row echelon form of and write the solution set parametrically as Once again, the distributive property of dot products tells us that such a vector is also orthogonal to any linear combination of and so this solution set is, in fact, the orthogonal complement . Indeed, we see that the vectors form a basis for , which is a two-dimensional subspace of .    To place this example in a slightly more general context, note that and , the columns of , form a basis of . Since , the column space of is the subspace of linear combinations of the columns of , we have .  This example also shows that the orthogonal complement is described by the solution set of . This solution set is what we have called , the null space of . In this way, we see the following proposition, which is visually represented in .    For any matrix , the orthogonal complement of is ; that is,         The orthogonal complement of the column space of is the null space of .     Properties of the matrix transpose  The transpose is a simple algebraic operation performed on a matrix. The next activity explores some of its properties.    In Sage, the transpose of a matrix A is given by A.T . Define the matrices    Evaluate and . What do you notice about the relationship between these two matrices?  What happens if you transpose a matrix twice; that is, what is ?  Find and . What do you notice about the relationship between these determinants?    Find the product and its transpose .  Is it possible to compute the product ? Explain why or why not.  Find the product and compare it to . What do you notice about the relationship between these two matrices?    What is the transpose of the identity matrix ?  If a square matrix is invertible, explain why you can guarantee that is invertible and why .         .     .             and     The product is not defined because has two columns and has three rows.                 We have so . This means that .           .     .          .          .       In spite of the fact that we are looking at some specific examples, this activity demonstrates the following general properties of the transpose, which may be verified with a little effort.   Properties of the transpose  Here are some properties of the matrix transpose, expressed in terms of general matrices , , and . We assume that is a square matrix.  If is defined, then .   .   .   .  If is defined, then . Notice that the order of the multiplication is reversed.   .    There is one final property we wish to record though we will wait until to explain why it is true.   For any matrix , we have    This proposition is important because it implies a relationship between the dimensions of a subspace and its orthogonal complement. For instance, if is an matrix, we saw in that and .  Now suppose that is an -dimensional subspace of with basis . If we form the matrix , then so that   The transpose is an matrix having . Since , we have This explains the following proposition.    If is a subspace of , then       In , we constructed the orthogonal complement of a line in . The dimension of the orthogonal complement should be , which explains why we found the orthogonal complement to be a plane.      In , we looked at , a -dimensional subspace of and found its orthogonal complement to be a -dimensional subspace of .         Suppose that is a -dimensional subspace of and that is a matrix whose columns form a basis for ; that is, .  What is the shape of ?  What is the rank of ?  What is the shape of ?  What is the rank of ?  What is ?  What is ?  How are the dimensions of and related?      Suppose that is a subspace of having basis    Find the dimensions and .    Find a basis for . It may be helpful to know that the Sage command A.right_kernel() produces a basis for .     Verify that each of the basis vectors you found for are orthogonal to the basis vectors for .                 is .     .     is .                    since the subspaces live in .           so .    A basis is and .    You can verify by computing the four dot products.                                                               and .    Verify by computing the four dot products.            Summary  This section introduced the matrix transpose, its connection to dot products, and its use in describing the orthogonal complement of a subspace.   The columns of the matrix are the rows of the matrix transpose .    The components of the product are the dot products of with the columns of .    The orthogonal complement of the column space of equals the null space of ; that is, .    If is a subspace of , then         Suppose that is a subspace of with basis    What are the dimensions and ?    Find a basis for .    Verify that each of the basis vectors for are orthogonal to and .          and .     and     Use the dot product.          and .     and     Verify that for all and .       Consider the matrix .   Find and a basis for .    Determine the dimension of and find a basis for it.          and a basis for is and      with basis .          so since there are two pivot positions. The reduced row echelon form shows that the third column is a linear combination of the first two so and form a basis for .    We know that . To find a basis, solve the equation to obtain .       Suppose that is the subspace of defined as the solution set of the equation    What are the dimensions and ?    Find a basis for .    Find a basis for .    In general, how can you easily find a basis for when is defined by           , .     , , and .                    where so . Therefore, .    A basis for is , , and .    Since every vector in satisfies , a basis for is             Determine whether the following statements are true or false and explain your reasoning.   If , then is in .    If is a matrix and is a matrix, then is a matrix.    If the columns of are , , and and , then is orthogonal to .    If is a matrix with , then is a line in .    If is a matrix with , then .         True    False    True    True    False         True, since , is in .    False, is a matrix, but it is given by .    True, because the second component of .    True, because .    False, .       Apply properties of matrix operations to simplify the following expressions.                                                                            A symmetric matrix is one for which .   Explain why a symmetric matrix must be square.    If and are general matrices and is a square diagonal matrix, which of the following matrices can you guarantee are symmetric?              .                 They must have the same number of rows and columns.       Yes    No    No    Yes            If is an matrix, then is If these matrices are the same, then .       If is diagonal, then .     so this matrix need not be symmetric.     so this matrix is symmetric.     so this matrix is symmetric.          If is a square matrix, remember that the characteristic polynomial of is and that the roots of the characteristic polynomial are the eigenvalues of .   Explain why and have the same characteristic polynomial.    Explain why and have the same set of eigenvalues.    Suppose that is diagonalizable with diagonalization . Explain why is diagonalizable and find a diagonalization.         Use properties of the matrix transpose.    Because they have the same characteristic polynomial.               and therefore .    The eigenvalues of a matrix are given by the roots of its characteristic polynomial. Since and have the same characteristic polynomial, they have the same eigenvalues.            This exercise introduces a version of the Pythagorean theorem that we'll use later.   Suppose that and are orthogonal to one another. Use the dot product to explain why     Suppose that is a subspace of and that is a vector in for which where is in and is in . Explain why which is an expression of the Pythagorean theorem.              Because and are orthogonal.              Because and are orthogonal.       In the next chapter, symmetric matrices---that is, matrices for which ---play an important role. It turns out that eigenvectors of a symmetric matrix that are associated to different eigenvalues are orthogonal. We will explain this fact in this exercise.   Viewing a vector as a matrix having one column, we may write . If is a matrix, explain why .    We have seen that the matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Verify that is symmetric and that and are orthogonal.    Suppose that is a general symmetric matrix and that is an eigenvector associated to eigenvalue and that is an eigenvector associated to a different eigenvalue . Beginning with , apply the identity from the first part of this exercise to explain why and are orthogonal.         Use properties of the transpose.    Compute the dot product of and .    It follows that           .     and .     so since .       Given an matrix , the row space of is the column space of ; that is, .   Suppose that is a matrix. For what is a subspace of ?    How can help us describe ?    Suppose that . Find bases for and .                   A basis for are the three rows of . A basis for is .         Since is , it follows that is a subspace of .          so . A basis for are the three rows of . is one-dimensional with basis .       "
},
{
  "id": "exploration-22",
  "level": "2",
  "url": "sec-transpose.html#exploration-22",
  "type": "Preview Activity",
  "number": "6.2.1",
  "title": "",
  "body": "   Sketch the vector on and one vector that is orthogonal to it.     Sketch the vector and one vector orthogonal to it.    If a vector is orthogonal to , what do we know about the dot product ?  If we write , use the dot product to write an equation for the vectors orthogonal to in terms of and .  Use this equation to sketch the set of all vectors orthogonal to in .   introduced the column space and null space of a matrix . If is a matrix, what is the meaning of the null space ?  What is the meaning of the column space ?        The vector is an example of a vector orthogonal to .    The dot product must be zero.     .    This is the line .    It is the set of vectors for which .    It is the set of vector for which the equation is consistent.      "
},
{
  "id": "fig-orthog-comps",
  "level": "2",
  "url": "sec-transpose.html#fig-orthog-comps",
  "type": "Figure",
  "number": "6.2.2",
  "title": "",
  "body": "     On the left is a line and its orthogonal complement . On the right is a plane and its orthogonal complement in .  "
},
{
  "id": "definition-28",
  "level": "2",
  "url": "sec-transpose.html#definition-28",
  "type": "Definition",
  "number": "6.2.3",
  "title": "",
  "body": " orthogonal complement   Given a subspace of , the orthogonal complement of is the set of vectors in each of which is orthogonal to every vector in . We denote the orthogonal complement by .   "
},
{
  "id": "activity-72",
  "level": "2",
  "url": "sec-transpose.html#activity-72",
  "type": "Activity",
  "number": "6.2.2",
  "title": "",
  "body": "  Suppose that and form a basis for , a two-dimensional subspace of . We will find a description of the orthogonal complement .   Suppose that the vector is orthogonal to . If we write , use the fact that to write a linear equation for , , and .    Suppose that is also orthogonal to . In the same way, write a linear equation for , , and that arises from the fact that .    If is orthogonal to both and , these two equations give us a linear system for some matrix . Identify the matrix and write a parametric description of the solution space to the equation .    Since and form a basis for the two-dimensional subspace , any vector in can be written as a linear combination If is orthogonal to both and , use the distributive property of dot products to explain why is orthogonal to .    Give a basis for the orthogonal complement and state the dimension .    Describe , the orthogonal complement of .          We have the equation .    We have the equation .    These two equations give where whose solutions have the parametric form .    By distributivity, .     is the solution space to the equation . Therefore, a basis consists of the single vector , and is one-dimensional.    Since every vector in is orthogonal to every vector in , the orthogonal complement of is .           .     .     so .    By distributivity, .    A basis consists of , and is one-dimensional.    The orthogonal complement of is .      "
},
{
  "id": "example-orthog-comp-line",
  "level": "2",
  "url": "sec-transpose.html#example-orthog-comp-line",
  "type": "Example",
  "number": "6.2.4",
  "title": "",
  "body": "  If is the line defined by in , we will describe the orthogonal complement , the set of vectors orthogonal to .  If is orthogonal to , it must be orthogonal to so we have   We can describe the solutions to this equation parametrically as Therefore, the orthogonal complement is a plane, a two-dimensional subspace of , spanned by the vectors and .   "
},
{
  "id": "example-orthog-comp-gen",
  "level": "2",
  "url": "sec-transpose.html#example-orthog-comp-gen",
  "type": "Example",
  "number": "6.2.5",
  "title": "",
  "body": "  Suppose that is the -dimensional subspace of with basis We will give a description of the orthogonal complement .  If is in , we know that is orthogonal to both and . Therefore, In other words, where The solutions may be described parametrically as The distributive property of dot products implies that any vector that is orthogonal to both and is also orthogonal to any linear combination of and since Therefore, is a -dimensional subspace of with basis One may check that the vectors , , and are each orthogonal to both and .   "
},
{
  "id": "definition-29",
  "level": "2",
  "url": "sec-transpose.html#definition-29",
  "type": "Definition",
  "number": "6.2.6",
  "title": "",
  "body": " transpose   The transpose of the matrix is the matrix whose rows are the columns of .   "
},
{
  "id": "example-56",
  "level": "2",
  "url": "sec-transpose.html#example-56",
  "type": "Example",
  "number": "6.2.7",
  "title": "",
  "body": "  If , then   "
},
{
  "id": "activity-73",
  "level": "2",
  "url": "sec-transpose.html#activity-73",
  "type": "Activity",
  "number": "6.2.3",
  "title": "",
  "body": "  This activity illustrates how multiplying a vector by is related to computing dot products with the columns of . You'll develop a better understanding of this relationship if you compute the dot products and matrix products in this activity without using technology.  If , write the matrix .  Suppose that Find the dot products and .  Now write the matrix and its transpose . Find the product and describe how this product computes both dot products and .  Suppose that is a vector that is orthogonal to both and . What does this say about the dot products and ? What does this say about the product ?  Use the matrix to give a parametric description of all the vectors that are orthogonal to and .   Remember that , the null space of , is the solution set of the equation . If is a vector in , explain why must be orthogonal to both and .    Remember that , the column space of , is the set of linear combinations of the columns of . Therefore, any vector in can be written as . If is a vector in , explain why is orthogonal to every vector in .                         Both dot products are 0 so we have .    We need to solve the equation so we find the reduced row echelon form The vectors orthogonal to both and have the form .     tells us that and .    Since is orthogonal to both and , we have                 , .          .     .     .    Apply the distributive property of dot products.      "
},
{
  "id": "prop-transpose-multiplication",
  "level": "2",
  "url": "sec-transpose.html#prop-transpose-multiplication",
  "type": "Proposition",
  "number": "6.2.8",
  "title": "",
  "body": "  If is the matrix whose columns are , then    "
},
{
  "id": "example-57",
  "level": "2",
  "url": "sec-transpose.html#example-57",
  "type": "Example",
  "number": "6.2.9",
  "title": "",
  "body": "  Suppose that is a subspace of having basis and that we wish to describe the orthogonal complement .  If is the matrix and is in , we have Describing vectors that are orthogonal to both and is therefore equivalent to the more familiar task of describing the solution set . To do so, we find the reduced row echelon form of and write the solution set parametrically as Once again, the distributive property of dot products tells us that such a vector is also orthogonal to any linear combination of and so this solution set is, in fact, the orthogonal complement . Indeed, we see that the vectors form a basis for , which is a two-dimensional subspace of .   "
},
{
  "id": "prop-col-orthog",
  "level": "2",
  "url": "sec-transpose.html#prop-col-orthog",
  "type": "Proposition",
  "number": "6.2.10",
  "title": "",
  "body": "  For any matrix , the orthogonal complement of is ; that is,    "
},
{
  "id": "fig-orthog-comp",
  "level": "2",
  "url": "sec-transpose.html#fig-orthog-comp",
  "type": "Figure",
  "number": "6.2.11",
  "title": "",
  "body": "    The orthogonal complement of the column space of is the null space of .  "
},
{
  "id": "activity-74",
  "level": "2",
  "url": "sec-transpose.html#activity-74",
  "type": "Activity",
  "number": "6.2.4",
  "title": "",
  "body": "  In Sage, the transpose of a matrix A is given by A.T . Define the matrices    Evaluate and . What do you notice about the relationship between these two matrices?  What happens if you transpose a matrix twice; that is, what is ?  Find and . What do you notice about the relationship between these determinants?    Find the product and its transpose .  Is it possible to compute the product ? Explain why or why not.  Find the product and compare it to . What do you notice about the relationship between these two matrices?    What is the transpose of the identity matrix ?  If a square matrix is invertible, explain why you can guarantee that is invertible and why .         .     .             and     The product is not defined because has two columns and has three rows.                 We have so . This means that .           .     .          .          .      "
},
{
  "id": "prop-col-row-rank",
  "level": "2",
  "url": "sec-transpose.html#prop-col-row-rank",
  "type": "Proposition",
  "number": "6.2.12",
  "title": "",
  "body": " For any matrix , we have   "
},
{
  "id": "prop-orthog-dim",
  "level": "2",
  "url": "sec-transpose.html#prop-orthog-dim",
  "type": "Proposition",
  "number": "6.2.13",
  "title": "",
  "body": "  If is a subspace of , then    "
},
{
  "id": "example-58",
  "level": "2",
  "url": "sec-transpose.html#example-58",
  "type": "Example",
  "number": "6.2.14",
  "title": "",
  "body": "  In , we constructed the orthogonal complement of a line in . The dimension of the orthogonal complement should be , which explains why we found the orthogonal complement to be a plane.   "
},
{
  "id": "example-59",
  "level": "2",
  "url": "sec-transpose.html#example-59",
  "type": "Example",
  "number": "6.2.15",
  "title": "",
  "body": "  In , we looked at , a -dimensional subspace of and found its orthogonal complement to be a -dimensional subspace of .   "
},
{
  "id": "activity-75",
  "level": "2",
  "url": "sec-transpose.html#activity-75",
  "type": "Activity",
  "number": "6.2.5",
  "title": "",
  "body": "     Suppose that is a -dimensional subspace of and that is a matrix whose columns form a basis for ; that is, .  What is the shape of ?  What is the rank of ?  What is the shape of ?  What is the rank of ?  What is ?  What is ?  How are the dimensions of and related?      Suppose that is a subspace of having basis    Find the dimensions and .    Find a basis for . It may be helpful to know that the Sage command A.right_kernel() produces a basis for .     Verify that each of the basis vectors you found for are orthogonal to the basis vectors for .                 is .     .     is .                    since the subspaces live in .           so .    A basis is and .    You can verify by computing the four dot products.                                                               and .    Verify by computing the four dot products.         "
},
{
  "id": "exercise-209",
  "level": "2",
  "url": "sec-transpose.html#exercise-209",
  "type": "Exercise",
  "number": "6.2.5.1",
  "title": "",
  "body": " Suppose that is a subspace of with basis    What are the dimensions and ?    Find a basis for .    Verify that each of the basis vectors for are orthogonal to and .          and .     and     Use the dot product.          and .     and     Verify that for all and .     "
},
{
  "id": "exercise-210",
  "level": "2",
  "url": "sec-transpose.html#exercise-210",
  "type": "Exercise",
  "number": "6.2.5.2",
  "title": "",
  "body": " Consider the matrix .   Find and a basis for .    Determine the dimension of and find a basis for it.          and a basis for is and      with basis .          so since there are two pivot positions. The reduced row echelon form shows that the third column is a linear combination of the first two so and form a basis for .    We know that . To find a basis, solve the equation to obtain .     "
},
{
  "id": "exercise-211",
  "level": "2",
  "url": "sec-transpose.html#exercise-211",
  "type": "Exercise",
  "number": "6.2.5.3",
  "title": "",
  "body": " Suppose that is the subspace of defined as the solution set of the equation    What are the dimensions and ?    Find a basis for .    Find a basis for .    In general, how can you easily find a basis for when is defined by           , .     , , and .                    where so . Therefore, .    A basis for is , , and .    Since every vector in satisfies , a basis for is           "
},
{
  "id": "exercise-212",
  "level": "2",
  "url": "sec-transpose.html#exercise-212",
  "type": "Exercise",
  "number": "6.2.5.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.   If , then is in .    If is a matrix and is a matrix, then is a matrix.    If the columns of are , , and and , then is orthogonal to .    If is a matrix with , then is a line in .    If is a matrix with , then .         True    False    True    True    False         True, since , is in .    False, is a matrix, but it is given by .    True, because the second component of .    True, because .    False, .     "
},
{
  "id": "exercise-213",
  "level": "2",
  "url": "sec-transpose.html#exercise-213",
  "type": "Exercise",
  "number": "6.2.5.5",
  "title": "",
  "body": " Apply properties of matrix operations to simplify the following expressions.                                                                          "
},
{
  "id": "exercise-214",
  "level": "2",
  "url": "sec-transpose.html#exercise-214",
  "type": "Exercise",
  "number": "6.2.5.6",
  "title": "",
  "body": " A symmetric matrix is one for which .   Explain why a symmetric matrix must be square.    If and are general matrices and is a square diagonal matrix, which of the following matrices can you guarantee are symmetric?              .                 They must have the same number of rows and columns.       Yes    No    No    Yes            If is an matrix, then is If these matrices are the same, then .       If is diagonal, then .     so this matrix need not be symmetric.     so this matrix is symmetric.     so this matrix is symmetric.        "
},
{
  "id": "exercise-215",
  "level": "2",
  "url": "sec-transpose.html#exercise-215",
  "type": "Exercise",
  "number": "6.2.5.7",
  "title": "",
  "body": " If is a square matrix, remember that the characteristic polynomial of is and that the roots of the characteristic polynomial are the eigenvalues of .   Explain why and have the same characteristic polynomial.    Explain why and have the same set of eigenvalues.    Suppose that is diagonalizable with diagonalization . Explain why is diagonalizable and find a diagonalization.         Use properties of the matrix transpose.    Because they have the same characteristic polynomial.               and therefore .    The eigenvalues of a matrix are given by the roots of its characteristic polynomial. Since and have the same characteristic polynomial, they have the same eigenvalues.          "
},
{
  "id": "exercise-216",
  "level": "2",
  "url": "sec-transpose.html#exercise-216",
  "type": "Exercise",
  "number": "6.2.5.8",
  "title": "",
  "body": " This exercise introduces a version of the Pythagorean theorem that we'll use later.   Suppose that and are orthogonal to one another. Use the dot product to explain why     Suppose that is a subspace of and that is a vector in for which where is in and is in . Explain why which is an expression of the Pythagorean theorem.              Because and are orthogonal.              Because and are orthogonal.     "
},
{
  "id": "exercise-217",
  "level": "2",
  "url": "sec-transpose.html#exercise-217",
  "type": "Exercise",
  "number": "6.2.5.9",
  "title": "",
  "body": " In the next chapter, symmetric matrices---that is, matrices for which ---play an important role. It turns out that eigenvectors of a symmetric matrix that are associated to different eigenvalues are orthogonal. We will explain this fact in this exercise.   Viewing a vector as a matrix having one column, we may write . If is a matrix, explain why .    We have seen that the matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Verify that is symmetric and that and are orthogonal.    Suppose that is a general symmetric matrix and that is an eigenvector associated to eigenvalue and that is an eigenvector associated to a different eigenvalue . Beginning with , apply the identity from the first part of this exercise to explain why and are orthogonal.         Use properties of the transpose.    Compute the dot product of and .    It follows that           .     and .     so since .     "
},
{
  "id": "exercise-218",
  "level": "2",
  "url": "sec-transpose.html#exercise-218",
  "type": "Exercise",
  "number": "6.2.5.10",
  "title": "",
  "body": " Given an matrix , the row space of is the column space of ; that is, .   Suppose that is a matrix. For what is a subspace of ?    How can help us describe ?    Suppose that . Find bases for and .                   A basis for are the three rows of . A basis for is .         Since is , it follows that is a subspace of .          so . A basis for are the three rows of . is one-dimensional with basis .     "
},
{
  "id": "sec-orthogonal-bases",
  "level": "1",
  "url": "sec-orthogonal-bases.html",
  "type": "Section",
  "number": "6.3",
  "title": "Orthogonal bases and projections",
  "body": " Orthogonal bases and projections   We know that a linear system is inconsistent when is not in , the column space of . Later in this chapter, we'll develop a strategy for dealing with inconsistent systems by finding , the vector in that minimizes the distance to . The equation is therefore consistent and its solution set can provide us with useful information about the original system .  In this section and the next, we'll develop some techniques that enable us to find , the vector in a given subspace that is closest to a given vector .    For this activity, it will be helpful to recall the distributive property of dot products: We'll work with the basis of formed by the vectors    Verify that the vectors and are orthogonal.    Suppose that and find the dot products and .    We would like to express as a linear combination of and , which means that we need to find weights and such that To find the weight , dot both sides of this expression with : and apply the distributive property.    In a similar fashion, find the weight .    Verify that using the weights you have found.          We can compute that .     and .     .     .     .       We frequently ask to write a given vector as a linear combination of given basis vectors. In the past, we have done this by solving a linear system. The preview activity illustrates how this task can be simplified when the basis vectors are orthogonal to each other. We'll explore this and other uses of orthogonal bases in this section.    Orthogonal sets  The preview activity dealt with a basis of formed by two orthogonal vectors. More generally, we will consider a set of orthogonal vectors, as described in the next definition.   orthogonal set   By an orthogonal set of vectors, we mean a set of nonzero vectors each of which is orthogonal to the others.      The 3-dimensional vectors form an orthogonal set, which can be verified by computing Notice that this set of vectors forms a basis for .      The vectors form an orthogonal set of 4-dimensional vectors. Since there are only three vectors, this set does not form a basis for . It does, however, form a basis for a 3-dimensional subspace of .    Suppose that a vector is a linear combination of an orthogonal set of vectors ; that is, suppose that Just as in the preview activity, we can find the weight by dotting both sides with and applying the distributive property of dot products: Notice how the presence of an orthogonal set causes most of the terms in the sum to vanish. In the same way, we find that so that   We'll record this fact in the following proposition.    If a vector is a linear combination of an orthogonal set of vectors , then     Using this proposition, we can see that an orthogonal set of vectors must be linearly independent. Suppose, for instance, that is a set of nonzero orthogonal vectors and that one of the vectors is a linear combination of the others, say, We therefore know that which cannot happen since we know that is nonzero. This tells us that    An orthogonal set of vectors is linearly independent.    If the vectors in an orthogonal set have dimension , they form a linearly independent set in and are therefore a basis for the subspace . If there are vectors in the orthogonal set, they form a basis for .    Consider the vectors    Verify that this set forms an orthogonal set of -dimensional vectors.     Explain why we know that this set of vectors forms a basis for .    Suppose that . Find the weights , , and that express as a linear combination using .    If we multiply a vector by a positive scalar , the length of is also multiplied by ; that is, .   unit vector Using this observation, find a vector that is parallel to and has length 1. Such vectors are called unit vectors .     Similarly, find a unit vector that is parallel to and a unit vector that is parallel to .    Construct the matrix and find the product . Use to explain your result.          We compute the dot products , , and .    We know that an orthogonal set of vectors is linearly independent. Therefore, we have a set of three linearly independent vectors in so they must form a basis for .    We find that .    Since , we find     We find that     We find since each entry in this matrix product is the dot product of two columns of .          We compute the dot products , , and .    An orthogonal set of vectors is linearly independent.     .         We find that             This activity introduces an important way of modifying an orthogonal set so that the vectors in the set have unit length. Recall that we may multiply any nonzero vector by a scalar so that the new vector has length 1. For instance, we know that if is a positive scalar, then . To obtain a vector having unit length, we want so that . Therefore, becomes a unit vector parallel to .  Orthogonal sets in which the vectors have unit length are called orthonormal and are especially convenient.   othonormal set   An orthonormal set is an orthogonal set of vectors each of which has unit length.      The vectors are an orthonormal set of vectors in and form an orthonormal basis for .  If we form the matrix we find that since tells us that     The previous activity and example illustrate the next proposition.    If the columns of the matrix form an orthonormal set, then , the identity matrix.      Orthogonal projections  We now turn to an important problem that will appear in many forms in the rest of our explorations. Suppose, as shown in , that we have a subspace of and a vector that is not in that subspace. We would like to find the vector in that is closest to , meaning the distance between and is as small as possible.      Given a plane in and a vector not in the plane, we wish to find the vector in the plane that is closest to .   To get started, let's consider a simpler problem where we have a line in , defined by the vector , and another vector that is not on the line, as shown on the left of . We wish to find , the vector on the line that is closest to , as illustrated in the right of .       Given a line and a vector , we seek the vector on that is closest to .   To find , we require that be orthogonal to . For instance, if is another vector on the line, as shown in , then the Pythagorean theorem implies that which means that . Therefore, is closer to than any other vector on the line .      The vector is closer to than because is orthogonal to .    orthogonal projection   Given a vector in and a subspace of , the orthogonal projection of onto is the vector in that is closest to . It is characterized by the property that is orthogonal to .      This activity demonstrates how to determine the orthogonal projection of a vector onto a subspace of .   Let's begin by considering a line , defined by the vector , and a vector not on , as illustrated in .       Finding the orthogonal projection of onto the line defined by .      To find , first notice that for some scalar . Since is orthogonal to , what do we know about the dot product     Apply the distributive property of dot products to find the scalar . What is the vector , the orthogonal projection of onto ?    More generally, explain why the orthogonal projection of onto the line defined by is        The same ideas apply more generally. Suppose we have an orthogonal set of vectors and that define a plane in . If another vector in , we seek the vector on the plane closest to . As before, the vector will be orthogonal to , as illustrated in .       Given a plane defined by the orthogonal vectors and and another vector , we seek the vector on closest to .      The vector is orthogonal to . What does this say about the dot products: and ?    Since is in the plane , we can write it as a linear combination . Then Find the weight by dotting with and applying the distributive property of dot products. Similarly, find the weight .    What is the vector , the orthogonal projection of onto the plane ?       Suppose that is a subspace of with orthogonal basis and that is a vector in . Explain why the orthogonal projection of onto is the vector     Suppose that is an orthonormal basis for ; that is, the vectors are orthogonal to one another and have unit length. Explain why the orthogonal projection is     If is the matrix whose columns are an orthonormal basis of , use to explain why .             This dot product should be 0 since the vectors are orthogonal.     .    As before,           These dot products are 0.                    We know and we can find by requiring that be orthogonal to every vector .    The vectors form an orthogonal set and since , the weights are .    We have so that              0     .               0                    We require that be orthogonal to every vector .         Use the fact that        In all the cases considered in the activity, we are looking for , the vector in a subspace closest to a vector , which is found by requiring that be orthogonal to . This means that for any vector in .  If we have an orthogonal basis for , then . Therefore, This leads to the projection formula:   Projection formula   If is a subspace of having an orthogonal basis and is a vector in , then the orthogonal projection of onto is      Caution  Remember that the projection formula given in applies only when the basis of is orthogonal .   If we have an orthonormal basis for , the projection formula simplifies to If we then form the matrix this expression may be succintly written   This leads to the following proposition.    If is an orthonormal basis for a subspace of , then the matrix transformation that projects vectors in orthogonally onto is represented by the matrix where       In the previous activity, we looked at the plane defined by the two orthogonal vectors We can form an orthonormal basis by scalar multiplying these vectors to have unit length: Using these vectors, we form the matrix The projection onto the plane is then given by the matrix   Let's check that this works by considering the vector and finding , its orthogonal projection onto the plane . In terms of the original basis and , the projection formula from tells us that   Alternatively, we use the matrix , as in , to find that          Suppose that is the line in defined by the vector .    Find an orthonormal basis for .    Construct the matrix and use it to construct the matrix that projects vectors orthogonally onto .    Use your matrix to find , the orthogonal projection of onto .    Find and explain its geometric significance.       The vectors form an orthogonal basis of , a two-dimensional subspace of .    Use the projection formula from to find , the orthogonal projection of onto .    Find an orthonormal basis and for and use it to construct the matrix that projects vectors orthogonally onto . Check that , the orthogonal projection you found in the previous part of this activity.    Find and explain its geometric significance.    Find a basis for .    Find a vector in such that     If is the matrix whose columns are and , find the product and explain your result.                               We find that , which makes sense because , a 1-dimensional subspace of .                     since     Since , then , which gives and     We can find      since this product computes the dot products between the columns of .                                                          and                     This activity demonstrates one issue of note. We found , the orthogonal projection of onto , by requiring that be orthogonal to . In other words, is a vector in the orthogonal complement , which we may denote . This explains the following proposition, which is illustrated in    If is a subspace of with orthogonal complement , then any -dimensional vector can be uniquely written as where is in and is in . The vector is the orthogonal projection of onto and is the orthogonal projection of onto .       A vector along with , its orthogonal projection onto the line , and , its orthogonal projection onto the orthogonal complement .   Let's summarize what we've found. If is a matrix whose columns form an orthonormal set in , then    , the identity matrix, because this product computes the dot products between the columns of .     is the matrix the projects vectors orthogonally onto , the subspace of spanned by .   As we've said before, matrix multiplication depends on the order in which we multiply the matrices, and we see this clearly here.  Because , there is a temptation to say that is invertible. This is usually not the case, however. Remember that an invertible matrix must be a square matrix, and the matrix will only be square if . In this case, there are vectors in the orthonormal set so the subspace spanned by the vectors is . If is a vector in , then is the orthogonal projection of onto . In other words, is the closest vector in to , and this closest vector must be itself. Therefore, , which means that . In this case, is an invertible matrix.    Consider the orthonormal set of vectors and the matrix they define In this case, and span a plane, a 2-dimensional subspace of . We know that and projects vectors orthogonally onto the plane. However, is not a square matrix so it cannot be invertible.      Now consider the orthonormal set of vectors and the matrix they define Here, , , and form a basis for so that both and . Therefore, is a square matrix and is invertible.  Moreover, since , we see that so finding the inverse of is as simple as writing its transpose. Matrices with this property are very special and will play an important role in our upcoming work. We will therefore give them a special name.      orthogonal matrix  A square matrix whose columns form an orthonormal basis for is called orthogonal .    This terminology can be a little confusing. We call a basis orthogonal if the basis vectors are orthogonal to one another. However, a matrix is orthogonal if the columns are orthogonal to one another and have unit length. It pays to keep this in mind when reading statements about orthogonal bases and orthogonal matrices. In the meantime, we record the following proposition.    An orthogonal matrix is invertible and its inverse .      Summary  This section introduced orthogonal sets and the projection formula that allows us to project vectors orthogonally onto a subspace.   Given an orthogonal set that spans an -dimensional subspace of , the orthogonal projection of onto is the vector in closest to and may be written as     If is an orthonormal basis of and is the matrix whose columns are , then the matrix projects vectors orthogonally onto .    If the columns of form an orthonormal basis for an -dimensional subspace of , then .    An orthogonal matrix is a square matrix whose columns form an orthonormal basis. In this case, so that .        Suppose that    Verify that and form an orthogonal basis for a plane in .    Use to find , the orthogonal projection of onto .    Find an orthonormal basis , for .    Find the matrix representing the matrix transformation that projects vectors in orthogonally onto . Verify that .    Determine and explain its geometric significance.          .     .     and                    Check that .    Applying the Projection Formula gives .     and     Form so that     Since , we have        Consider the vectors    Explain why these vectors form an orthogonal basis for .    Suppose that and evaluate the product . Why is this product a diagonal matrix and what is the significance of the diagonal entries?    Express the vector as a linear combination of , , and .    Multiply the vectors , , by appropriate scalars to find an orthonormal basis , , of .    If , find the matrix product and explain the result.          , , and .          .                   Check that all three dot products , , and .     . This matrix is diagonal because the vectors form an orthogonal set.    We may find the weights of this linear combination by finding so that .         The columns of form an orthonormal basis for so is orthogonal. Therefore, .       Suppose that form an orthogonal basis for a subspace of .   Find , the orthogonal projection of onto .    Find the vector in such that .    Find a basis for . and express as a linear combination of the basis vectors.                   A basis for is and . We then have .         Apply the Projection Formula to find .     .    Constructing a matrix whose columns are and allows us to find a basis for . This gives the basis and . We then have .       Consider the vectors    If is the line defined by the vector , find the vector in closest to . Call this vector .    If is the subspace spanned by and , find the vector in closest to . Call this vector .    Determine whether or is closer to and explain why.                             Applying the Projection Formula gives     Applying the Projection Formula gives      is the closest vector in to . Since is contained in , cannot be closer. Therefore, must be closer to than .       Suppose that defines a line in .   Find the orthogonal projections of the vectors , , onto .    Find the matrix .    Use to explain why the columns of are related to the orthogonal projections you found in the first part of this exericse.                   The columns of are the results of projecting the standard basis vectors onto .         Applying the Projection Formula gives the projections          If , then , which projects vectors orthogonally onto . The columns of are the results of projecting the standard basis vectors onto .       Suppose that form the basis for a plane in .   Find a basis for the line that is the orthogonal complement .    Given the vector , find , the orthogonal projection of onto the line .    Explain why the vector must be in and write as a linear combination of and .                    .         A basis vector is           is the orthogonal projection of onto so must be orthogonal to , which means that it is in . We see that .       Determine whether the following statements are true or false and explain your thinking.   If the columns of form an orthonormal basis for a subspace and is a vector in , then .    An orthogonal set of vectors in can have no more than 8 vectors.    If is a matrix whose columns are orthonormal, then .    If is a matrix whose columns are orthonormal, then .    If the orthogonal projection of onto a subspace satisfies , then is in .         True    True    False    True    True        True, because is the closest vector in to . Therefore, .    True, because the orthogonal set of vectors is linearly independent.    False, projects vectors orthogonally onto the 5-dimensional subspace .    True, because computes the dot products between the columns of     True, because . Therefore, is in .      Suppose that is an orthogonal matrix.   Remembering that , explain why     Explain why .  This means that the length of a vector is unchanged after multiplying by an orthogonal matrix.    If is a real eigenvalue of , explain why .          .         If , then so           .         If , then so        Explain why the following statements are true.   If is an orthogonal matrix, then .    If is a matrix whose columns are orthonormal, then is an matrix whose rank is 4.    If is the orthogonal projection of onto a subspace , then is the orthogonal projection of onto .         Since , we have .    The four columns of form a basis for the column space     If , then is in .         Since , we have     The columns of form an orthonormal basis for , a 4-dimensional subspace of . Therefore, projects vectors orthogonally onto so and .    If , then is in , the orthogonal complement of . This means that is the orthogonal projection of onto .       This exercise is about orthogonal matrices.   In , we saw that the matrix represents a rotation by an angle . Explain why this matrix is an orthogonal matrix.    We also saw that the matrix represents a reflection in a line. Explain why this matrix is an orthogonal matrix.    Suppose that is a 2-dimensional unit vector. Use a sketch to indicate all the possible vectors such that and form an orthonormal basis of .    Explain why every orthogonal matrix is either a rotation or a reflection.          .     .     or     The first column of has the form . Now apply the result of the last part of this problem.         If this matrix is , we have .    If this matrix is , we have .     or     If is an orthogonal matrix, then is a unit vector and has the form for some angle . By the last part of this problem, there are only two choices for , one of which gives a rotation and one of which gives a reflection.       "
},
{
  "id": "preview-orthogonal-basis",
  "level": "2",
  "url": "sec-orthogonal-bases.html#preview-orthogonal-basis",
  "type": "Preview Activity",
  "number": "6.3.1",
  "title": "",
  "body": "  For this activity, it will be helpful to recall the distributive property of dot products: We'll work with the basis of formed by the vectors    Verify that the vectors and are orthogonal.    Suppose that and find the dot products and .    We would like to express as a linear combination of and , which means that we need to find weights and such that To find the weight , dot both sides of this expression with : and apply the distributive property.    In a similar fashion, find the weight .    Verify that using the weights you have found.          We can compute that .     and .     .     .     .      "
},
{
  "id": "definition-30",
  "level": "2",
  "url": "sec-orthogonal-bases.html#definition-30",
  "type": "Definition",
  "number": "6.3.1",
  "title": "",
  "body": " orthogonal set   By an orthogonal set of vectors, we mean a set of nonzero vectors each of which is orthogonal to the others.   "
},
{
  "id": "example-orthogonal-basis",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-orthogonal-basis",
  "type": "Example",
  "number": "6.3.2",
  "title": "",
  "body": "  The 3-dimensional vectors form an orthogonal set, which can be verified by computing Notice that this set of vectors forms a basis for .   "
},
{
  "id": "example-orthogonal-set",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-orthogonal-set",
  "type": "Example",
  "number": "6.3.3",
  "title": "",
  "body": "  The vectors form an orthogonal set of 4-dimensional vectors. Since there are only three vectors, this set does not form a basis for . It does, however, form a basis for a 3-dimensional subspace of .   "
},
{
  "id": "prop-orthog-lincomb",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-lincomb",
  "type": "Proposition",
  "number": "6.3.4",
  "title": "",
  "body": "  If a vector is a linear combination of an orthogonal set of vectors , then    "
},
{
  "id": "prop-orthog-lin-indep",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-lin-indep",
  "type": "Proposition",
  "number": "6.3.5",
  "title": "",
  "body": "  An orthogonal set of vectors is linearly independent.   "
},
{
  "id": "activity-76",
  "level": "2",
  "url": "sec-orthogonal-bases.html#activity-76",
  "type": "Activity",
  "number": "6.3.2",
  "title": "",
  "body": "  Consider the vectors    Verify that this set forms an orthogonal set of -dimensional vectors.     Explain why we know that this set of vectors forms a basis for .    Suppose that . Find the weights , , and that express as a linear combination using .    If we multiply a vector by a positive scalar , the length of is also multiplied by ; that is, .   unit vector Using this observation, find a vector that is parallel to and has length 1. Such vectors are called unit vectors .     Similarly, find a unit vector that is parallel to and a unit vector that is parallel to .    Construct the matrix and find the product . Use to explain your result.          We compute the dot products , , and .    We know that an orthogonal set of vectors is linearly independent. Therefore, we have a set of three linearly independent vectors in so they must form a basis for .    We find that .    Since , we find     We find that     We find since each entry in this matrix product is the dot product of two columns of .          We compute the dot products , , and .    An orthogonal set of vectors is linearly independent.     .         We find that            "
},
{
  "id": "definition-31",
  "level": "2",
  "url": "sec-orthogonal-bases.html#definition-31",
  "type": "Definition",
  "number": "6.3.6",
  "title": "",
  "body": " othonormal set   An orthonormal set is an orthogonal set of vectors each of which has unit length.   "
},
{
  "id": "example-62",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-62",
  "type": "Example",
  "number": "6.3.7",
  "title": "",
  "body": "  The vectors are an orthonormal set of vectors in and form an orthonormal basis for .  If we form the matrix we find that since tells us that    "
},
{
  "id": "prop-orthonormal-QTQ",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthonormal-QTQ",
  "type": "Proposition",
  "number": "6.3.8",
  "title": "",
  "body": "  If the columns of the matrix form an orthonormal set, then , the identity matrix.   "
},
{
  "id": "fig-3d-orthog-proj",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-3d-orthog-proj",
  "type": "Figure",
  "number": "6.3.9",
  "title": "",
  "body": "    Given a plane in and a vector not in the plane, we wish to find the vector in the plane that is closest to .  "
},
{
  "id": "fig-projection-line-a",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-projection-line-a",
  "type": "Figure",
  "number": "6.3.10",
  "title": "",
  "body": "     Given a line and a vector , we seek the vector on that is closest to .  "
},
{
  "id": "fig-projection-line-b",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-projection-line-b",
  "type": "Figure",
  "number": "6.3.11",
  "title": "",
  "body": "    The vector is closer to than because is orthogonal to .  "
},
{
  "id": "definition-32",
  "level": "2",
  "url": "sec-orthogonal-bases.html#definition-32",
  "type": "Definition",
  "number": "6.3.12",
  "title": "",
  "body": " orthogonal projection   Given a vector in and a subspace of , the orthogonal projection of onto is the vector in that is closest to . It is characterized by the property that is orthogonal to .   "
},
{
  "id": "activity-77",
  "level": "2",
  "url": "sec-orthogonal-bases.html#activity-77",
  "type": "Activity",
  "number": "6.3.3",
  "title": "",
  "body": "  This activity demonstrates how to determine the orthogonal projection of a vector onto a subspace of .   Let's begin by considering a line , defined by the vector , and a vector not on , as illustrated in .       Finding the orthogonal projection of onto the line defined by .      To find , first notice that for some scalar . Since is orthogonal to , what do we know about the dot product     Apply the distributive property of dot products to find the scalar . What is the vector , the orthogonal projection of onto ?    More generally, explain why the orthogonal projection of onto the line defined by is        The same ideas apply more generally. Suppose we have an orthogonal set of vectors and that define a plane in . If another vector in , we seek the vector on the plane closest to . As before, the vector will be orthogonal to , as illustrated in .       Given a plane defined by the orthogonal vectors and and another vector , we seek the vector on closest to .      The vector is orthogonal to . What does this say about the dot products: and ?    Since is in the plane , we can write it as a linear combination . Then Find the weight by dotting with and applying the distributive property of dot products. Similarly, find the weight .    What is the vector , the orthogonal projection of onto the plane ?       Suppose that is a subspace of with orthogonal basis and that is a vector in . Explain why the orthogonal projection of onto is the vector     Suppose that is an orthonormal basis for ; that is, the vectors are orthogonal to one another and have unit length. Explain why the orthogonal projection is     If is the matrix whose columns are an orthonormal basis of , use to explain why .             This dot product should be 0 since the vectors are orthogonal.     .    As before,           These dot products are 0.                    We know and we can find by requiring that be orthogonal to every vector .    The vectors form an orthogonal set and since , the weights are .    We have so that              0     .               0                    We require that be orthogonal to every vector .         Use the fact that       "
},
{
  "id": "prop-proj-formula",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-proj-formula",
  "type": "Proposition",
  "number": "6.3.15",
  "title": "Projection formula.",
  "body": " Projection formula   If is a subspace of having an orthogonal basis and is a vector in , then the orthogonal projection of onto is    "
},
{
  "id": "prop-proj-orthonormal",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-proj-orthonormal",
  "type": "Proposition",
  "number": "6.3.16",
  "title": "",
  "body": "  If is an orthonormal basis for a subspace of , then the matrix transformation that projects vectors in orthogonally onto is represented by the matrix where    "
},
{
  "id": "example-projection-matrix",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-projection-matrix",
  "type": "Example",
  "number": "6.3.17",
  "title": "",
  "body": "  In the previous activity, we looked at the plane defined by the two orthogonal vectors We can form an orthonormal basis by scalar multiplying these vectors to have unit length: Using these vectors, we form the matrix The projection onto the plane is then given by the matrix   Let's check that this works by considering the vector and finding , its orthogonal projection onto the plane . In terms of the original basis and , the projection formula from tells us that   Alternatively, we use the matrix , as in , to find that    "
},
{
  "id": "activity-78",
  "level": "2",
  "url": "sec-orthogonal-bases.html#activity-78",
  "type": "Activity",
  "number": "6.3.4",
  "title": "",
  "body": "     Suppose that is the line in defined by the vector .    Find an orthonormal basis for .    Construct the matrix and use it to construct the matrix that projects vectors orthogonally onto .    Use your matrix to find , the orthogonal projection of onto .    Find and explain its geometric significance.       The vectors form an orthogonal basis of , a two-dimensional subspace of .    Use the projection formula from to find , the orthogonal projection of onto .    Find an orthonormal basis and for and use it to construct the matrix that projects vectors orthogonally onto . Check that , the orthogonal projection you found in the previous part of this activity.    Find and explain its geometric significance.    Find a basis for .    Find a vector in such that     If is the matrix whose columns are and , find the product and explain your result.                               We find that , which makes sense because , a 1-dimensional subspace of .                     since     Since , then , which gives and     We can find      since this product computes the dot products between the columns of .                                                          and                    "
},
{
  "id": "prop-orthog-decomp",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-decomp",
  "type": "Proposition",
  "number": "6.3.18",
  "title": "",
  "body": " If is a subspace of with orthogonal complement , then any -dimensional vector can be uniquely written as where is in and is in . The vector is the orthogonal projection of onto and is the orthogonal projection of onto .  "
},
{
  "id": "fig-orthog-decomp",
  "level": "2",
  "url": "sec-orthogonal-bases.html#fig-orthog-decomp",
  "type": "Figure",
  "number": "6.3.19",
  "title": "",
  "body": "    A vector along with , its orthogonal projection onto the line , and , its orthogonal projection onto the orthogonal complement .  "
},
{
  "id": "example-64",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-64",
  "type": "Example",
  "number": "6.3.20",
  "title": "",
  "body": "  Consider the orthonormal set of vectors and the matrix they define In this case, and span a plane, a 2-dimensional subspace of . We know that and projects vectors orthogonally onto the plane. However, is not a square matrix so it cannot be invertible.   "
},
{
  "id": "example-65",
  "level": "2",
  "url": "sec-orthogonal-bases.html#example-65",
  "type": "Example",
  "number": "6.3.21",
  "title": "",
  "body": "  Now consider the orthonormal set of vectors and the matrix they define Here, , , and form a basis for so that both and . Therefore, is a square matrix and is invertible.  Moreover, since , we see that so finding the inverse of is as simple as writing its transpose. Matrices with this property are very special and will play an important role in our upcoming work. We will therefore give them a special name.   "
},
{
  "id": "definition-33",
  "level": "2",
  "url": "sec-orthogonal-bases.html#definition-33",
  "type": "Definition",
  "number": "6.3.22",
  "title": "",
  "body": "  orthogonal matrix  A square matrix whose columns form an orthonormal basis for is called orthogonal .   "
},
{
  "id": "prop-orthog-matrix",
  "level": "2",
  "url": "sec-orthogonal-bases.html#prop-orthog-matrix",
  "type": "Proposition",
  "number": "6.3.23",
  "title": "",
  "body": "  An orthogonal matrix is invertible and its inverse .   "
},
{
  "id": "exercise-219",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-219",
  "type": "Exercise",
  "number": "6.3.4.1",
  "title": "",
  "body": " Suppose that    Verify that and form an orthogonal basis for a plane in .    Use to find , the orthogonal projection of onto .    Find an orthonormal basis , for .    Find the matrix representing the matrix transformation that projects vectors in orthogonally onto . Verify that .    Determine and explain its geometric significance.          .     .     and                    Check that .    Applying the Projection Formula gives .     and     Form so that     Since , we have      "
},
{
  "id": "exercise-220",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-220",
  "type": "Exercise",
  "number": "6.3.4.2",
  "title": "",
  "body": " Consider the vectors    Explain why these vectors form an orthogonal basis for .    Suppose that and evaluate the product . Why is this product a diagonal matrix and what is the significance of the diagonal entries?    Express the vector as a linear combination of , , and .    Multiply the vectors , , by appropriate scalars to find an orthonormal basis , , of .    If , find the matrix product and explain the result.          , , and .          .                   Check that all three dot products , , and .     . This matrix is diagonal because the vectors form an orthogonal set.    We may find the weights of this linear combination by finding so that .         The columns of form an orthonormal basis for so is orthogonal. Therefore, .     "
},
{
  "id": "exercise-221",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-221",
  "type": "Exercise",
  "number": "6.3.4.3",
  "title": "",
  "body": " Suppose that form an orthogonal basis for a subspace of .   Find , the orthogonal projection of onto .    Find the vector in such that .    Find a basis for . and express as a linear combination of the basis vectors.                   A basis for is and . We then have .         Apply the Projection Formula to find .     .    Constructing a matrix whose columns are and allows us to find a basis for . This gives the basis and . We then have .     "
},
{
  "id": "exercise-222",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-222",
  "type": "Exercise",
  "number": "6.3.4.4",
  "title": "",
  "body": " Consider the vectors    If is the line defined by the vector , find the vector in closest to . Call this vector .    If is the subspace spanned by and , find the vector in closest to . Call this vector .    Determine whether or is closer to and explain why.                             Applying the Projection Formula gives     Applying the Projection Formula gives      is the closest vector in to . Since is contained in , cannot be closer. Therefore, must be closer to than .     "
},
{
  "id": "exercise-223",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-223",
  "type": "Exercise",
  "number": "6.3.4.5",
  "title": "",
  "body": " Suppose that defines a line in .   Find the orthogonal projections of the vectors , , onto .    Find the matrix .    Use to explain why the columns of are related to the orthogonal projections you found in the first part of this exericse.                   The columns of are the results of projecting the standard basis vectors onto .         Applying the Projection Formula gives the projections          If , then , which projects vectors orthogonally onto . The columns of are the results of projecting the standard basis vectors onto .     "
},
{
  "id": "exercise-224",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-224",
  "type": "Exercise",
  "number": "6.3.4.6",
  "title": "",
  "body": " Suppose that form the basis for a plane in .   Find a basis for the line that is the orthogonal complement .    Given the vector , find , the orthogonal projection of onto the line .    Explain why the vector must be in and write as a linear combination of and .                    .         A basis vector is           is the orthogonal projection of onto so must be orthogonal to , which means that it is in . We see that .     "
},
{
  "id": "exercise-225",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-225",
  "type": "Exercise",
  "number": "6.3.4.7",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If the columns of form an orthonormal basis for a subspace and is a vector in , then .    An orthogonal set of vectors in can have no more than 8 vectors.    If is a matrix whose columns are orthonormal, then .    If is a matrix whose columns are orthonormal, then .    If the orthogonal projection of onto a subspace satisfies , then is in .         True    True    False    True    True        True, because is the closest vector in to . Therefore, .    True, because the orthogonal set of vectors is linearly independent.    False, projects vectors orthogonally onto the 5-dimensional subspace .    True, because computes the dot products between the columns of     True, because . Therefore, is in .    "
},
{
  "id": "exercise-226",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-226",
  "type": "Exercise",
  "number": "6.3.4.8",
  "title": "",
  "body": " Suppose that is an orthogonal matrix.   Remembering that , explain why     Explain why .  This means that the length of a vector is unchanged after multiplying by an orthogonal matrix.    If is a real eigenvalue of , explain why .          .         If , then so           .         If , then so      "
},
{
  "id": "exercise-227",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-227",
  "type": "Exercise",
  "number": "6.3.4.9",
  "title": "",
  "body": " Explain why the following statements are true.   If is an orthogonal matrix, then .    If is a matrix whose columns are orthonormal, then is an matrix whose rank is 4.    If is the orthogonal projection of onto a subspace , then is the orthogonal projection of onto .         Since , we have .    The four columns of form a basis for the column space     If , then is in .         Since , we have     The columns of form an orthonormal basis for , a 4-dimensional subspace of . Therefore, projects vectors orthogonally onto so and .    If , then is in , the orthogonal complement of . This means that is the orthogonal projection of onto .     "
},
{
  "id": "exercise-228",
  "level": "2",
  "url": "sec-orthogonal-bases.html#exercise-228",
  "type": "Exercise",
  "number": "6.3.4.10",
  "title": "",
  "body": " This exercise is about orthogonal matrices.   In , we saw that the matrix represents a rotation by an angle . Explain why this matrix is an orthogonal matrix.    We also saw that the matrix represents a reflection in a line. Explain why this matrix is an orthogonal matrix.    Suppose that is a 2-dimensional unit vector. Use a sketch to indicate all the possible vectors such that and form an orthonormal basis of .    Explain why every orthogonal matrix is either a rotation or a reflection.          .     .     or     The first column of has the form . Now apply the result of the last part of this problem.         If this matrix is , we have .    If this matrix is , we have .     or     If is an orthogonal matrix, then is a unit vector and has the form for some angle . By the last part of this problem, there are only two choices for , one of which gives a rotation and one of which gives a reflection.     "
},
{
  "id": "sec-gram-schmidt",
  "level": "1",
  "url": "sec-gram-schmidt.html",
  "type": "Section",
  "number": "6.4",
  "title": "Finding orthogonal bases",
  "body": " Finding orthogonal bases   The last section demonstrated the value of working with orthogonal, and especially orthonormal, sets. If we have an orthogonal basis for a subspace , the Projection Formula tells us that the orthogonal projection of a vector onto is An orthonormal basis is even more convenient: after forming the matrix , we have .  In the examples we've seen so far, however, orthogonal bases were given to us. What we need now is a way to form orthogonal bases. In this section, we'll explore an algorithm that begins with a basis for a subspace and creates an orthogonal basis. Once we have an orthogonal basis, we can scale each of the vectors appropriately to produce an orthonormal basis.    Suppose we have a basis for consisting of the vectors as shown in . Notice that this basis is not orthogonal.      A basis for .      Find the vector that is the orthogonal projection of onto the line defined by .    Explain why is orthogonal to .    Define the new vectors and and sketch them in . Explain why and define an orthogonal basis for .      Sketch the new basis and .     Write the vector as a linear combination of and .    Scale the vectors and to produce an orthonormal basis and for .           .    The orthogonal projection is defined so that is orthogonal to .     and are othogonal, which can be checked with the dot product.    The projection formula gives .     and .         Gram-Schmidt orthogonalization   Gram-Schmidt The preview activity illustrates the main idea behind an algorithm, known as Gram-Schmidt orthogonalization , that begins with a basis for some subspace of and produces an orthogonal or orthonormal basis. The algorithm relies on our construction of the orthogonal projection. Remember that we formed the orthogonal projection of onto a subspace by requiring that is orthogonal to as shown in .      If is the orthogonal projection of onto , then is orthogonal to .   This observation guides our construction of an orthogonal basis for it allows us to create a vector that is orthogonal to a given subspace. Let's see how the Gram-Schmidt algorithm works.    Suppose that is a three-dimensional subspace of with basis: We can see that this basis is not orthogonal by noting that . Our goal is to create an orthogonal basis , , and for .  To begin, we declare that , and we call the line defined by .      Find the vector that is the orthogonal projection of onto , the line defined by .    Form the vector and verify that it is orthogonal to .    Explain why by showing that any linear combination of and can be written as a linear combination of and and vice versa.    The vectors and are an orthogonal basis for a two-dimensional subspace of . Find the vector that is the orthogonal projection of onto .    Verify that is orthogonal to both and .    Explain why , , and form an orthogonal basis for .    Now find an orthonormal basis for .           .         We have and . Therefore, a linear combination of and can be rewritten as In the same way, and so any linear combination of and can be rewritten as a linear combination of and .    By the Projection Formula, .         We can check that , , and form an orthogonal set. Since can be written in terms of and vice-versa, these new vectors form a basis for .                .         We have and so a linear combination of and can be rewritten as a linear combination of and .     .         Since can be written in terms of and vice-versa, these new vectors form a basis for .            As this activity illustrates, Gram-Schmidt orthogonalization begins with a basis for a subspace of and creates an orthogonal basis for . Let's work through a second example.    Let's start with the basis which is a basis for .  To get started, we'll simply set . We construct from by subtracting its orthogonal projection onto , the line defined by . This gives   Notice that we found . Therefore, we can rewrite any linear combination of and as a linear combination of and . This tells us that In other words, and is a orthogonal basis for , the 2-dimensional subspace that is the span of and .  Finally, we form from by subtracting its orthogonal projection onto :   We can now check that is an orthogonal set. Furthermore, we have, as before, , which says that we have found a new orthogonal basis for .  To create an orthonormal basis, we form unit vectors parallel to each of the vectors in the orthogonal basis:     More generally, if we have a basis for a subspace of , the Gram-Schmidt algorithm creates an orthogonal basis for in the following way:   From here, we may form an orthonormal basis by constructing a unit vector parallel to each vector in the orthogonal basis: .    Sage can automate these computations for us. Before we begin, however, it will be helpful to understand how we can combine things using a list in Python. For instance, if the vectors v1 , v2 , and v3 form a basis for a subspace, we can bundle them together using square brackets: [v1, v2, v3] . Furthermore, we could assign this to a variable, such as basis = [v1, v2, v3] .  Evaluating the following cell will load in some special commands.    There is a command to apply the projection formula: projection(b, basis) returns the orthogonal projection of b onto the subspace spanned by basis , which is a list of vectors.    The command unit(w) returns a unit vector parallel to w .    Given a collection of vectors, say, v1 and v2 , we can form the matrix whose columns are v1 and v2 using matrix([v1, v2]).T . When given a list of vectors, Sage constructs a matrix whose rows are the given vectors. For this reason, we need to apply the transpose.     Let's now consider , the subspace of having basis    Apply the Gram-Schmidt algorithm to find an orthogonal basis , , and for .     Find , the orthogonal projection of onto .    Explain why we know that is a linear combination of the original vectors , , and and then find weights so that     Find an orthonormal basis , , for for and form the matrix whose columns are these vectors.     Find the product and explain the result.    Find the matrix that projects vectors orthogonally onto and verify that gives , the orthogonal projection that you found earlier.                    We know that is in and , , and is a basis for . We find .          since the matrix product computes the dot products of the columns of .                          .                         factorizations  Now that we've seen how the Gram-Schmidt algorithm forms an orthonormal basis for a given subspace, we will explore how the algorithm leads to an important matrix factorization known as the factorization.    Suppose that is the matrix whose columns are These vectors form a basis for , the subspace of that we encountered in . Since these vectors are the columns of , we have .    When we implemented Gram-Schmidt, we first found an orthogonal basis , , and using Use these expressions to write , , and as linear combinations of , , and .    We next normalized the orthogonal basis , , and to obtain an orthonormal basis , , and .  Write the vectors as scalar multiples of . Then use these expressions to write , , and as linear combinations of , , and .    Suppose that . Use the result of the previous part to find a vector so that .    Then find vectors and such that and .    Construct the matrix . Remembering that , explain why .    What is special about the shape of ?    Suppose that is a matrix whose columns are linearly independent. This means that the columns of form a basis for , a 6-dimensional subspace of . Suppose that we apply Gram-Schmidt orthogonalization to create an orthonormal basis whose vectors form the columns of and that we write . What are the shape of and what the shape of ?           Therefore,      so we have This leads to     Since , we have .    In the same way, we have and .    We have .     so is upper triangular.     will be and will be a upper triangular matrix.                     .     and .    We have .     is upper triangular.     will be and will be .       When the columns of a matrix are linearly independent, they form a basis for so that we can perform the Gram-Schmidt algorithm. The previous activity shows how this leads to a factorization of as the product of a matrix whose columns are an orthonormal basis for and an upper triangular matrix .    factorization  If is an matrix whose columns are linearly independent, we may write where is an matrix whose columns form an orthonormal basis for and is an upper triangular matrix.     We'll consider the matrix whose columns, which we'll denote , , and , are the basis of that we considered in . There we found an orthogonal basis , , and that satisfied   In terms of the resulting orthonormal basis , , and , we had so that   Therefore, if , we have the factorization     The value of the factorization will become clear in the next section where we use it to solve least-squares problems.    As before, we would like to use Sage to automate the process of finding and using the factorization of a matrix . Evaluating the following cell provides a command QR(A) that returns the factorization, which may be stored using, for example, Q, R = QR(A) .   Suppose that is the following matrix whose columns are linearly independent.    If , what is the shape of and ? What is special about the form of ?    Find the factorization using Q, R = QR(A) and verify that has the predicted shape and that .     Find the matrix that orthogonally projects vectors onto .    Find , the orthogonal projection of onto .    Explain why the equation must be consistent and then find .           is and is a upper triangular matrix.    We see that               Since is in , the system must be consistent. We find a solution by augmenting by and row reducing: .           is and is a upper triangular matrix.    We see that                .       In fact, Sage provides its own version of the factorization that is a bit different than the way we've developed the factorization here. For this reason, we have provided our own version of the factorization.    Summary  This section explored the Gram-Schmidt orthogonalization algorithm and how it leads to the matrix factorization when the columns of are linearly independent.   Beginning with a basis for a subspace of , the vectors form an orthogonal basis for .    We may scale each vector appropriately to obtain an orthonormal basis .    Expressing the Gram-Schmidt algorithm in matrix form shows that, if the columns of are linearly independent, then we can write , where the columns of form an orthonormal basis for and is upper triangular.        Suppose that a subspace of has a basis formed by    Find an orthogonal basis for .    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find the orthogonal projection of onto .          and      and                     and      and                  Find the factorization of .    and    Applying Gram-Schmidt, we have and , which leads to and . It therefore follows that and .  This leads to where and .    Consider the basis of given by the vectors     Apply the Gram-Schmit orthogonalization algorithm to find an orthonormal basis , , for .    If is the whose columns are , , and , find the factorization of .    Suppose that we want to solve the equation , which we can rewrite as .   If we set , the equation becomes . Explain how to solve the equation in a computationally efficient manner.    Explain how to solve the equation in a computationally efficient manner.    Find the solution by first solving and then .                          .     is upper triangular                 We find that             Since , we have .     is upper triangular so this equation can be solved using back substitution.               Consider the vectors and the subspace of that they span.    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find , the orthogonal projection of onto .    Express as a linear combination of , , and .                                  Applying Gram-Schmidt gives us     Let be the matrix whose columns are the orthonormal basis that results from Gram-Schmidt. Then                  Consider the set of vectors    What happens when we apply the Gram-Schmit orthogonalization algorithm?    Why does the algorithm fail to produce an orthogonal basis for ?         We find .    This set of vectors is linearly dependent.         We find Since , this does not produce a basis for .    The vector so this set of vectors is linearly dependent. Since is in the span of and , projecting into that subspace gives so that .       Suppose that is a matrix with linearly independent columns and having the factorization . Determine whether the following statements are true or false and explain your thinking.   It follows that .    The matrix is invertible.    The product projects vectors orthogonally onto .    The columns of are an orthogonal basis for .    The orthogonal complement .        True  True  False  True  True        True. Since , we have .    True. Since is upper triangular and the diagonal entries of are the lengths of the nonzero vectors , we have , which means that is invertible.    False, because .    True. In fact, they are an orthonormal basis for .    True. If , then for every vector in an orthonormal basis of . Therefore, is orthogonal to .       Suppose we have the factorization , where is a matrix.   What is the shape of the product ? Explain the significance of this product.    What is the shape of the product ? Explain the significance of this product.    What is the shape of the matrix ?    If is a diagonal matrix, what can you say about the columns of ?                        They form an orthogonal set.          is and projects vectors orthogonally onto .     is the identity matrix because the product computes dot products between the columns of .     is a upper triangular matrix.    The columns of form an orthogonal set.       Suppose we have the factorization where the columns of are and the columns of are .   How can the matrix product be expressed in terms of dot products?    How can the matrix product be expressed in terms of dot products?    Explain why .    Explain why the dot products .                        This follows from the previous parts of this exercise.         The entries of are the dot products .    The entries of are the dot products .         This follows from the previous parts of this exercise.       "
},
{
  "id": "exploration-24",
  "level": "2",
  "url": "sec-gram-schmidt.html#exploration-24",
  "type": "Preview Activity",
  "number": "6.4.1",
  "title": "",
  "body": "  Suppose we have a basis for consisting of the vectors as shown in . Notice that this basis is not orthogonal.      A basis for .      Find the vector that is the orthogonal projection of onto the line defined by .    Explain why is orthogonal to .    Define the new vectors and and sketch them in . Explain why and define an orthogonal basis for .      Sketch the new basis and .     Write the vector as a linear combination of and .    Scale the vectors and to produce an orthonormal basis and for .           .    The orthogonal projection is defined so that is orthogonal to .     and are othogonal, which can be checked with the dot product.    The projection formula gives .     and .      "
},
{
  "id": "fig-proj-orthog",
  "level": "2",
  "url": "sec-gram-schmidt.html#fig-proj-orthog",
  "type": "Figure",
  "number": "6.4.3",
  "title": "",
  "body": "    If is the orthogonal projection of onto , then is orthogonal to .  "
},
{
  "id": "activity-gram-schmidt",
  "level": "2",
  "url": "sec-gram-schmidt.html#activity-gram-schmidt",
  "type": "Activity",
  "number": "6.4.2",
  "title": "",
  "body": "  Suppose that is a three-dimensional subspace of with basis: We can see that this basis is not orthogonal by noting that . Our goal is to create an orthogonal basis , , and for .  To begin, we declare that , and we call the line defined by .      Find the vector that is the orthogonal projection of onto , the line defined by .    Form the vector and verify that it is orthogonal to .    Explain why by showing that any linear combination of and can be written as a linear combination of and and vice versa.    The vectors and are an orthogonal basis for a two-dimensional subspace of . Find the vector that is the orthogonal projection of onto .    Verify that is orthogonal to both and .    Explain why , , and form an orthogonal basis for .    Now find an orthonormal basis for .           .         We have and . Therefore, a linear combination of and can be rewritten as In the same way, and so any linear combination of and can be rewritten as a linear combination of and .    By the Projection Formula, .         We can check that , , and form an orthogonal set. Since can be written in terms of and vice-versa, these new vectors form a basis for .                .         We have and so a linear combination of and can be rewritten as a linear combination of and .     .         Since can be written in terms of and vice-versa, these new vectors form a basis for .           "
},
{
  "id": "example-gram-schmidt",
  "level": "2",
  "url": "sec-gram-schmidt.html#example-gram-schmidt",
  "type": "Example",
  "number": "6.4.4",
  "title": "",
  "body": "  Let's start with the basis which is a basis for .  To get started, we'll simply set . We construct from by subtracting its orthogonal projection onto , the line defined by . This gives   Notice that we found . Therefore, we can rewrite any linear combination of and as a linear combination of and . This tells us that In other words, and is a orthogonal basis for , the 2-dimensional subspace that is the span of and .  Finally, we form from by subtracting its orthogonal projection onto :   We can now check that is an orthogonal set. Furthermore, we have, as before, , which says that we have found a new orthogonal basis for .  To create an orthonormal basis, we form unit vectors parallel to each of the vectors in the orthogonal basis:    "
},
{
  "id": "activity-80",
  "level": "2",
  "url": "sec-gram-schmidt.html#activity-80",
  "type": "Activity",
  "number": "6.4.3",
  "title": "",
  "body": "  Sage can automate these computations for us. Before we begin, however, it will be helpful to understand how we can combine things using a list in Python. For instance, if the vectors v1 , v2 , and v3 form a basis for a subspace, we can bundle them together using square brackets: [v1, v2, v3] . Furthermore, we could assign this to a variable, such as basis = [v1, v2, v3] .  Evaluating the following cell will load in some special commands.    There is a command to apply the projection formula: projection(b, basis) returns the orthogonal projection of b onto the subspace spanned by basis , which is a list of vectors.    The command unit(w) returns a unit vector parallel to w .    Given a collection of vectors, say, v1 and v2 , we can form the matrix whose columns are v1 and v2 using matrix([v1, v2]).T . When given a list of vectors, Sage constructs a matrix whose rows are the given vectors. For this reason, we need to apply the transpose.     Let's now consider , the subspace of having basis    Apply the Gram-Schmidt algorithm to find an orthogonal basis , , and for .     Find , the orthogonal projection of onto .    Explain why we know that is a linear combination of the original vectors , , and and then find weights so that     Find an orthonormal basis , , for for and form the matrix whose columns are these vectors.     Find the product and explain the result.    Find the matrix that projects vectors orthogonally onto and verify that gives , the orthogonal projection that you found earlier.                    We know that is in and , , and is a basis for . We find .          since the matrix product computes the dot products of the columns of .                          .                     "
},
{
  "id": "activity-81",
  "level": "2",
  "url": "sec-gram-schmidt.html#activity-81",
  "type": "Activity",
  "number": "6.4.4",
  "title": "",
  "body": "  Suppose that is the matrix whose columns are These vectors form a basis for , the subspace of that we encountered in . Since these vectors are the columns of , we have .    When we implemented Gram-Schmidt, we first found an orthogonal basis , , and using Use these expressions to write , , and as linear combinations of , , and .    We next normalized the orthogonal basis , , and to obtain an orthonormal basis , , and .  Write the vectors as scalar multiples of . Then use these expressions to write , , and as linear combinations of , , and .    Suppose that . Use the result of the previous part to find a vector so that .    Then find vectors and such that and .    Construct the matrix . Remembering that , explain why .    What is special about the shape of ?    Suppose that is a matrix whose columns are linearly independent. This means that the columns of form a basis for , a 6-dimensional subspace of . Suppose that we apply Gram-Schmidt orthogonalization to create an orthonormal basis whose vectors form the columns of and that we write . What are the shape of and what the shape of ?           Therefore,      so we have This leads to     Since , we have .    In the same way, we have and .    We have .     so is upper triangular.     will be and will be a upper triangular matrix.                     .     and .    We have .     is upper triangular.     will be and will be .      "
},
{
  "id": "prop-qr",
  "level": "2",
  "url": "sec-gram-schmidt.html#prop-qr",
  "type": "Proposition",
  "number": "6.4.5",
  "title": "<span class=\"process-math\">\\(QR\\)<\/span> factorization.",
  "body": "  factorization  If is an matrix whose columns are linearly independent, we may write where is an matrix whose columns form an orthonormal basis for and is an upper triangular matrix.  "
},
{
  "id": "example-67",
  "level": "2",
  "url": "sec-gram-schmidt.html#example-67",
  "type": "Example",
  "number": "6.4.6",
  "title": "",
  "body": "  We'll consider the matrix whose columns, which we'll denote , , and , are the basis of that we considered in . There we found an orthogonal basis , , and that satisfied   In terms of the resulting orthonormal basis , , and , we had so that   Therefore, if , we have the factorization    "
},
{
  "id": "activity-82",
  "level": "2",
  "url": "sec-gram-schmidt.html#activity-82",
  "type": "Activity",
  "number": "6.4.5",
  "title": "",
  "body": "  As before, we would like to use Sage to automate the process of finding and using the factorization of a matrix . Evaluating the following cell provides a command QR(A) that returns the factorization, which may be stored using, for example, Q, R = QR(A) .   Suppose that is the following matrix whose columns are linearly independent.    If , what is the shape of and ? What is special about the form of ?    Find the factorization using Q, R = QR(A) and verify that has the predicted shape and that .     Find the matrix that orthogonally projects vectors onto .    Find , the orthogonal projection of onto .    Explain why the equation must be consistent and then find .           is and is a upper triangular matrix.    We see that               Since is in , the system must be consistent. We find a solution by augmenting by and row reducing: .           is and is a upper triangular matrix.    We see that                .      "
},
{
  "id": "exercise-229",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-229",
  "type": "Exercise",
  "number": "6.4.4.1",
  "title": "",
  "body": " Suppose that a subspace of has a basis formed by    Find an orthogonal basis for .    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find the orthogonal projection of onto .          and      and                     and      and                "
},
{
  "id": "exercise-230",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-230",
  "type": "Exercise",
  "number": "6.4.4.2",
  "title": "",
  "body": " Find the factorization of .    and    Applying Gram-Schmidt, we have and , which leads to and . It therefore follows that and .  This leads to where and .  "
},
{
  "id": "exercise-231",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-231",
  "type": "Exercise",
  "number": "6.4.4.3",
  "title": "",
  "body": " Consider the basis of given by the vectors     Apply the Gram-Schmit orthogonalization algorithm to find an orthonormal basis , , for .    If is the whose columns are , , and , find the factorization of .    Suppose that we want to solve the equation , which we can rewrite as .   If we set , the equation becomes . Explain how to solve the equation in a computationally efficient manner.    Explain how to solve the equation in a computationally efficient manner.    Find the solution by first solving and then .                          .     is upper triangular                 We find that             Since , we have .     is upper triangular so this equation can be solved using back substitution.             "
},
{
  "id": "exercise-232",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-232",
  "type": "Exercise",
  "number": "6.4.4.4",
  "title": "",
  "body": " Consider the vectors and the subspace of that they span.    Find an orthonormal basis for .    Find the matrix that projects vectors orthogonally onto .    Find , the orthogonal projection of onto .    Express as a linear combination of , , and .                                  Applying Gram-Schmidt gives us     Let be the matrix whose columns are the orthonormal basis that results from Gram-Schmidt. Then                "
},
{
  "id": "exercise-233",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-233",
  "type": "Exercise",
  "number": "6.4.4.5",
  "title": "",
  "body": " Consider the set of vectors    What happens when we apply the Gram-Schmit orthogonalization algorithm?    Why does the algorithm fail to produce an orthogonal basis for ?         We find .    This set of vectors is linearly dependent.         We find Since , this does not produce a basis for .    The vector so this set of vectors is linearly dependent. Since is in the span of and , projecting into that subspace gives so that .     "
},
{
  "id": "exercise-234",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-234",
  "type": "Exercise",
  "number": "6.4.4.6",
  "title": "",
  "body": " Suppose that is a matrix with linearly independent columns and having the factorization . Determine whether the following statements are true or false and explain your thinking.   It follows that .    The matrix is invertible.    The product projects vectors orthogonally onto .    The columns of are an orthogonal basis for .    The orthogonal complement .        True  True  False  True  True        True. Since , we have .    True. Since is upper triangular and the diagonal entries of are the lengths of the nonzero vectors , we have , which means that is invertible.    False, because .    True. In fact, they are an orthonormal basis for .    True. If , then for every vector in an orthonormal basis of . Therefore, is orthogonal to .     "
},
{
  "id": "exercise-235",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-235",
  "type": "Exercise",
  "number": "6.4.4.7",
  "title": "",
  "body": " Suppose we have the factorization , where is a matrix.   What is the shape of the product ? Explain the significance of this product.    What is the shape of the product ? Explain the significance of this product.    What is the shape of the matrix ?    If is a diagonal matrix, what can you say about the columns of ?                        They form an orthogonal set.          is and projects vectors orthogonally onto .     is the identity matrix because the product computes dot products between the columns of .     is a upper triangular matrix.    The columns of form an orthogonal set.     "
},
{
  "id": "exercise-236",
  "level": "2",
  "url": "sec-gram-schmidt.html#exercise-236",
  "type": "Exercise",
  "number": "6.4.4.8",
  "title": "",
  "body": " Suppose we have the factorization where the columns of are and the columns of are .   How can the matrix product be expressed in terms of dot products?    How can the matrix product be expressed in terms of dot products?    Explain why .    Explain why the dot products .                        This follows from the previous parts of this exercise.         The entries of are the dot products .    The entries of are the dot products .         This follows from the previous parts of this exercise.     "
},
{
  "id": "sec-least-squares",
  "level": "1",
  "url": "sec-least-squares.html",
  "type": "Section",
  "number": "6.5",
  "title": "Orthogonal least squares",
  "body": " Orthogonal least squares   Suppose we collect some data when performing an experiment and plot it as shown on the left of . Notice that there is no line on which all the points lie; in fact, it would be surprising if there were since we can expect some uncertainty in the measurements recorded. There does, however, appear to be a line, as shown on the right, on which the points almost lie.       A collection of points and a line approximating the linear relationship implied by them.   In this section, we'll explore how the techniques developed in this chapter enable us to find the line that best approximates the data. More specifically, we'll see how the search for a line passing through the data points leads to an inconsistent system . Since we are unable to find a solution, we instead seek the vector where is as close as possible to . Orthogonal projection gives us just the right tool for doing this.       Is there a solution to the equation where and are such that      We know that and form a basis for . Find an orthogonal basis for .    Find the orthogonal projection of onto .    Explain why the equation must be consistent and then find its solution.          The reduced row echelon form shows that there is no solution.    Applying Gram-Schmidt, we find an orthogonal basis consisting of and .    The projection formula gives .    The equation is consistent because is in . We find the solution .         A first example  When we've encountered inconsistent systems in the past, we've simply said there is no solution and moved on. The preview activity, however, shows how we can find approximate solutions to an inconsistent system: if there are no solutions to , we instead solve the consistent system , the orthogonal projection of onto . As we'll see, this solution is, in a specific sense, the best possible.    Suppose we have three data points , , and and that we would like to find a line passing through them.   Plot these three points in . Are you able to draw a line that passes through all three points?     Plot the three data points here.      Remember that the equation of a line can be written as where is the slope and is the -intercept. We will try to find and so that the three points lie on the line.  The first data point gives an equation for and . In particular, we know that when , then so we have or . Use the other two data points to create a linear system describing and .    We have obtained a linear system having three equations, one from each data point, for the two unknowns and . Identify a matrix and vector so that the system has the form , where .  Notice that the unknown vector describes the line that we seek.    Is there a solution to this linear system? How does this question relate to your attempt to draw a line through the three points above?     Since this system is inconsistent, we know that is not in the column space . Find an orthogonal basis for and use it to find the orthogonal projection of onto .    Since is in , the equation is consistent. Find its solution and sketch the line in . We say that this is the line of best fit.          After plotting the points, we see that it's not possible to draw a line through all three points.    We have the equations     We have and .    Finding the reduced row echelon form of the associated augmented matrix tells us this is an inconsistent system. Since a solution would describe a line passing through the three points, we should expect this.    Applying Gram-Schmidt gives us the orthogonal basis and . Projecting onto gives .    Solving the equation gives , which describes a line having vertical intercept and the slope . This line is shown in .      The line that best approximates the three data points.           It's not possible to draw a line through all three points.    We have the equations     We have and .    This linear system is inconsistent.     .     . This line is shown in .      The line that best approximates the three data points.        This activity illustrates the idea behind a technique known as orthogonal least squares , which we have been working toward throughout this chapter. If the data points are denoted as , we construct the matrix and vector as With the vector representing the line , we see that the equation describes a line passing through all the data points. In our activity, it is visually apparent that there is no such line, which agrees with the fact that the equation is inconsistent.  Remember that , the orthogonal projection of onto , is the closest vector in to . Therefore, when we solve the equation , we are finding the vector so that is as close to as possible. Let's think about what this means within the context of this problem.  The difference so that the square of the distance between and is Our approach finds the values for and that make this sum of squares as small as possible, which is why we call this a least-squares problem.  Drawing the line defined by the vector , the quantity reflects the vertical distance between the line and the data point , as shown in . Seen in this way, the square of the distance is a measure of how much the line defined by the vector misses the data points. The solution to the least-squares problem is the line that misses the data points by the smallest amount possible.      The solution of the least-squares problem and the vertical distances between the line and the data points.     Solving least-squares problems  Now that we've seen an example of what we're trying to accomplish, let's put this technique into a more general framework.  Given an inconsistent system , we seek the vector that minimizes the distance from to . In other words, satisfies , where is the orthogonal projection of onto the column space . We know the equation is consistent since is in , and we know there is only one solution if we assume that the columns of are linearly independent.  We will usually denote the solution of by and call this vector the least-squares approximate solution of to distinguish it from a (possibly non-existent) solution of .  There is an alternative method for finding that does not involve first finding the orthogonal projection . Remember that is defined by the fact that is orthogonal to . In other words, is in the orthogonal complement , which tells us is the same as . Since is in , it follows that Because the least-squares approximate solution is the vector such that , we can rearrange this equation to see that  normal equation This equation is called the normal equation , and we have the following proposition.    If the columns of are linearly independent, then there is a unique least-squares approximate solution to the equation given by the normal equation      Consider the equation with matrix and vector . Since this equation is inconsistent, we will find the least-squares approximate solution by solving the normal equation , which has the form and the solution .     The rate at which a cricket chirps is related to the outdoor temperature, as reflected in some experimental data that we'll study in this activity. The chirp rate is expressed in chirps per second while the temperature is in degrees Fahrenheit. Evaluate the following cell to load the data: Evaluating this cell also provides:   the vectors chirps and temps formed from the columns of the dataset.    the command onesvec(n) , which creates an -dimensional vector whose entries are all one.    Remember that you can form a matrix whose columns are the vectors v1 and v2 with matrix([v1, v2]).T .     We would like to represent this relationship by a linear function    Use the first data point to write an equation involving and .    Suppose that we represent the unknowns using a vector . Use the 15 data points to create the matrix and vector so that the linear system describes the unknown vector .     Write the normal equations ; that is, find the matrix and the vector .    Solve the normal equations to find , the least-squares approximate solution to the equation . Call your solution xhat since x has another meaning in Sage.   What are the values of and that you found?    If the chirp rate is 22 chirps per second, what is your prediction for the temperature?  You can plot the data and your line, assuming you called the solution xhat , using the cell below.           We have the equation .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .    The predicted temperature is degrees.           .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .     degrees.       This example demonstrates an approach, called linear regression , in which a collection of data is modeled using a linear function found by solving a least-squares problem. Once we have the linear function that best fits the data, we can make predictions about situations that we haven't encountered in the data.  If we're going to use our function to make predictions, it's natural to ask how much confidence we have in these predictions. This is a statistical question that leads to a rich and well-developed theory For example, see Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. An Introduction to Statistical Learning: with Applications in R. Springer, 2013. , which we won't explore in much detail here. However, there is one simple measure of how well our linear function fits the data that is known as the coefficient of determination and denoted by .  We have seen that the square of the distance measures the amount by which the line fails to pass through the data points. When the line is close to the data points, we expect this number to be small. However, the size of this measure depends on the scale of the data. For instance, the two lines shown in seem to fit the data equally well, but is 100 times larger on the right.       The lines appear to fit equally well in spite of the fact that differs by a factor of 100.   The coefficient of determination is defined by normalizing so that it is independent of the scale. Recall that we described how to demean a vector in : given a vector , we obtain by subtracting the average of the components from each component.   Coefficient of determination  coefficient of determination  R squared   The coefficient of determination is where is the vector obtained by demeaning .    A more complete explanation of this definition relies on the concept of variance, which we explore in and the next chapter. For the time being, it's enough to know that and that the closer is to 1, the better the line fits the data. In our original example, illustrated in , we find that , and in our study of cricket chirp rates, we have . However, assessing the confidence we have in predictions made by solving a least-squares problem can require considerable thought, and it would be naive to rely only on the value of .    Using factorizations  As we've seen, the least-squares approximate solution to may be found by solving the normal equation , and this can be a practical strategy for some problems. However, this approach can be problematic as small rounding errors can accumulate and lead to inaccurate final results.  As the next activity demonstrates, there is an alternate method for finding the least-squares approximate solution using a factorization of the matrix , and this method is preferable as it is numerically more reliable.       Suppose we are interested in finding the least-squares approximate solution to the equation and that we have the factorization . Explain why the least-squares approximate solution is given by solving     Multiply both sides of the second expression by and explain why   Since is upper triangular, this is a relatively simple equation to solve using back substitution, as we saw in . We will therefore write the least-squares approximate solution as and put this to use in the following context.    Brozak’s formula, which is used to calculate a person's body fat index , is where denotes a person's body density in grams per cubic centimeter. Obtaining an accurate measure of is difficult, however, because it requires submerging the person in water and measuring the volume of water displaced. Instead, we will gather several other body measurements, which are more easily obtained, and use it to predict .  For instance, suppose we take 10 patients and measure their weight in pounds, height in inches, abdomen in centimeters, wrist circumference in centimeters, neck circumference in centimeters, and . Evaluating the following cell loads and displays the data. In addition, that cell provides:   vectors weight , height , abdomen , wrist , neck , and BFI formed from the columns of the dataset.    the command onesvec(n) , which returns an -dimensional vector whose entries are all one.    the command QR(A) that returns the factorization of as Q, R = QR(A) .    the command demean(v) , which returns the demeaned vector .     We would like to find the linear function that best fits the data.  Use the first data point to write an equation for the parameters .    Describe the linear system for these parameters. More specifically, describe how the matrix and the vector are formed.    Construct the matrix and find its factorization in the cell below.     Find the least-squares approximate solution by solving the equation . You may want to use N(xhat) to display a decimal approximation of the vector. What are the parameters that best fit the data?    Find the coefficient of determination for your parameters. What does this imply about the quality of the fit?     Suppose a person's measurements are: weight 190, height 70, abdomen 90, wrist 18, and neck 35. Estimate this person's .          The columns of form an orthonormal basis for so that . The equation then becomes .    Since , we have , which gives .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating           Use the fact that .    Use the fact that .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating        To summarize, we have seen that    If the columns of are linearly independent and we have the factorization , then the least-squares approximate solution to the equation is given by       Polynomial Regression  In the examples we've seen so far, we have fit a linear function to a dataset. Sometimes, however, a polynomial, such as a quadratic function, may be more appropriate. It turns out that the techniques we've developed in this section are still useful as the next activity demonstrates.       Suppose that we have a small dataset containing the points , , , and , such as appear when the following cell is evaluated. In addition to loading and plotting the data, evaluating that cell provides the following commands:    Q, R = QR(A) returns the factorization of .     demean(v) returns the demeaned vector .     Let's fit a quadratic function of the form to this dataset.  Write four equations, one for each data point, that describe the coefficients , , and .    Express these four equations as a linear system where .  Find the factorization of and use it to find the least-squares approximate solution .     Use the parameters , , and that you found to write the quadratic function that fits the data. You can plot this function, along with the data, by entering your function in the place indicated below.     What is your predicted value when ?    Find the coefficient of determination for the quadratic function. What does this say about the quality of the fit?    Now fit a cubic polynomial of the form to this dataset.     Find the coefficient of determination for the cubic function. What does this say about the quality of the fit?    What do you notice when you plot the cubic function along with the data? How does this reflect the value of that you found?           We have the equations     With and , we find     The quadratic function is .    The predicted value is .         We find .     , which means that we have a perfect fit.    The graph of the cubic function passes through each data point.          We have the equations           .     .          .         The graph of the cubic function passes through each data point.       The matrices that you created in the last activity when fitting a quadratic and cubic function to a dataset have a special form. In particular, if the data points are labeled and we seek a degree polynomial, then This is called a Vandermonde matrix of degree .    This activity explores a dataset describing Arctic sea ice and that comes from Sustainability Math.   Evaluating the cell below will plot the extent of Arctic sea ice, in millions of square kilometers, during the twelve months of 2012. In addition, you have access to a few special variables and commands:    month is the vector of month values and ice is the vector of sea ice values from the table above.     vandermonde(x, k) constructs the Vandermonde matrix of degree using the points in the vector x .     Q, R = QR(A) provides the factorization of .     demean(v) returns the demeaned vector .        Find the vector , the least-squares approximate solution to the linear system that results from fitting a degree 5 polynomial to the data.     If your result is stored in the variable xhat , you may plot the polynomial and the data together using the following cell.     Find the coefficient of determination for this polynomial fit.    Repeat these steps to fit a degree 8 polynomial to the data, plot the polynomial with the data, and find .     Repeat one more time by fitting a degree 11 polynomial to the data, creating a plot, and finding .   It's certainly true that higher degree polynomials fit the data better, as seen by the increasing values of , but that's not always a good thing. For instance, when , you may notice that the graph of the polynomial wiggles a little more than we would expect. In this case, the polynomial is trying too hard to fit the data, which usually contains some uncertainty, especially if it's obtained from measurements. The error built in to the data is called noise, and its presence means that we shouldn't expect our polynomial to fit the data perfectly. When we choose a polynomial whose degree is too high, we give the noise too much weight in the model, which leads to some undesirable behavior, like the wiggles in the graph.  Fitting the data with a polynomial whose degree is too high is called overfitting , a phenomenon that can appear in many machine learning applications. Generally speaking, we would like to choose large enough to capture the essential features of the data but not so large that we overfit and build the noise into the model. There are ways to determine the optimal value of , but we won't pursue that here.    Choosing a reasonable value of , estimate the extent of Arctic sea ice at month 6.5, roughly at the Summer Solstice.               The fifth degree polynomial fits the data fairly well.          .          seems like a good choice, and this gives the prediction of million square kilometers of sea ice.               The fifth degree polynomial fits the data fairly well.          .          million square kilometers of sea ice.         Summary  This section introduced some types of least-squares problems and a framework for working with them.   Given an inconsistent system , we find , the least-squares approximate solution, by requiring that be as possible to as possible. In other words, where is the orthogonal projection of onto .    One way to find is by solving the normal equations This is not our preferred method since numerical problems can arise.    A second way to find uses a factorization of . If , then and finding is computationally feasible since is upper triangular.    This technique may be applied widely and is useful for modeling data. We saw examples in this section where linear functions of several input variables and polynomials provided effective models for different datasets.    A simple measure of the quality of the fit is the coefficient of determination though some additional thought should be given in real applications.       Evaluating the following cell loads in some commands that will be helpful in the following exercises. In particular, there are commands:    QR(A) that returns the factorization of A as Q, R = QR(A) ,     onesvec(n) that returns the -dimensional vector whose entries are all 1,     demean(v) that demeans the vector v ,     vandermonde(x, k) that returns the Vandermonde matrix of degree formed from the components of the vector x , and     plot_model(xhat, data) that plots the data and the model xhat .       Suppose we write the linear system as .   Find an orthogonal basis for .    Find , the orthogonal projection of onto .    Find a solution to the linear system .          and      .     .         Applying Gram-Schmidt gives the orthogonal basis     Applying the Projection Formula gives .    Solving the linear system gives .       Consider the data in .  A dataset with four points.            1  1    2  1    3  1    4  2        Set up the linear system that describes the line passing through these points.    Write the normal equations that describe the least-squares approximate solution to .    Find the least-squares approximate solution and plot the data and the resulting line.    What is your predicted -value when ?    Find the coefficient of determination .          , .               .              The matrix and the vector .              The predicted value is .            Consider the four points in .    Set up a linear system that describes a quadratic function passing through the points.    Use a factorization to find the least-squares approximate solution and plot the data and the graph of the resulting quadratic function.    What is your predicted -value when ?    Find the coefficient of determination .         We have the Vandermonde matrix and vector .    The quadratic function is .    This gives the predicted value .            Consider the data in .  A simple dataset               1  1  4.2    1  2  3.3    2  1  5.9    2  2  5.1    3  2  7.5    3  3  6.3        Set up a linear system that describes the relationship     Find the least-squares approximate solution .    What is your predicted -value when and ?    Find the coefficient of determination .                    .                         .            Determine whether the following statements are true or false and explain your thinking.   If is consistent, then is a solution to .    If , then the least-squares approximate solution is also a solution to the original equation .    Given the factorization , we have .    A factorization provides a method for finding the least-squares approximate solution to that is more reliable than solving the normal equations.    A solution to is the least-squares approximate solution to .        True  True  False  True  False        True. If , then is in so .    True. If , then . Therefore, .    False. The product rather than the matrix that projects vectors orthogonally onto .    True, numerical issues are more likely to arise when solving the normal equations.    False. The normal equations gives the least-squares approximate solution.       Explain your response to the following questions.   If , what does this say about the vector ?    If the columns of are orthonormal, how can you easily find the least-squares approximate solution to ?          is in .     .         Since and , we know that . This says that is in .    In this case, , which means we have . If we multiply both sides by , we have .       The following cell loads in some data showing the number of people in Bangladesh living without electricity over 27 years. It also defines vectors year , which records the years in the dataset, and people , which records the number of people.    Suppose we want to write where is the year and is the number of people. Construct the matrix and vector so that the linear system describes the vector .    Using a factorization of , find the values of and in the least-squares approximate solution .    What is the coefficient of determination and what does this tell us about the quality of the approximation?    What is your prediction for the number of people living without electricity in 1985?    Estimate the year in which there will be no people living without electricity.         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.     .          .    2045         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.    We obtain .          .    Solve to obtain 2045.       This problem concerns a dataset describing planets in our Solar system. For each planet, we have the length of the semi-major axis, essentially the distance from the planet to the Sun in AU (astronomical units), and the period , the length of time in years required to complete one orbit around the Sun.  We would like to model this data using the function where and are parameters we need to determine. Since this isn't a linear function, we will transform this relationship by taking the natural logarithm of both sides to obtain   Evaluating the following cell loads the dataset and defines two vectors logaxis , whose components are , and logperiod , whose components are .    Construct the matrix and vector so that the solution to is the vector .    Find the least-squares approximate solution . What does this give for the values of and ?    Find the coefficient of determination . What does this tell us about the quality of the approximation?   Suppose that the orbit of an asteroid has a semi-major axis whose length is AU. Estimate the period of the asteroid's orbit.   Halley's Comet has a period of years. Estimate the length of its semi-major axis.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.     and .          years.     AU.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.    We find so that and . This means that .     , which means that we have a very good fit because this data reflects a physical law.     years.    The expression means that . Solving for AU.       Evaluating the following cell loads a dataset describing the temperature in the Earth's atmosphere at various altitudes. There are also two vectors altitude , expressed in kilometers, and temperature , in degrees Celsius.    Describe how to form the matrix and vector so that the linear system describes a degree polynomial fitting the data.    After choosing a value of , construct the matrix and vector , and find the least-squares approximate solution .    Plot the polynomial and data using plot_model(xhat, data) .    Now examine what happens as you vary the degree of the polynomial . Choose an appropriate value of that seems to capture the most important features of the data while avoiding overfitting, and explain your choice.    Use your value of to estimate the temperature at an altitude of 55 kilometers.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.    We estimate the temperature to be degrees Celsius.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.     seems to work well. With this function, we estimate the temperature to be degrees Celsius.       The following cell loads some data describing 1057 houses in a particular real estate market. For each house, we record the living area in square feet, the lot size in acres, the age in years, and the price in dollars. The cell also defines variables area , size , age , and price . We will use linear regression to predict the price of a house given its living area, lot size, and age:    Use a factorization to find the least-squares approximate solution .    Discuss the significance of the signs of , , and .    If two houses are identical except for differing in age by one year, how would you predict that their prices compare to each another?    Find the coefficient of determination . What does this say about the quality of the fit?    Predict the price of a house whose living area is 2000 square feet, lot size is 1.5 acres, and age is 50 years.              Increasing the living area or the lot size will cause the house price to increase, but an older house will cost less.    A house that's one year older will cost about less.         We estimate .               and are positive, which means that increasing the living area or the lot size will cause the house price to increase. However, is negative, which means that an older house will cost less.    Because , we would expect a house that's one year older to cost less.     , which is far from perfect without being terrible. There are other factors that are not included in the dataset that can also influence the house price, such as number of bedrooms and the location.    We estimate .       We observed that if the columns of are linearly independent, then there is a unique least-squares approximate solution to the equation because the equation has a unique solution. We also said that is the unique solution to the normal equation without explaining why this equation has a unique solution. This exercise offers an explanation.  Assuming that the columns of are linearly independent, we would like to conclude that the equation has a unique solution.   Suppose that is a vector for which . Explain why the following argument is valid and allows us to conclude that . In other words, if , we know that .    If the columns of are linearly independent and , what do we know about the vector ?    Explain why can only happen when .    Assuming that the columns of are linearly independent, explain why has a unique solution.         Use the fact that .    It must be true that .    This follows from the previous two parts of this exercise.    The columns of must be linearly independent.         Starting with the assumption that , we dot both sides of the equation with to obtain . From here, we write This says that the length of must be zero, which tells us that .    If the columns of are linearly independent, we know that the only solution to the homogeneous equation is .    The previous two parts of this exercise tell us that implies that , which implies that .    Since the only solution to the homogeneous equation is , the columns of are linearly independent, which means that has only one solution.       This problem is about the meaning of the coefficient of determination and its connection to variance, a topic that appears in the next section. Throughout this problem, we consider the linear system and the approximate least-squares solution , where . We suppose that is an matrix, and we will denote the -dimensional vector .     Explain why , the mean of the components of , can be found as the dot product     In the examples we have seen in this section, explain why is in .    If we write , explain why and hence why the mean of the components of is zero.    The variance of an -dimensional vector is , where is the vector obtained by demeaning .  Explain why     Explain why and hence   These expressions indicate why it is sometimes said that measures the fraction of variance explained by the function we are using to fit the data. As seen in the previous exercise, there may be other features that are not recorded in the dataset that influence the quantity we wish to predict.    Explain why .          simply sums the components of .     is a column of .     is in .    Use the fact that and are orthogonal.    This follows from .    This follow from .          simply sums the components of .    The examples we've seen fit the data using functions that have a constant term such as . This means that will be a column of the matrix and hence in .    Since is in , any vector in , such as , will be orthogonal to .    Notice that which says that Since and are orthogonal, we have . This means that so that .     , which explains why Then     The variances are nonnegative so we have . Also,        "
},
{
  "id": "lst-squares-intro",
  "level": "2",
  "url": "sec-least-squares.html#lst-squares-intro",
  "type": "Figure",
  "number": "6.5.1",
  "title": "",
  "body": "     A collection of points and a line approximating the linear relationship implied by them.  "
},
{
  "id": "exploration-25",
  "level": "2",
  "url": "sec-least-squares.html#exploration-25",
  "type": "Preview Activity",
  "number": "6.5.1",
  "title": "",
  "body": "     Is there a solution to the equation where and are such that      We know that and form a basis for . Find an orthogonal basis for .    Find the orthogonal projection of onto .    Explain why the equation must be consistent and then find its solution.          The reduced row echelon form shows that there is no solution.    Applying Gram-Schmidt, we find an orthogonal basis consisting of and .    The projection formula gives .    The equation is consistent because is in . We find the solution .      "
},
{
  "id": "activity-83",
  "level": "2",
  "url": "sec-least-squares.html#activity-83",
  "type": "Activity",
  "number": "6.5.2",
  "title": "",
  "body": "  Suppose we have three data points , , and and that we would like to find a line passing through them.   Plot these three points in . Are you able to draw a line that passes through all three points?     Plot the three data points here.      Remember that the equation of a line can be written as where is the slope and is the -intercept. We will try to find and so that the three points lie on the line.  The first data point gives an equation for and . In particular, we know that when , then so we have or . Use the other two data points to create a linear system describing and .    We have obtained a linear system having three equations, one from each data point, for the two unknowns and . Identify a matrix and vector so that the system has the form , where .  Notice that the unknown vector describes the line that we seek.    Is there a solution to this linear system? How does this question relate to your attempt to draw a line through the three points above?     Since this system is inconsistent, we know that is not in the column space . Find an orthogonal basis for and use it to find the orthogonal projection of onto .    Since is in , the equation is consistent. Find its solution and sketch the line in . We say that this is the line of best fit.          After plotting the points, we see that it's not possible to draw a line through all three points.    We have the equations     We have and .    Finding the reduced row echelon form of the associated augmented matrix tells us this is an inconsistent system. Since a solution would describe a line passing through the three points, we should expect this.    Applying Gram-Schmidt gives us the orthogonal basis and . Projecting onto gives .    Solving the equation gives , which describes a line having vertical intercept and the slope . This line is shown in .      The line that best approximates the three data points.           It's not possible to draw a line through all three points.    We have the equations     We have and .    This linear system is inconsistent.     .     . This line is shown in .      The line that best approximates the three data points.       "
},
{
  "id": "fig-least-squares-def",
  "level": "2",
  "url": "sec-least-squares.html#fig-least-squares-def",
  "type": "Figure",
  "number": "6.5.5",
  "title": "",
  "body": "    The solution of the least-squares problem and the vertical distances between the line and the data points.  "
},
{
  "id": "proposition-47",
  "level": "2",
  "url": "sec-least-squares.html#proposition-47",
  "type": "Proposition",
  "number": "6.5.6",
  "title": "",
  "body": "  If the columns of are linearly independent, then there is a unique least-squares approximate solution to the equation given by the normal equation    "
},
{
  "id": "example-68",
  "level": "2",
  "url": "sec-least-squares.html#example-68",
  "type": "Example",
  "number": "6.5.7",
  "title": "",
  "body": " Consider the equation with matrix and vector . Since this equation is inconsistent, we will find the least-squares approximate solution by solving the normal equation , which has the form and the solution .  "
},
{
  "id": "activity-84",
  "level": "2",
  "url": "sec-least-squares.html#activity-84",
  "type": "Activity",
  "number": "6.5.3",
  "title": "",
  "body": "  The rate at which a cricket chirps is related to the outdoor temperature, as reflected in some experimental data that we'll study in this activity. The chirp rate is expressed in chirps per second while the temperature is in degrees Fahrenheit. Evaluate the following cell to load the data: Evaluating this cell also provides:   the vectors chirps and temps formed from the columns of the dataset.    the command onesvec(n) , which creates an -dimensional vector whose entries are all one.    Remember that you can form a matrix whose columns are the vectors v1 and v2 with matrix([v1, v2]).T .     We would like to represent this relationship by a linear function    Use the first data point to write an equation involving and .    Suppose that we represent the unknowns using a vector . Use the 15 data points to create the matrix and vector so that the linear system describes the unknown vector .     Write the normal equations ; that is, find the matrix and the vector .    Solve the normal equations to find , the least-squares approximate solution to the equation . Call your solution xhat since x has another meaning in Sage.   What are the values of and that you found?    If the chirp rate is 22 chirps per second, what is your prediction for the temperature?  You can plot the data and your line, assuming you called the solution xhat , using the cell below.           We have the equation .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .    The predicted temperature is degrees.           .     is the matrix whose first column consists only of 1's and whose second column is the vector of chirp rates. The vector is the vector of temperatures.     and      .     degrees.      "
},
{
  "id": "fig-regression-scale",
  "level": "2",
  "url": "sec-least-squares.html#fig-regression-scale",
  "type": "Figure",
  "number": "6.5.8",
  "title": "",
  "body": "     The lines appear to fit equally well in spite of the fact that differs by a factor of 100.  "
},
{
  "id": "definition-34",
  "level": "2",
  "url": "sec-least-squares.html#definition-34",
  "type": "Definition",
  "number": "6.5.9",
  "title": "Coefficient of determination.",
  "body": " Coefficient of determination  coefficient of determination  R squared   The coefficient of determination is where is the vector obtained by demeaning .   "
},
{
  "id": "activity-BFI",
  "level": "2",
  "url": "sec-least-squares.html#activity-BFI",
  "type": "Activity",
  "number": "6.5.4",
  "title": "",
  "body": "     Suppose we are interested in finding the least-squares approximate solution to the equation and that we have the factorization . Explain why the least-squares approximate solution is given by solving     Multiply both sides of the second expression by and explain why   Since is upper triangular, this is a relatively simple equation to solve using back substitution, as we saw in . We will therefore write the least-squares approximate solution as and put this to use in the following context.    Brozak’s formula, which is used to calculate a person's body fat index , is where denotes a person's body density in grams per cubic centimeter. Obtaining an accurate measure of is difficult, however, because it requires submerging the person in water and measuring the volume of water displaced. Instead, we will gather several other body measurements, which are more easily obtained, and use it to predict .  For instance, suppose we take 10 patients and measure their weight in pounds, height in inches, abdomen in centimeters, wrist circumference in centimeters, neck circumference in centimeters, and . Evaluating the following cell loads and displays the data. In addition, that cell provides:   vectors weight , height , abdomen , wrist , neck , and BFI formed from the columns of the dataset.    the command onesvec(n) , which returns an -dimensional vector whose entries are all one.    the command QR(A) that returns the factorization of as Q, R = QR(A) .    the command demean(v) , which returns the demeaned vector .     We would like to find the linear function that best fits the data.  Use the first data point to write an equation for the parameters .    Describe the linear system for these parameters. More specifically, describe how the matrix and the vector are formed.    Construct the matrix and find its factorization in the cell below.     Find the least-squares approximate solution by solving the equation . You may want to use N(xhat) to display a decimal approximation of the vector. What are the parameters that best fit the data?    Find the coefficient of determination for your parameters. What does this imply about the quality of the fit?     Suppose a person's measurements are: weight 190, height 70, abdomen 90, wrist 18, and neck 35. Estimate this person's .          The columns of form an orthonormal basis for so that . The equation then becomes .    Since , we have , which gives .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating           Use the fact that .    Use the fact that .          is the matrix whose columns are a vector of all 1's followed by the vectors of weights, heights, abdominal, wrist, and neck measurements. The vector is the vector of BFI readings.     is a matrix and is a upper triangular matrix.    We find that          Evaluating       "
},
{
  "id": "proposition-48",
  "level": "2",
  "url": "sec-least-squares.html#proposition-48",
  "type": "Proposition",
  "number": "6.5.10",
  "title": "",
  "body": "  If the columns of are linearly independent and we have the factorization , then the least-squares approximate solution to the equation is given by    "
},
{
  "id": "activity-86",
  "level": "2",
  "url": "sec-least-squares.html#activity-86",
  "type": "Activity",
  "number": "6.5.5",
  "title": "",
  "body": "     Suppose that we have a small dataset containing the points , , , and , such as appear when the following cell is evaluated. In addition to loading and plotting the data, evaluating that cell provides the following commands:    Q, R = QR(A) returns the factorization of .     demean(v) returns the demeaned vector .     Let's fit a quadratic function of the form to this dataset.  Write four equations, one for each data point, that describe the coefficients , , and .    Express these four equations as a linear system where .  Find the factorization of and use it to find the least-squares approximate solution .     Use the parameters , , and that you found to write the quadratic function that fits the data. You can plot this function, along with the data, by entering your function in the place indicated below.     What is your predicted value when ?    Find the coefficient of determination for the quadratic function. What does this say about the quality of the fit?    Now fit a cubic polynomial of the form to this dataset.     Find the coefficient of determination for the cubic function. What does this say about the quality of the fit?    What do you notice when you plot the cubic function along with the data? How does this reflect the value of that you found?           We have the equations     With and , we find     The quadratic function is .    The predicted value is .         We find .     , which means that we have a perfect fit.    The graph of the cubic function passes through each data point.          We have the equations           .     .          .         The graph of the cubic function passes through each data point.      "
},
{
  "id": "activity-87",
  "level": "2",
  "url": "sec-least-squares.html#activity-87",
  "type": "Activity",
  "number": "6.5.6",
  "title": "",
  "body": "  This activity explores a dataset describing Arctic sea ice and that comes from Sustainability Math.   Evaluating the cell below will plot the extent of Arctic sea ice, in millions of square kilometers, during the twelve months of 2012. In addition, you have access to a few special variables and commands:    month is the vector of month values and ice is the vector of sea ice values from the table above.     vandermonde(x, k) constructs the Vandermonde matrix of degree using the points in the vector x .     Q, R = QR(A) provides the factorization of .     demean(v) returns the demeaned vector .        Find the vector , the least-squares approximate solution to the linear system that results from fitting a degree 5 polynomial to the data.     If your result is stored in the variable xhat , you may plot the polynomial and the data together using the following cell.     Find the coefficient of determination for this polynomial fit.    Repeat these steps to fit a degree 8 polynomial to the data, plot the polynomial with the data, and find .     Repeat one more time by fitting a degree 11 polynomial to the data, creating a plot, and finding .   It's certainly true that higher degree polynomials fit the data better, as seen by the increasing values of , but that's not always a good thing. For instance, when , you may notice that the graph of the polynomial wiggles a little more than we would expect. In this case, the polynomial is trying too hard to fit the data, which usually contains some uncertainty, especially if it's obtained from measurements. The error built in to the data is called noise, and its presence means that we shouldn't expect our polynomial to fit the data perfectly. When we choose a polynomial whose degree is too high, we give the noise too much weight in the model, which leads to some undesirable behavior, like the wiggles in the graph.  Fitting the data with a polynomial whose degree is too high is called overfitting , a phenomenon that can appear in many machine learning applications. Generally speaking, we would like to choose large enough to capture the essential features of the data but not so large that we overfit and build the noise into the model. There are ways to determine the optimal value of , but we won't pursue that here.    Choosing a reasonable value of , estimate the extent of Arctic sea ice at month 6.5, roughly at the Summer Solstice.               The fifth degree polynomial fits the data fairly well.          .          seems like a good choice, and this gives the prediction of million square kilometers of sea ice.               The fifth degree polynomial fits the data fairly well.          .          million square kilometers of sea ice.      "
},
{
  "id": "exercise-237",
  "level": "2",
  "url": "sec-least-squares.html#exercise-237",
  "type": "Exercise",
  "number": "6.5.6.1",
  "title": "",
  "body": " Suppose we write the linear system as .   Find an orthogonal basis for .    Find , the orthogonal projection of onto .    Find a solution to the linear system .          and      .     .         Applying Gram-Schmidt gives the orthogonal basis     Applying the Projection Formula gives .    Solving the linear system gives .     "
},
{
  "id": "ex-lst-squares-line",
  "level": "2",
  "url": "sec-least-squares.html#ex-lst-squares-line",
  "type": "Exercise",
  "number": "6.5.6.2",
  "title": "",
  "body": " Consider the data in .  A dataset with four points.            1  1    2  1    3  1    4  2        Set up the linear system that describes the line passing through these points.    Write the normal equations that describe the least-squares approximate solution to .    Find the least-squares approximate solution and plot the data and the resulting line.    What is your predicted -value when ?    Find the coefficient of determination .          , .               .              The matrix and the vector .              The predicted value is .          "
},
{
  "id": "exercise-239",
  "level": "2",
  "url": "sec-least-squares.html#exercise-239",
  "type": "Exercise",
  "number": "6.5.6.3",
  "title": "",
  "body": " Consider the four points in .    Set up a linear system that describes a quadratic function passing through the points.    Use a factorization to find the least-squares approximate solution and plot the data and the graph of the resulting quadratic function.    What is your predicted -value when ?    Find the coefficient of determination .         We have the Vandermonde matrix and vector .    The quadratic function is .    This gives the predicted value .          "
},
{
  "id": "exercise-240",
  "level": "2",
  "url": "sec-least-squares.html#exercise-240",
  "type": "Exercise",
  "number": "6.5.6.4",
  "title": "",
  "body": " Consider the data in .  A simple dataset               1  1  4.2    1  2  3.3    2  1  5.9    2  2  5.1    3  2  7.5    3  3  6.3        Set up a linear system that describes the relationship     Find the least-squares approximate solution .    What is your predicted -value when and ?    Find the coefficient of determination .                    .                         .          "
},
{
  "id": "exercise-241",
  "level": "2",
  "url": "sec-least-squares.html#exercise-241",
  "type": "Exercise",
  "number": "6.5.6.5",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If is consistent, then is a solution to .    If , then the least-squares approximate solution is also a solution to the original equation .    Given the factorization , we have .    A factorization provides a method for finding the least-squares approximate solution to that is more reliable than solving the normal equations.    A solution to is the least-squares approximate solution to .        True  True  False  True  False        True. If , then is in so .    True. If , then . Therefore, .    False. The product rather than the matrix that projects vectors orthogonally onto .    True, numerical issues are more likely to arise when solving the normal equations.    False. The normal equations gives the least-squares approximate solution.     "
},
{
  "id": "exercise-242",
  "level": "2",
  "url": "sec-least-squares.html#exercise-242",
  "type": "Exercise",
  "number": "6.5.6.6",
  "title": "",
  "body": " Explain your response to the following questions.   If , what does this say about the vector ?    If the columns of are orthonormal, how can you easily find the least-squares approximate solution to ?          is in .     .         Since and , we know that . This says that is in .    In this case, , which means we have . If we multiply both sides by , we have .     "
},
{
  "id": "exercise-243",
  "level": "2",
  "url": "sec-least-squares.html#exercise-243",
  "type": "Exercise",
  "number": "6.5.6.7",
  "title": "",
  "body": " The following cell loads in some data showing the number of people in Bangladesh living without electricity over 27 years. It also defines vectors year , which records the years in the dataset, and people , which records the number of people.    Suppose we want to write where is the year and is the number of people. Construct the matrix and vector so that the linear system describes the vector .    Using a factorization of , find the values of and in the least-squares approximate solution .    What is the coefficient of determination and what does this tell us about the quality of the approximation?    What is your prediction for the number of people living without electricity in 1985?    Estimate the year in which there will be no people living without electricity.         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.     .          .    2045         The matrix will be the matrix whose first column is all 1's and whose second column is the vector of years. The vector is the vector that records the number of people.    We obtain .          .    Solve to obtain 2045.     "
},
{
  "id": "exercise-244",
  "level": "2",
  "url": "sec-least-squares.html#exercise-244",
  "type": "Exercise",
  "number": "6.5.6.8",
  "title": "",
  "body": " This problem concerns a dataset describing planets in our Solar system. For each planet, we have the length of the semi-major axis, essentially the distance from the planet to the Sun in AU (astronomical units), and the period , the length of time in years required to complete one orbit around the Sun.  We would like to model this data using the function where and are parameters we need to determine. Since this isn't a linear function, we will transform this relationship by taking the natural logarithm of both sides to obtain   Evaluating the following cell loads the dataset and defines two vectors logaxis , whose components are , and logperiod , whose components are .    Construct the matrix and vector so that the solution to is the vector .    Find the least-squares approximate solution . What does this give for the values of and ?    Find the coefficient of determination . What does this tell us about the quality of the approximation?   Suppose that the orbit of an asteroid has a semi-major axis whose length is AU. Estimate the period of the asteroid's orbit.   Halley's Comet has a period of years. Estimate the length of its semi-major axis.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.     and .          years.     AU.         The matrix is the matrix whose first column is all 1's and whose second column is the vector of the logarithms of the semi-major axes. The vector is the vector that records the logarithms of the periods.    We find so that and . This means that .     , which means that we have a very good fit because this data reflects a physical law.     years.    The expression means that . Solving for AU.     "
},
{
  "id": "exercise-245",
  "level": "2",
  "url": "sec-least-squares.html#exercise-245",
  "type": "Exercise",
  "number": "6.5.6.9",
  "title": "",
  "body": " Evaluating the following cell loads a dataset describing the temperature in the Earth's atmosphere at various altitudes. There are also two vectors altitude , expressed in kilometers, and temperature , in degrees Celsius.    Describe how to form the matrix and vector so that the linear system describes a degree polynomial fitting the data.    After choosing a value of , construct the matrix and vector , and find the least-squares approximate solution .    Plot the polynomial and data using plot_model(xhat, data) .    Now examine what happens as you vary the degree of the polynomial . Choose an appropriate value of that seems to capture the most important features of the data while avoiding overfitting, and explain your choice.    Use your value of to estimate the temperature at an altitude of 55 kilometers.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.    We estimate the temperature to be degrees Celsius.         The matrix will be the Vandermonde matrix of degree using the altitude readings while is the vector of temperatures.     appears to give a good fit without overfitting.    We have     A small value of does not adequately track important features of the data. If we increase too far, the graph tries too hard to match the data, and we see spurious features in the graph.     seems to work well. With this function, we estimate the temperature to be degrees Celsius.     "
},
{
  "id": "exercise-246",
  "level": "2",
  "url": "sec-least-squares.html#exercise-246",
  "type": "Exercise",
  "number": "6.5.6.10",
  "title": "",
  "body": " The following cell loads some data describing 1057 houses in a particular real estate market. For each house, we record the living area in square feet, the lot size in acres, the age in years, and the price in dollars. The cell also defines variables area , size , age , and price . We will use linear regression to predict the price of a house given its living area, lot size, and age:    Use a factorization to find the least-squares approximate solution .    Discuss the significance of the signs of , , and .    If two houses are identical except for differing in age by one year, how would you predict that their prices compare to each another?    Find the coefficient of determination . What does this say about the quality of the fit?    Predict the price of a house whose living area is 2000 square feet, lot size is 1.5 acres, and age is 50 years.              Increasing the living area or the lot size will cause the house price to increase, but an older house will cost less.    A house that's one year older will cost about less.         We estimate .               and are positive, which means that increasing the living area or the lot size will cause the house price to increase. However, is negative, which means that an older house will cost less.    Because , we would expect a house that's one year older to cost less.     , which is far from perfect without being terrible. There are other factors that are not included in the dataset that can also influence the house price, such as number of bedrooms and the location.    We estimate .     "
},
{
  "id": "exercise-247",
  "level": "2",
  "url": "sec-least-squares.html#exercise-247",
  "type": "Exercise",
  "number": "6.5.6.11",
  "title": "",
  "body": " We observed that if the columns of are linearly independent, then there is a unique least-squares approximate solution to the equation because the equation has a unique solution. We also said that is the unique solution to the normal equation without explaining why this equation has a unique solution. This exercise offers an explanation.  Assuming that the columns of are linearly independent, we would like to conclude that the equation has a unique solution.   Suppose that is a vector for which . Explain why the following argument is valid and allows us to conclude that . In other words, if , we know that .    If the columns of are linearly independent and , what do we know about the vector ?    Explain why can only happen when .    Assuming that the columns of are linearly independent, explain why has a unique solution.         Use the fact that .    It must be true that .    This follows from the previous two parts of this exercise.    The columns of must be linearly independent.         Starting with the assumption that , we dot both sides of the equation with to obtain . From here, we write This says that the length of must be zero, which tells us that .    If the columns of are linearly independent, we know that the only solution to the homogeneous equation is .    The previous two parts of this exercise tell us that implies that , which implies that .    Since the only solution to the homogeneous equation is , the columns of are linearly independent, which means that has only one solution.     "
},
{
  "id": "ex-r2-meaning",
  "level": "2",
  "url": "sec-least-squares.html#ex-r2-meaning",
  "type": "Exercise",
  "number": "6.5.6.12",
  "title": "",
  "body": " This problem is about the meaning of the coefficient of determination and its connection to variance, a topic that appears in the next section. Throughout this problem, we consider the linear system and the approximate least-squares solution , where . We suppose that is an matrix, and we will denote the -dimensional vector .     Explain why , the mean of the components of , can be found as the dot product     In the examples we have seen in this section, explain why is in .    If we write , explain why and hence why the mean of the components of is zero.    The variance of an -dimensional vector is , where is the vector obtained by demeaning .  Explain why     Explain why and hence   These expressions indicate why it is sometimes said that measures the fraction of variance explained by the function we are using to fit the data. As seen in the previous exercise, there may be other features that are not recorded in the dataset that influence the quantity we wish to predict.    Explain why .          simply sums the components of .     is a column of .     is in .    Use the fact that and are orthogonal.    This follows from .    This follow from .          simply sums the components of .    The examples we've seen fit the data using functions that have a constant term such as . This means that will be a column of the matrix and hence in .    Since is in , any vector in , such as , will be orthogonal to .    Notice that which says that Since and are orthogonal, we have . This means that so that .     , which explains why Then     The variances are nonnegative so we have . Also,      "
},
{
  "id": "sec-symmetric-matrices",
  "level": "1",
  "url": "sec-symmetric-matrices.html",
  "type": "Section",
  "number": "7.1",
  "title": "Symmetric matrices and variance",
  "body": " Symmetric matrices and variance   In this section, we will revisit the theory of eigenvalues and eigenvectors for the special class of matrices that are symmetric , meaning that the matrix equals its transpose. This understanding of symmetric matrices will enable us to form singular value decompositions later in the chapter. We'll also begin studying variance in this section as it provides an important context that motivates some of our later work.  To begin, remember that if is a square matrix, we say that is an eigenvector of with associated eigenvalue if . In other words, for these special vectors, the operation of matrix multiplication simplifies to scalar multiplication.    This preview activity reminds us how a basis of eigenvectors can be used to relate a square matrix to a diagonal one.       Use these plots to sketch the vectors requested in the preview activity.      Suppose that and that and .   Sketch the vectors and on the left side of .    Sketch the vectors and on the left side of .    Sketch the vectors and on the left side.    Give a geometric description of the matrix transformation defined by .       Now suppose we have vectors and and that is a matrix such that That is, and are eigenvectors of with associated eigenvalues and .   Sketch the vectors and on the right side of .    Sketch the vectors and on the right side of .    Sketch the vectors and on the right side.    Give a geometric description of the matrix transformation defined by .       In what ways are the matrix transformations defined by and related to one another?                             stretches vectors horizontally by a factor of 3 and reflects them in the horizontal axis.                          stretches vectors in the direction of by a factor of 3 and reflects them in the line defined by .       The effect of the two transformations are the same when viewed in the coordinate systems given by the appropriate set of vectors.       The preview activity asks us to compare the matrix transformations defined by two matrices, a diagonal matrix and a matrix whose eigenvectors are given to us. The transformation defined by stretches horizontally by a factor of 3 and reflects in the horizontal axis, as shown in       The matrix transformation defined by .   By contrast, the transformation defined by stretches the plane by a factor of 3 in the direction of and reflects in the line defined by , as seen in .      The matrix transformation defined by .   In this way, we see that the matrix transformations defined by these two matrices are equivalent after a rotation. This notion of equivalence is what we called similarity in . There we considered a square matrix that provided enough eigenvectors to form a basis of . For example, suppose we can construct a basis for using eigenvectors having associated eigenvalues . Forming the matrices, enables us to write . This is what it means for to be diagonalizable.  For the example in the preview activity, we are led to form which tells us that .  Notice that the matrix has eigenvectors and that not only form a basis for but, in fact, form an orthogonal basis for . Given the prominent role played by orthogonal bases in the last chapter, we would like to understand what conditions on a matrix enable us to form an orthogonal basis of eigenvectors.    Symmetric matrices and orthogonal diagonalization  Let's begin by looking at some examples in the next activity.    Remember that the Sage command A.right_eigenmatrix() attempts to find a basis for consisting of eigenvectors of . In particular, the assignment D, P = A.right_eigenmatrix() provides a diagonal matrix constructed from the eigenvalues of with the columns of containing the associated eigenvectors.    For each of the following matrices, determine whether there is a basis for consisting of eigenvectors of that matrix. When there is such a basis, form the matrices and and verify that the matrix equals .    .     .     .     .       For which of these examples is it possible to form an orthogonal basis for consisting of eigenvectors?    For any such matrix , find an orthonormal basis of eigenvectors and explain why where is an orthogonal matrix.    Finally, explain why in this case.    When , what is the relationship between and ?             The eigenvalues of this matrix are complex so there is no such basis.    There is one eigenvalue with multiplicity two. The associated eigenspace is one-dimensional so there is not a basis of consisting of eigenvectors.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix .    We form an orthonormal basis by scaling the eigenvectors to have length 1. This gives , which is orthogonal since the columns form an orthonormal basis of .    Orthogonal matrices are invertible and have     If , we have . This means that the matrix is symmetric.             There is no such basis.    There is no such basis.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix.                      The examples in this activity illustrate a range of possibilities. First, a matrix may have complex eigenvalues, in which case it will not be diagonalizable. Second, even if all the eigenvalues are real, there may not be a basis of eigenvalues if the dimension of one of the eigenspaces is less than the algebraic multiplicity of the associated eigenvalue.  We are interested in matrices for which there is an orthogonal basis of eigenvectors. When this happens, we can create an orthonormal basis of eigenvectors by scaling each eigenvector in the basis so that its length is 1. Putting these orthonormal vectors into a matrix produces an orthogonal matrix, which means that . We then have In this case, we say that is orthogonally diagonalizable .   orthogonal diagonalization   If there is an orthonormal basis of consisting of eigenvectors of the matrix , we say that is orthogonally diagonalizable . In particular, we can write where is an orthogonal matrix.    When is orthogonally diagonalizable, notice that That is, when is orthogonally diagonalizable, and we say that is symmetric .   symmetric matrix   A symmetric matrix is one for which .      Consider the matrix , which has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that and are orthogonal so we can form an orthonormal basis of eigenvectors:   In this way, we construct the matrices and note that .  Notice also that, as expected, is symmetric; that is, .      If , then there is an orthogonal basis of eigenvectors and with eigenvalues and . Using these eigenvectors, we form the orthogonal matrix consisting of eigenvectors and the diagonal matrix , where Then we have .  Notice that the matrix transformation represented by is a rotation while that represented by is a rotation. Therefore, if we multiply a vector by , we can decompose the multiplication as That is, we first rotate by , then apply the diagonal matrix , which stretches and reflects, and finally rotate by . We may visualize this factorization as in .      The transformation defined by can be interpreted as a sequence of geometric transformations: rotates by , stretches and reflects, and rotates by .   In fact, a similar picture holds any time the matrix is orthogonally diagonalizable.    We have seen that a matrix that is orthogonally diagonalizable must be symmetric. In fact, it turns out that any symmetric matrix is orthogonally diagonalizable. We record this fact in the next theorem.   The Spectral Theorem   The matrix is orthogonally diagonalizable if and only if is symmetric.      Each of the following matrices is symmetric so the Spectral Theorem tells us that each is orthogonally diagonalizable. The point of this activity is to find an orthogonal diagonalization for each matrix.  To begin, find a basis for each eigenspace. Use this basis to find an orthogonal basis for each eigenspace and put these bases together to find an orthogonal basis for consisting of eigenvectors. Use this basis to write an orthogonal diagonalization of the matrix.     .     .     .    Consider the matrix where . Explain how we know that is symmetric and then find an orthogonal diagonalization of .          We have eigenvectors and with associated eigenvalues and . We form an orthonormal basis of eigenvectors, and . This gives     We find     We have eigenvalues with associated eigenvector and with associated eigenvectors and . Notice that is orthogonal to both and , but and are not orthogonal to one another. We can, however, apply Gram-Schmidt to create an orthogonal basis of the eigenspace . We can then form an orthonormal basis so that     We have so must be symmetric. Then we find the orthogonal diagonalization                                  As the examples in illustrate, the Spectral Theorem implies a number of things. Namely, if is a symmetric matrix, then   the eigenvalues of are real.    there is a basis of consisting of eigenvectors.    two eigenvectors that are associated to different eigenvalues are orthogonal.     We won't justify the first two facts here since that would take us rather far afield. However, it will be helpful to explain the third fact. To begin, notice the following: This is a useful fact that we'll employ quite a bit in the future so let's summarize it in the following proposition.    For any matrix , we have In particular, if is symmetric, then       Suppose a symmetric matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that Since by , we have which can only happen if . Therefore, and are orthogonal.  More generally, the same argument shows that two eigenvectors of a symmetric matrix associated to distinct eigenvalues are orthogonal.      Variance  Many of the ideas we'll encounter in this chapter, such as orthogonal diagonalizations, can be applied to the study of data. In fact, it can be useful to understand these applications because they provide an important context in which mathematical ideas have a more concrete meaning and their motivation appears more clearly. For that reason, we will now introduce the statistical concept of variance as a way to gain insight into the significance of orthogonal diagonalizations.  Given a set of data points, their variance measures how spread out the points are. The next activity looks at some examples.    We'll begin with a set of three data points    Find the centroid, or mean, . Then plot the data points and their centroid in .      Plot the data points and their centroid here.     Notice that the centroid lies in the center of the data so the spread of the data will be measured by how far away the points are from the centroid. To simplify our calculations, find the demeaned data points and plot them in .      Plot the demeaned data points here.     Now that the data has been demeaned, we will define the total variance as the average of the squares of the distances from the origin; that is, the total variance is Find the total variance for our set of three points.    Now plot the projections of the demeaned data onto the and axes using and find the variances and of the projected points.       Plot the projections of the demeaned data onto the and axes.     Which of the variances, and , is larger and how does the plot of the projected points explain your response?    What do you notice about the relationship between , , and ? How does the Pythagorean theorem explain this relationship?    Plot the projections of the demeaned data points onto the lines defined by vectors and using and find the variances and of these projected points.      Plot the projections of the deameaned data onto the lines defined by and .     What is the relationship between the total variance and and ? How does the Pythagorean theorem explain your response?          The centroid is .    The demeaned data points are     The total variance is .    We find and . Notice that is larger because the points are more spread out in the vertical direction.    We have due to the Pythagorean theorem.    The points projected onto the line defined by are , , and . This gives the variance .  The points projected onto the line defined by are , , and . This gives the variance .    Once again, because of the Pythagorean theorem.           .          .     and                          Notice that variance enjoys an additivity property. Consider, for instance, the situation where our data points are two-dimensional and suppose that the demeaned points are . We have If we take the average over all data points, we find that the total variance is the sum of the variances in the and directions:   More generally, suppose that we have an orthonormal basis and . If we project the demeaned points onto the line defined by , we obtain the points so that   For each of our demeaned data points, the Projection Formula tells us that We then have since . When we average over all the data points, we find that the total variance is the sum of the variances in the and directions. This leads to the following proposition, in which this observation is expressed more generally.   Additivity of Variance   If is a subspace with orthonormal basis , , , , then the variance of the points projected onto is the sum of the variances in the directions:     The next activity demonstrates a more efficient way to find the variance in a particular direction and connects our discussion of variance with symmetric matrices.    Let's return to the dataset from the previous activity in which we have demeaned data points: Our goal is to compute the variance in the direction defined by a unit vector .  To begin, form the demeaned data matrix and suppose that is a unit vector.   Write the vector in terms of the dot products .    Explain why .    Apply to explain why     In general, the matrix is called the covariance matrix of the dataset, and it is useful because the variance , as we have just seen. Find the matrix for our dataset with three points.     Use the covariance matrix to find the variance when .    Use the covariance matrix to find the variance when . Since and are orthogonal, verify that the sum of and gives the total variance.    Explain why the covariance matrix is a symmetric matrix.               Projecting onto gives , whose length squared is . Then                   . Then , which is the total variance.                                       .            This activity introduced the covariance matrix of a dataset, which is defined to be where is the matrix of demeaned data points. Notice that which tells us that is symmetric. In particular, we know that it is orthogonally diagonalizable, an observation that will play an important role in the future.  This activity also demonstrates the significance of the covariance matrix, which is recorded in the following proposition.    If is the covariance matrix associated to a demeaned dataset and is a unit vector, then the variance of the demeaned points projected onto the line defined by is     Our goal in the future will be to find directions where the variance is as large as possible and directions where it is as small as possible. The next activity demonstrates why this is useful.       Evaluating the following Sage cell loads a dataset consisting of 100 demeaned data points and provides a plot of them. It also provides the demeaned data matrix .   What is the shape of the covariance matrix ? Find and verify your response.     By visually inspecting the data, determine which is larger, or . Then compute both of these quantities to verify your response.    What is the total variance ?    In approximately what direction is the variance greatest? Choose a reasonable vector that points in approximately that direction and find .    In approximately what direction is the variance smallest? Choose a reasonable vector that points in approximately that direction and find .    How are the directions and in the last two parts of this problem related to one another? Why does this relationship hold?           will be the matrix      and , which agrees with the fact that the data is more spread out in the horizontal than vertical direction.         It looks like the direction defined by the unit vector . We find that , which is almost all of the total variance.    It looks like the direction defined by the unit vector . We find that .    They are orthogonal to one another. Since the total variance when and are orthogonal, will be as large as possible when is as small as possible.                and          If , then .    If , then .    They are orthogonal to one another.       This activity illustrates how variance can identify a line along which the data are concentrated. When the data primarily lie along a line defined by a vector , then the variance in that direction will be large while the variance in an orthogonal direction will be small.  Remember that variance is additive, according to , so that if and are orthogonal unit vectors, then the total variance is Therefore, if we choose to be the direction where is a maximum, then will be a minimum.  In the next section, we will use an orthogonal diagonalization of the covariance matrix to find the directions having the greatest and smallest variances. In this way, we will be able to determine when data are concentrated along a line or subspace.    Summary  This section explored both symmetric matrices and variance. In particular, we saw that   A matrix is orthogonally diagonalizable if there is an orthonormal basis of eigenvectors. In particular, we can write , where is a diagonal matrix of eigenvalues and is an orthogonal matrix of eigenvectors.    The Spectral Theorem tells us that a matrix is orthogonally diagonalizable if and only if it is symmetric; that is, .    The variance of a dataset can be computed using the covariance matrix , where is the matrix of demeaned data points. In particular, the variance of the demeaned data points projected onto the line defined by the unit vector is .    Variance is additive so that if is a subspace with orthonormal basis , then         For each of the following matrices, find the eigenvalues and a basis for each eigenspace. Determine whether the matrix is diagonalizable and, if so, find a diagonalization. Determine whether the matrix is orthogonally diagonalizable and, if so, find an orthogonal diagonalization.                             Not diagonalizable         Diagonalizable, but not orthogonally diagonalizable.               This matrix is not diagonalizable because there is not a basis of consisting of eigenvectors.    This matrix is symmetric so it is orthogonally diagonalizable:     This matrix is diagonalizable but not orthogonally diagonalizable.     This matrix is symmetric so it's orthogonally diagonalizable.        Consider the matrix whose eigenvalues are , , and .    Explain why is orthogonally diagonalizable.    Find an orthonormal basis for the eigenspace .    Find a basis for the eigenspace .    Now find an orthonormal basis for .    Find matrices and such that .         Because of the Spectral Theorem          and      and               Since the matrix is symmetric, the Spectral Theorem says it is orthogonally diagonalizable.          and      and             Find an orthogonal diagonalization, if one exists, for the following matrices.     .     .     .              Not orthogonally diagonalizable                   This matrix is not symmetric so it is not orthogonally diagonalizable.            Suppose that is an matrix and that .   Explain why is orthogonally diagonalizable.    Explain why .    Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Explain why the eigenvalues of are nonnegative.    If is the covariance matrix associated to a demeaned dataset, explain why the eigenvalues of are nonnegative.          is symmetric               .     .         Because , the matrix is symmetric and hence orthogonally diagonalizable.     .         Because .    In the same way, so that . Therefore, .       Suppose that you have the data points    Find the demeaned data points.    Find the total variance of the dataset.    Find the variance in the direction and the variance in the direction .    Project the demeaned data points onto the line defined by and find the variance of these projected points.    Project the demeaned data points onto the line defined by and find the variance of these projected points.    How and why are the results of from the last two parts related to the total variance?                   and               The variances add to the toal variance.                  and     Let be the unit vector parallel to so that .    Let be the unit vector parallel to so that .    The vectors are parallel so the variances add to the toal variance.      Suppose you have six 2-dimensional data points arranged in the matrix    Find the matrix of demeaned data points and plot the points in .     A plot for the demeaned data points.      Construct the covariance matrix and explain why you know that it is orthogonally diagonalizable.    Find an orthogonal diagonalization of .    Sketch the lines corresponding to the two eigenvectors on the plot above.    Find the variances in the directions of the eigenvectors.                         and .     and                is orthonally diagonalizable because it is symmetric.         Sketch the lines defined by and .    The variances are the eigenvalues and        Suppose that is the covariance matrix of a demeaned dataset.   Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Suppose that the covariance matrix of a demeaned dataset can be written as where What is ? What does this tell you about the demeaned data?    Explain why the total variance of a dataset equals the sum of the eigenvalues of the covariance matrix.          .                   The variance is .                 Determine whether the following statements are true or false and explain your thinking.   If is an invertible, orthogonally diagonalizable matrix, then so is .    If is an eigenvalue of , then cannot be orthogonally diagonalizable.    If there is a basis for consisting of eigenvectors of , then is orthogonally diagonalizable.    If and are eigenvectors of a symmetric matrix associated to eigenvalues -2 and 3, then .    If is a square matrix, then .         True.    True.    False.    True.    False.         True. If is invertible, then the eigenvalues are nonzero, which means that is invertible. Therefore, , which says that is orthogonally diagonalizable.    True. In this case, there cannot be a basis for consisting of eigenvalues of so is not diagonalizable.    False. This condition implies that is diagonalizable, but it may not be orthogonally diagonalizable.    True. The eigenvectors of a symmetric matrix associated to different eigenvalues are orthogonal.    False. This is only true if is symmetric.       Suppose that is a noninvertible, symmetric matrix having eigenvectors and associated eigenvalues and . Find matrices and such that .       Since is not invertible, the third eigenvalue must be zero: . Also, an eigenvector associated to must be orthogonal to both and . We can find such a vector by finding where . This leads to     Suppose that is a plane in and that is the matrix that projects vectors orthogonally onto .   Explain why is orthogonally diagonalizable.    What are the eigenvalues of ?    Explain the relationship between the eigenvectors of and the plane .          is symmetric    0 or 1    The eigenspaces and          If is a matrix whose columns are an orthonormal basis for , then . This means that so is symmetric and hence orthogonally diagonalizable.    If is in , then and if is in , then . This means that the eigenvalues of are either 0 or 1.    The eigenspaces and        "
},
{
  "id": "exploration-26",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exploration-26",
  "type": "Preview Activity",
  "number": "7.1.1",
  "title": "",
  "body": "  This preview activity reminds us how a basis of eigenvectors can be used to relate a square matrix to a diagonal one.       Use these plots to sketch the vectors requested in the preview activity.      Suppose that and that and .   Sketch the vectors and on the left side of .    Sketch the vectors and on the left side of .    Sketch the vectors and on the left side.    Give a geometric description of the matrix transformation defined by .       Now suppose we have vectors and and that is a matrix such that That is, and are eigenvectors of with associated eigenvalues and .   Sketch the vectors and on the right side of .    Sketch the vectors and on the right side of .    Sketch the vectors and on the right side.    Give a geometric description of the matrix transformation defined by .       In what ways are the matrix transformations defined by and related to one another?                             stretches vectors horizontally by a factor of 3 and reflects them in the horizontal axis.                          stretches vectors in the direction of by a factor of 3 and reflects them in the line defined by .       The effect of the two transformations are the same when viewed in the coordinate systems given by the appropriate set of vectors.      "
},
{
  "id": "fig-eigen-diag-D",
  "level": "2",
  "url": "sec-symmetric-matrices.html#fig-eigen-diag-D",
  "type": "Figure",
  "number": "7.1.2",
  "title": "",
  "body": "    The matrix transformation defined by .  "
},
{
  "id": "fig-eigen-diag-general",
  "level": "2",
  "url": "sec-symmetric-matrices.html#fig-eigen-diag-general",
  "type": "Figure",
  "number": "7.1.3",
  "title": "",
  "body": "    The matrix transformation defined by .  "
},
{
  "id": "activity-88",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-88",
  "type": "Activity",
  "number": "7.1.2",
  "title": "",
  "body": "  Remember that the Sage command A.right_eigenmatrix() attempts to find a basis for consisting of eigenvectors of . In particular, the assignment D, P = A.right_eigenmatrix() provides a diagonal matrix constructed from the eigenvalues of with the columns of containing the associated eigenvectors.    For each of the following matrices, determine whether there is a basis for consisting of eigenvectors of that matrix. When there is such a basis, form the matrices and and verify that the matrix equals .    .     .     .     .       For which of these examples is it possible to form an orthogonal basis for consisting of eigenvectors?    For any such matrix , find an orthonormal basis of eigenvectors and explain why where is an orthogonal matrix.    Finally, explain why in this case.    When , what is the relationship between and ?             The eigenvalues of this matrix are complex so there is no such basis.    There is one eigenvalue with multiplicity two. The associated eigenspace is one-dimensional so there is not a basis of consisting of eigenvectors.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix .    We form an orthonormal basis by scaling the eigenvectors to have length 1. This gives , which is orthogonal since the columns form an orthonormal basis of .    Orthogonal matrices are invertible and have     If , we have . This means that the matrix is symmetric.             There is no such basis.    There is no such basis.    This matrix is diagonalizable with     This matrix is also diagonalizable with        Only the last matrix.                     "
},
{
  "id": "def-orthog-diag",
  "level": "2",
  "url": "sec-symmetric-matrices.html#def-orthog-diag",
  "type": "Definition",
  "number": "7.1.4",
  "title": "",
  "body": " orthogonal diagonalization   If there is an orthonormal basis of consisting of eigenvectors of the matrix , we say that is orthogonally diagonalizable . In particular, we can write where is an orthogonal matrix.   "
},
{
  "id": "definition-36",
  "level": "2",
  "url": "sec-symmetric-matrices.html#definition-36",
  "type": "Definition",
  "number": "7.1.5",
  "title": "",
  "body": " symmetric matrix   A symmetric matrix is one for which .   "
},
{
  "id": "example-69",
  "level": "2",
  "url": "sec-symmetric-matrices.html#example-69",
  "type": "Example",
  "number": "7.1.6",
  "title": "",
  "body": "  Consider the matrix , which has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that and are orthogonal so we can form an orthonormal basis of eigenvectors:   In this way, we construct the matrices and note that .  Notice also that, as expected, is symmetric; that is, .   "
},
{
  "id": "example-70",
  "level": "2",
  "url": "sec-symmetric-matrices.html#example-70",
  "type": "Example",
  "number": "7.1.7",
  "title": "",
  "body": "  If , then there is an orthogonal basis of eigenvectors and with eigenvalues and . Using these eigenvectors, we form the orthogonal matrix consisting of eigenvectors and the diagonal matrix , where Then we have .  Notice that the matrix transformation represented by is a rotation while that represented by is a rotation. Therefore, if we multiply a vector by , we can decompose the multiplication as That is, we first rotate by , then apply the diagonal matrix , which stretches and reflects, and finally rotate by . We may visualize this factorization as in .      The transformation defined by can be interpreted as a sequence of geometric transformations: rotates by , stretches and reflects, and rotates by .   In fact, a similar picture holds any time the matrix is orthogonally diagonalizable.   "
},
{
  "id": "theorem-3",
  "level": "2",
  "url": "sec-symmetric-matrices.html#theorem-3",
  "type": "Theorem",
  "number": "7.1.9",
  "title": "The Spectral Theorem.",
  "body": " The Spectral Theorem   The matrix is orthogonally diagonalizable if and only if is symmetric.   "
},
{
  "id": "activity-orthog-diag",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-orthog-diag",
  "type": "Activity",
  "number": "7.1.3",
  "title": "",
  "body": "  Each of the following matrices is symmetric so the Spectral Theorem tells us that each is orthogonally diagonalizable. The point of this activity is to find an orthogonal diagonalization for each matrix.  To begin, find a basis for each eigenspace. Use this basis to find an orthogonal basis for each eigenspace and put these bases together to find an orthogonal basis for consisting of eigenvectors. Use this basis to write an orthogonal diagonalization of the matrix.     .     .     .    Consider the matrix where . Explain how we know that is symmetric and then find an orthogonal diagonalization of .          We have eigenvectors and with associated eigenvalues and . We form an orthonormal basis of eigenvectors, and . This gives     We find     We have eigenvalues with associated eigenvector and with associated eigenvectors and . Notice that is orthogonal to both and , but and are not orthogonal to one another. We can, however, apply Gram-Schmidt to create an orthogonal basis of the eigenspace . We can then form an orthonormal basis so that     We have so must be symmetric. Then we find the orthogonal diagonalization                                 "
},
{
  "id": "prop-symmetric-dot",
  "level": "2",
  "url": "sec-symmetric-matrices.html#prop-symmetric-dot",
  "type": "Proposition",
  "number": "7.1.10",
  "title": "",
  "body": "  For any matrix , we have In particular, if is symmetric, then    "
},
{
  "id": "example-71",
  "level": "2",
  "url": "sec-symmetric-matrices.html#example-71",
  "type": "Example",
  "number": "7.1.11",
  "title": "",
  "body": "  Suppose a symmetric matrix has eigenvectors , with associated eigenvalue , and , with associated eigenvalue . Notice that Since by , we have which can only happen if . Therefore, and are orthogonal.  More generally, the same argument shows that two eigenvectors of a symmetric matrix associated to distinct eigenvalues are orthogonal.   "
},
{
  "id": "activity-90",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-90",
  "type": "Activity",
  "number": "7.1.4",
  "title": "",
  "body": "  We'll begin with a set of three data points    Find the centroid, or mean, . Then plot the data points and their centroid in .      Plot the data points and their centroid here.     Notice that the centroid lies in the center of the data so the spread of the data will be measured by how far away the points are from the centroid. To simplify our calculations, find the demeaned data points and plot them in .      Plot the demeaned data points here.     Now that the data has been demeaned, we will define the total variance as the average of the squares of the distances from the origin; that is, the total variance is Find the total variance for our set of three points.    Now plot the projections of the demeaned data onto the and axes using and find the variances and of the projected points.       Plot the projections of the demeaned data onto the and axes.     Which of the variances, and , is larger and how does the plot of the projected points explain your response?    What do you notice about the relationship between , , and ? How does the Pythagorean theorem explain this relationship?    Plot the projections of the demeaned data points onto the lines defined by vectors and using and find the variances and of these projected points.      Plot the projections of the deameaned data onto the lines defined by and .     What is the relationship between the total variance and and ? How does the Pythagorean theorem explain your response?          The centroid is .    The demeaned data points are     The total variance is .    We find and . Notice that is larger because the points are more spread out in the vertical direction.    We have due to the Pythagorean theorem.    The points projected onto the line defined by are , , and . This gives the variance .  The points projected onto the line defined by are , , and . This gives the variance .    Once again, because of the Pythagorean theorem.           .          .     and                         "
},
{
  "id": "prop-variance-additivity",
  "level": "2",
  "url": "sec-symmetric-matrices.html#prop-variance-additivity",
  "type": "Proposition",
  "number": "7.1.16",
  "title": "Additivity of Variance.",
  "body": " Additivity of Variance   If is a subspace with orthonormal basis , , , , then the variance of the points projected onto is the sum of the variances in the directions:    "
},
{
  "id": "activity-91",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-91",
  "type": "Activity",
  "number": "7.1.5",
  "title": "",
  "body": "  Let's return to the dataset from the previous activity in which we have demeaned data points: Our goal is to compute the variance in the direction defined by a unit vector .  To begin, form the demeaned data matrix and suppose that is a unit vector.   Write the vector in terms of the dot products .    Explain why .    Apply to explain why     In general, the matrix is called the covariance matrix of the dataset, and it is useful because the variance , as we have just seen. Find the matrix for our dataset with three points.     Use the covariance matrix to find the variance when .    Use the covariance matrix to find the variance when . Since and are orthogonal, verify that the sum of and gives the total variance.    Explain why the covariance matrix is a symmetric matrix.               Projecting onto gives , whose length squared is . Then                   . Then , which is the total variance.                                       .           "
},
{
  "id": "prop-covariance",
  "level": "2",
  "url": "sec-symmetric-matrices.html#prop-covariance",
  "type": "Proposition",
  "number": "7.1.17",
  "title": "",
  "body": "  If is the covariance matrix associated to a demeaned dataset and is a unit vector, then the variance of the demeaned points projected onto the line defined by is    "
},
{
  "id": "activity-92",
  "level": "2",
  "url": "sec-symmetric-matrices.html#activity-92",
  "type": "Activity",
  "number": "7.1.6",
  "title": "",
  "body": "     Evaluating the following Sage cell loads a dataset consisting of 100 demeaned data points and provides a plot of them. It also provides the demeaned data matrix .   What is the shape of the covariance matrix ? Find and verify your response.     By visually inspecting the data, determine which is larger, or . Then compute both of these quantities to verify your response.    What is the total variance ?    In approximately what direction is the variance greatest? Choose a reasonable vector that points in approximately that direction and find .    In approximately what direction is the variance smallest? Choose a reasonable vector that points in approximately that direction and find .    How are the directions and in the last two parts of this problem related to one another? Why does this relationship hold?           will be the matrix      and , which agrees with the fact that the data is more spread out in the horizontal than vertical direction.         It looks like the direction defined by the unit vector . We find that , which is almost all of the total variance.    It looks like the direction defined by the unit vector . We find that .    They are orthogonal to one another. Since the total variance when and are orthogonal, will be as large as possible when is as small as possible.                and          If , then .    If , then .    They are orthogonal to one another.      "
},
{
  "id": "exercise-249",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-249",
  "type": "Exercise",
  "number": "7.1.4.1",
  "title": "",
  "body": " For each of the following matrices, find the eigenvalues and a basis for each eigenspace. Determine whether the matrix is diagonalizable and, if so, find a diagonalization. Determine whether the matrix is orthogonally diagonalizable and, if so, find an orthogonal diagonalization.                             Not diagonalizable         Diagonalizable, but not orthogonally diagonalizable.               This matrix is not diagonalizable because there is not a basis of consisting of eigenvectors.    This matrix is symmetric so it is orthogonally diagonalizable:     This matrix is diagonalizable but not orthogonally diagonalizable.     This matrix is symmetric so it's orthogonally diagonalizable.      "
},
{
  "id": "exercise-250",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-250",
  "type": "Exercise",
  "number": "7.1.4.2",
  "title": "",
  "body": " Consider the matrix whose eigenvalues are , , and .    Explain why is orthogonally diagonalizable.    Find an orthonormal basis for the eigenspace .    Find a basis for the eigenspace .    Now find an orthonormal basis for .    Find matrices and such that .         Because of the Spectral Theorem          and      and               Since the matrix is symmetric, the Spectral Theorem says it is orthogonally diagonalizable.          and      and           "
},
{
  "id": "exercise-251",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-251",
  "type": "Exercise",
  "number": "7.1.4.3",
  "title": "",
  "body": " Find an orthogonal diagonalization, if one exists, for the following matrices.     .     .     .              Not orthogonally diagonalizable                   This matrix is not symmetric so it is not orthogonally diagonalizable.          "
},
{
  "id": "exercise-252",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-252",
  "type": "Exercise",
  "number": "7.1.4.4",
  "title": "",
  "body": " Suppose that is an matrix and that .   Explain why is orthogonally diagonalizable.    Explain why .    Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Explain why the eigenvalues of are nonnegative.    If is the covariance matrix associated to a demeaned dataset, explain why the eigenvalues of are nonnegative.          is symmetric               .     .         Because , the matrix is symmetric and hence orthogonally diagonalizable.     .         Because .    In the same way, so that . Therefore, .     "
},
{
  "id": "exercise-253",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-253",
  "type": "Exercise",
  "number": "7.1.4.5",
  "title": "",
  "body": " Suppose that you have the data points    Find the demeaned data points.    Find the total variance of the dataset.    Find the variance in the direction and the variance in the direction .    Project the demeaned data points onto the line defined by and find the variance of these projected points.    Project the demeaned data points onto the line defined by and find the variance of these projected points.    How and why are the results of from the last two parts related to the total variance?                   and               The variances add to the toal variance.                  and     Let be the unit vector parallel to so that .    Let be the unit vector parallel to so that .    The vectors are parallel so the variances add to the toal variance.    "
},
{
  "id": "exercise-254",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-254",
  "type": "Exercise",
  "number": "7.1.4.6",
  "title": "",
  "body": " Suppose you have six 2-dimensional data points arranged in the matrix    Find the matrix of demeaned data points and plot the points in .     A plot for the demeaned data points.      Construct the covariance matrix and explain why you know that it is orthogonally diagonalizable.    Find an orthogonal diagonalization of .    Sketch the lines corresponding to the two eigenvectors on the plot above.    Find the variances in the directions of the eigenvectors.                         and .     and                is orthonally diagonalizable because it is symmetric.         Sketch the lines defined by and .    The variances are the eigenvalues and      "
},
{
  "id": "exercise-255",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-255",
  "type": "Exercise",
  "number": "7.1.4.7",
  "title": "",
  "body": " Suppose that is the covariance matrix of a demeaned dataset.   Suppose that is an eigenvector of with associated eigenvalue and that has unit length. Explain why .    Suppose that the covariance matrix of a demeaned dataset can be written as where What is ? What does this tell you about the demeaned data?    Explain why the total variance of a dataset equals the sum of the eigenvalues of the covariance matrix.          .                   The variance is .               "
},
{
  "id": "exercise-256",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-256",
  "type": "Exercise",
  "number": "7.1.4.8",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If is an invertible, orthogonally diagonalizable matrix, then so is .    If is an eigenvalue of , then cannot be orthogonally diagonalizable.    If there is a basis for consisting of eigenvectors of , then is orthogonally diagonalizable.    If and are eigenvectors of a symmetric matrix associated to eigenvalues -2 and 3, then .    If is a square matrix, then .         True.    True.    False.    True.    False.         True. If is invertible, then the eigenvalues are nonzero, which means that is invertible. Therefore, , which says that is orthogonally diagonalizable.    True. In this case, there cannot be a basis for consisting of eigenvalues of so is not diagonalizable.    False. This condition implies that is diagonalizable, but it may not be orthogonally diagonalizable.    True. The eigenvectors of a symmetric matrix associated to different eigenvalues are orthogonal.    False. This is only true if is symmetric.     "
},
{
  "id": "exercise-257",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-257",
  "type": "Exercise",
  "number": "7.1.4.9",
  "title": "",
  "body": " Suppose that is a noninvertible, symmetric matrix having eigenvectors and associated eigenvalues and . Find matrices and such that .       Since is not invertible, the third eigenvalue must be zero: . Also, an eigenvector associated to must be orthogonal to both and . We can find such a vector by finding where . This leads to   "
},
{
  "id": "exercise-258",
  "level": "2",
  "url": "sec-symmetric-matrices.html#exercise-258",
  "type": "Exercise",
  "number": "7.1.4.10",
  "title": "",
  "body": " Suppose that is a plane in and that is the matrix that projects vectors orthogonally onto .   Explain why is orthogonally diagonalizable.    What are the eigenvalues of ?    Explain the relationship between the eigenvectors of and the plane .          is symmetric    0 or 1    The eigenspaces and          If is a matrix whose columns are an orthonormal basis for , then . This means that so is symmetric and hence orthogonally diagonalizable.    If is in , then and if is in , then . This means that the eigenvalues of are either 0 or 1.    The eigenspaces and      "
},
{
  "id": "sec-quadratic-forms",
  "level": "1",
  "url": "sec-quadratic-forms.html",
  "type": "Section",
  "number": "7.2",
  "title": "Quadratic forms",
  "body": " Quadratic forms   With our understanding of symmetric matrices and variance in hand, we'll now explore how to determine the directions in which the variance of a dataset is as large as possible and where it is as small as possible. This is part of a much larger story involving a type of function, called a quadratic form , that we'll introduce here.    Let's begin by looking at an example. Suppose we have three data points that form the demeaned data matrix    Plot the demeaned data points in . In which direction does the variance appear to be largest and in which does it appear to be smallest?      Use this coordinate grid to plot the demeaned data points.     Construct the covariance matrix and determine the variance in the direction of and the variance in the direction of .     What is the total variance of this dataset?    Generally speaking, if is the covariance matrix of a dataset and is an eigenvector of having unit length and with associated eigenvalue , what is ?          The variance appears to be greatest in the direction of and smallest in the direction of .    In the direction of , the variance is , while in the direction of , the variance is .    The total variance is .              Quadratic forms  Given a matrix of demeaned data points, the symmetric covariance matrix determines the variance in a particular direction where is a unit vector defining the direction.  More generally, a symmetric matrix defines a function by Notice that this expression is similar to the one we use to find the variance in terms of the covariance matrix . The only difference is that we allow to be any vector rather than requiring it to be a unit vector.    Suppose that . If we write , then we have   We may evaluate the quadratic form using some input vectors: Notice that the value of the quadratic form is a scalar.     quadratic form   If is a symmetric matrix, the quadratic form defined by is the function .      Let's look at some more examples of quadratic forms.   Consider the symmetric matrix . Write the quadratic form defined by in terms of the components of . What is the value of ?    Given the symmetric matrix , write the quadratic form defined by and evaluate .    Suppose that . Find a symmetric matrix such that is the quadratic form defined by .    Suppose that is a quadratic form and that . What is ? ? ?    Suppose that is a symmetric matrix and is the quadratic form defined by . Suppose that is an eigenvector of with associated eigenvalue -4 and with length 7. What is ?           and      and          Notice that . In the same way, we have and .                and      and           , and             Linear algebra is principally about things that are linear. However, quadratic forms, as the name implies, have a distinctly non-linear character. First, if , is a symmetric matrix, then the associated quadratic form is Notice how the variables and are multiplied together, which tells us this isn't a linear function.  This expression assumes an especially simple form when is a diagonal matrix. In particular, if , then . This is special because there is no cross-term involving .  Remember that matrix transformations have the property that . Quadratic forms behave differently: For instance, when we multiply by the scalar 2, then . Also, notice that since the scalar is squared.  Finally, evaluating a quadratic form on an eigenvector has a particularly simple form. Suppose that is an eigenvector of with associated eigenvalue . We then have   Let's now return to our motivating question: in which direction is the variance of a dataset as large as possible and in which is it as small as possible. Remembering that the vector is a unit vector, we can now state a more general form of this question: If is a quadratic form, for which unit vectors is as large as possible and for which is it as small as possible? Since a unit vector specifies a direction, we will often ask for the directions in which the quadratic form is at its maximum or minimum value.    We can gain some intuition about this problem by graphing the quadratic form and paying particular attention to the unit vectors.   Evaluating the following cell defines the matrix and displays the graph of the associated quadratic form . In addition, the points corresponding to vectors with unit length are displayed as a curve. Notice that the matrix is diagonal. In which directions does the quadratic form have its maximum and minimum values?    Write the quadratic form associated to . What is the value of ? What is the value of ?    Consider a unit vector so that , an expression we can rewrite as . Write the quadratic form and replace by . Now explain why the maximum of is 3. In which direction does the maximum occur? Does this agree with what you observed from the graph above?    Write the quadratic form and replace by . What is the minimum value of and in which direction does the minimum occur?    Use the previous Sage cell to change the matrix to and display the graph of the quadratic form . Determine the directions in which the maximum and minimum occur?    Remember that is symmetric so that where is the diagonal matrix above and is the orthogonal matrix that rotates vectors by . Notice that where . That is, we have .  Explain why is also a unit vector; that is, explain why     Using the fact that , explain how we now know the maximum value of is 3 and determine the direction in which it occurs. Also, determine the minimum value of and determine the direction in which it occurs.          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    We have so that the quadratic form only depends on . The graph of this function of is a parabola that has a maximum of when . Since , this means that the maximum occurs when . We therefore see that the maximum value of is in the direction as we saw from the graph.    Now , which has a minimum value of when . Therefore, the minimum value of is in the direction .    The graph of appears to be similar to the graph of only rotated by . This means the maximum appears to occur in the direction and the minimum in the direction .    Since is orthogonal, we have so that     Since , the maximum of is , which occurs when . This means that , the eigenvector of associated to .  In the same way, the minimum value of is , which occurs when , the eigenvector of associated to .          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    The maximum value of is in the direction .    The minimum value of is in the direction .    The maximum appears to occur in the direction and the minimum in the direction .    Use the fact that     The maximum of is , which occurs when .  The maximum of is , which occurs when .       This activity demonstrates how the eigenvalues of determine the maximum and minimum values of the quadratic form when evaluated on unit vectors and how the associated eigenvectors determine the directions in which the maximum and minimum values occur. Let's look at another example so that this connection is clear.    Consider the symmetric matrix . Because is symmetric, we know that it can be orthogonally diagonalized. In fact, we have where From this diagonalization, we know that is the largest eigenvalue of with associated eigenvector and that is the smallest eigenvalue with associated eigenvector .  Let's first study the quadratic form because the absence of the cross-term makes it comparatively simple. Remembering that is a unit vector, we have , which means that . Therefore, This tells us that has a maximum value of , which occurs when or in the direction .  In the same way, rewriting allows us to conclude that the minimum value of is , which occurs in the direction .  Let's now return to the matrix whose quadratic form is related to because . In particular, we have In other words, we have where . This is quite useful because it allows us to relate the values of to those of , which we already understand quite well.  Now it turns out that is also a unit vector because Therefore, the maximum value of is the same as , which we know to be and which occurs in the direction . This means that the maximum value of is also and that this occurs in the direction . We now know that the maximum value of is the largest eigenvalue and that this maximum value occurs in the direction of an associated eigenvector.  In the same way, we see that the minimum value of is the smallest eigenvalue and that this minimum occurs in the direction of , an associated eigenvector.    More generally, we have    Suppose that is a symmetric matrix, that we list its eigenvalues in decreasing order , and that is a basis of associated eigenvectors. The maximum value of among all unit vectors is , which occurs in the direction . Similarly, the minimum value of is , which occurs in the direction .      Suppose that is the symmetric matrix , which may be orthogonally diagonalized as where We see that the maximum value of is 12, which occurs in the direction , and the minimum value is -6, which occurs in the direction .      Suppose we have the matrix of demeaned data points that we considered in . The data points are shown in .      The set of demeaned data points from .   Constructing the covariance matrix gives , which has eigenvalues , with associated eigenvector , and , with associated eigenvector .  Remember that the variance in a direction is . Therefore, the variance attains a maximum value of 9 in the direction and a minimum value of 1\/3 in the direction . shows the data projected onto the lines defined by these vectors.       The demeaned data from is shown projected onto the lines of maximal and minimal variance.   Remember that variance is additive, as stated in , which tells us that the total variance is .    We've been focused on finding the directions in which a quadratic form attains its maximum and minimum values, but there's another important observation to make after this activity. Recall how we used the fact that a symmetric matrix is orthogonally diagonalizable: if , then where .  More generally, if we define , we have Remembering that the quadratic form associated to a diagonal form has no cross terms, we obtain In other words, after a change of coordinates, the quadratic form can be written without cross terms. This is known as the Principle Axes Theorem.   Principle Axes Theorem   If is a symmetric matrix with eigenvalues , then the quadratic form can be written, after an orthogonal change of coordinates , as     We will put this to use in the next section.    Definite symmetric matrices  While our questions about variance provide some motivation for exploring quadratic forms, these functions appear in a variety of other contexts so it's worth spending some more time with them. For example, quadratic forms appear in multivariable calculus when describing the behavior of a function of several variables near a critical point and in physics when describing the kinetic energy of a rigid body.  The following definition will be important in this section.    A symmetric matrix is called positive definite if its associated quadratic form satisfies for any nonzero vector . If for all nonzero vectors , we say that is positive semidefinite .  Likewise, we say that is negative definite if for all nonzero vectors .  Finally, is called indefinite if for some and for others.      This activity explores the relationship between the eigenvalues of a symmetric matrix and its definiteness.   Consider the diagonal matrix and write its quadratic form in terms of the components of . How does this help you decide whether is positive definite or not?    Now consider and write its quadratic form in terms of and . What can you say about the definiteness of ?    If is a diagonal matrix, what condition on the diagonal entries guarantee that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?      Suppose that is a symmetric matrix with eigenvalues 4 and 2 so that where . If , then we have . Explain why this tells us that is positive definite.    Suppose that is a symmetric matrix with eigenvalues 4 and 0. What can you say about the definiteness of in this case?    What condition on the eigenvalues of a symmetric matrix guarantees that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?             . Both addends are nonnegative, and one of them is positive if is nonzero. This means that when is nonzero and so is positive definite.     , which is always nonnegative. However, so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .       Since we know that when is nonzero, we know that when is nonzero. Therefore, is positive definite.    It will be positive semidefinite.    They will be the same as before.           so is positive definite.     so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .        is positive definite.     is positive semidefinite.    They will be the same as before.       As seen in this activity, it is straightforward to determine the definiteness of a diagonal matrix. For instance, if , then This shows that when either or is not zero so we conclude that is positive definite. In the same way, we see that is positive semidefinite if all the diagonal entries are nonnegative.  Understanding this behavior for diagonal matrices enables us to understand more general symmetric matrices. As we saw previously, the quadratic form for a symmetric matrix agrees with the quadratic form for the diagonal matrix after a change of coordinates. In particular, where . Now the diagonal entries of are the eigenvalues of from which we conclude that if all the eigenvalues of are positive. Likewise, if all the eigenvalues are nonnegative.    A symmetric matrix is positive definite if all its eigenvalues are positive. It is positive semidefinite if all its eigenvalues are nonnegative.  Likewise, a symmetric matrix is indefinite if some eigenvalues are positive and some are negative.    We will now apply what we've learned about quadratic forms to study the nature of critical points in multivariable calculus. The rest of this section assumes that the reader is familiar with ideas from multivariable calculus and can be skipped by others.  First, suppose that is a differentiable function. We will use and to denote the partial derivatives of with respect to and . Similarly, , , and denote the second partial derivatives. You may recall that the mixed partials, and are equal under a mild assumption on the function . A typical question in calculus is to determine where this function has its maximum and minimum values.  Any local maximum or minimum of appears at a critical point where Near a critical point, the quadratic approximation of tells us that     Let's explore how our understanding of quadratic forms helps us determine the behavior of a function near a critical point.   Consider the function . Find the partial derivatives and and use these expressions to determine the critical points of .    Evaluate the second partial derivatives , , and .    Let's first consider the critical point . Use the quadratic approximation as written above to find an expression approximating near the critical point.    Using the vector , rewrite your approximation as for some matrix . What is the matrix in this case?    Find the eigenvalues of . What can you conclude about the definiteness of ?    Recall that is a local minimum for if for nearby points . Explain why our understanding of the eigenvalues of shows that is a local minimum for .           We have     We have     This gives     The matrix is .    The eigenvalues are and , both of which are positive, which means that is positive definite.    We have if is nonzero so for points near to .          We have     We have     This gives      .     is positive definite.     for points near to .       Near a critical point of a function , we can write where and . If is positive definite, then , which tells us that and that the critical point is therefore a local minimum.  The matrix is called the Hessian of , and we see now that the eigenvalues of this symmetric matrix determine the nature of the critical point . In particular, if the eigenvalues are both positive, then is positive definite, and the critical point is a local minimum.  This observation leads to the Second Derivative Test for multivariable functions.   Second Derivative Test   The nature of a critical point of a multivariable function is determined by the Hessian of the function at the critical point. If    has all positive eigenvalues, the critical point is a local minimum.     has all negative eigenvalues, the critical point is a local maximum.     has both positive and negative eigenvalues, the critical point is neither a local maximum nor minimum.       Most multivariable calculus texts assume that the reader is not familiar with linear algebra and so write the second derivative test for functions of two variables in terms of . If    and , then is a local minimum.     and , then is a local maximum.     , then is neither a local maximum nor minimum.   The conditions in this version of the second derivative test are simply algebraic criteria that tell us about the definiteness of the Hessian matrix .    Summary  This section explored quadratic forms, functions that are defined by symmetric matrices.   If is a symmetric matrix, then the quadratic form defined by is the function . Quadratic forms appear when studying the variance of a dataset. If is the covariance matrix, then the variance in the direction defined by a unit vector is .  Similarly, quadratic forms appear in multivariable calculus when analyzing the behavior of a function of several variables near a critical point.    If is the largest eigenvalue of a symmetric matrix and the smallest, then the maximum value of among unit vectors , is , and this maximum value occurs in the direction of , a unit eigenvector associated to .  Similarly, the minimum value of is , which appears in the direction of , an eigenvector associated to .    A symmetric matrix is positive definite if its eigenvalues are all positive, positive semidefinite if its eigenvalues are all nonnegative, and indefinite if it has both positive and negative eigenvalues.    If the Hessian of a multivariable function is positive definite at a critical point, then the critical point is a local minimum. Likewise, if the Hessian is negative definite, the critical point is a local maximum.        Suppose that .   Find an orthogonal diagonalization of .    Evaluate the quadratic form .    Find the unit vector for which is as large as possible. What is the value of in this direction?    Find the unit vector for which is as small as possible. What is the value of in this direction?          where          The maximum value is in the direction .    The minimum value is in the direction .          where          The maximum value is in the direction .    The minimum value is in the direction .       Consider the quadratic form    Find a matrix such that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.              The maximum value is in the direction . The minimum value is in the direction .              The maximum value is in the direction . The minimum value is in the direction .       Suppose that is a demeaned data matrix:    Find the covariance matrix .    What is the variance of the data projected onto the line defined by .    What is the total variance?    In which direction is the variance greatest and what is the variance in this direction?                    .    In the direction where .                   The total variance is .    The variance is greatest in the direction where .       Consider the matrix .   Find and such that .    Find the maximum and minimum values of among all unit vectors .    Describe the direction in which the minimum value occurs. What can you say about the direction in which the maximum occurs?              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .       Consider the matrix .   Find the matrix so that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.    What does the minimum value of tell you about the matrix ?          .    The maximum value is in the direction and the minimum value is in the direction .    The columns of are linearly dependent.          so .    The maximum value is in the direction and the minimum value is in the direction .    The minimum value of says there is a vector such that , which means that . So there is a nonzero vector that is a solution to the homogeneous equations, which implies that the columns of are linearly dependent.       Consider the quadratic form    What can you say about the definiteness of the matrix that defines the quadratic form?    Find a matrix so that the change of coordinates transforms the quadratic form into one that has no cross terms. Write the quadratic form in terms of .    What are the maximum and minimum values for among all unit vectors ?          is positive definite.     and     The maximum value is and the minimum value is .         The matrix has eigenvalues , , and so is positive definite.     . If , then     The maximum value is and the minimum value is .       Explain why the following statements are true.   Given any matrix , the matrix is a symmetric, positive semidefinite matrix.    If both and are symmetric, positive definite matrices, then is a symmetric, positive definite matrix.    If is a symmetric, invertible, positive definite matrix, then is also.          .     .    The eigenvalues of are the reciprocals of the eigenvalues of .         The matrix is symmetric because . If is an eigenvector having unit length, then and .    If is a nonzero vector, then .    In this case, we know that the eigenvalues of are all positive. If is an eigenvalue of , then is an eigenvalue of so is also positive definite.       Determine whether the following statements are true or false and explain your reasoning.   If is an indefinite matrix, we can't know whether it is positive definite or not.    If the smallest eigenvalue of is 3, then is positive definite.    If is the covariance matrix associated with a dataset, then is positive semidefinite.    If is a symmetric matrix and the maximum and minimum values of occur at and , then is diagonal.    If is negative definite and is an orthogonal matrix with , then is negative definite.         False    True    True    True    True         False. We know that it is neither positive nor negative definite.    True. This implies that for all unit vectors . Then for any other vector .    True. If is an eigenvalue with associated unit eigenvector , then     True. The matrix is the identity so .    True. In this case, and are similar so they have the same eigenvalues, all of which are negative.       Determine the critical points for each of the following functions. At each critical point, determine the Hessian , describe the definiteness of , and determine whether the critical point is a local maximum or minimum.    .     .          is neither a local maximum nor minimum.     is neither a local maximum nor minimum. Both and are local minima.            Also, , , and so that the Hessian matrix is . The eigenvalues are and so this matrix is indefinite and the critical point is neither a local maximum nor minimum.     so and . Therefore, so , , or .  Also, , , and so that the Hessian matrix is .  At , the Hessian is indefinite so this critical point is neither a local maximum nor minimum. At both and , it is positive definite so the critical points are local minima.       Consider the function .   Show that has a critical point at and construct the Hessian at that point.    Find the eigenvalues of . Is this a definite matrix of some kind?    What does this imply about whether is a local maximum or minimum?         The Hessian matrix at that point is      is positive definite.    The critical point is a local minimum.          and all three equations are satisfied at the point .  The Hessian matrix at that point is     The eigenvalues are , , and so is positive definite    This implies that the critical point is a local minimum.       "
},
{
  "id": "preview-quadforms",
  "level": "2",
  "url": "sec-quadratic-forms.html#preview-quadforms",
  "type": "Preview Activity",
  "number": "7.2.1",
  "title": "",
  "body": "  Let's begin by looking at an example. Suppose we have three data points that form the demeaned data matrix    Plot the demeaned data points in . In which direction does the variance appear to be largest and in which does it appear to be smallest?      Use this coordinate grid to plot the demeaned data points.     Construct the covariance matrix and determine the variance in the direction of and the variance in the direction of .     What is the total variance of this dataset?    Generally speaking, if is the covariance matrix of a dataset and is an eigenvector of having unit length and with associated eigenvalue , what is ?          The variance appears to be greatest in the direction of and smallest in the direction of .    In the direction of , the variance is , while in the direction of , the variance is .    The total variance is .           "
},
{
  "id": "example-72",
  "level": "2",
  "url": "sec-quadratic-forms.html#example-72",
  "type": "Example",
  "number": "7.2.2",
  "title": "",
  "body": "  Suppose that . If we write , then we have   We may evaluate the quadratic form using some input vectors: Notice that the value of the quadratic form is a scalar.   "
},
{
  "id": "definition-37",
  "level": "2",
  "url": "sec-quadratic-forms.html#definition-37",
  "type": "Definition",
  "number": "7.2.3",
  "title": "",
  "body": " quadratic form   If is a symmetric matrix, the quadratic form defined by is the function .   "
},
{
  "id": "activity-93",
  "level": "2",
  "url": "sec-quadratic-forms.html#activity-93",
  "type": "Activity",
  "number": "7.2.2",
  "title": "",
  "body": "  Let's look at some more examples of quadratic forms.   Consider the symmetric matrix . Write the quadratic form defined by in terms of the components of . What is the value of ?    Given the symmetric matrix , write the quadratic form defined by and evaluate .    Suppose that . Find a symmetric matrix such that is the quadratic form defined by .    Suppose that is a quadratic form and that . What is ? ? ?    Suppose that is a symmetric matrix and is the quadratic form defined by . Suppose that is an eigenvector of with associated eigenvalue -4 and with length 7. What is ?           and      and          Notice that . In the same way, we have and .                and      and           , and            "
},
{
  "id": "activity-94",
  "level": "2",
  "url": "sec-quadratic-forms.html#activity-94",
  "type": "Activity",
  "number": "7.2.3",
  "title": "",
  "body": "  We can gain some intuition about this problem by graphing the quadratic form and paying particular attention to the unit vectors.   Evaluating the following cell defines the matrix and displays the graph of the associated quadratic form . In addition, the points corresponding to vectors with unit length are displayed as a curve. Notice that the matrix is diagonal. In which directions does the quadratic form have its maximum and minimum values?    Write the quadratic form associated to . What is the value of ? What is the value of ?    Consider a unit vector so that , an expression we can rewrite as . Write the quadratic form and replace by . Now explain why the maximum of is 3. In which direction does the maximum occur? Does this agree with what you observed from the graph above?    Write the quadratic form and replace by . What is the minimum value of and in which direction does the minimum occur?    Use the previous Sage cell to change the matrix to and display the graph of the quadratic form . Determine the directions in which the maximum and minimum occur?    Remember that is symmetric so that where is the diagonal matrix above and is the orthogonal matrix that rotates vectors by . Notice that where . That is, we have .  Explain why is also a unit vector; that is, explain why     Using the fact that , explain how we now know the maximum value of is 3 and determine the direction in which it occurs. Also, determine the minimum value of and determine the direction in which it occurs.          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    We have so that the quadratic form only depends on . The graph of this function of is a parabola that has a maximum of when . Since , this means that the maximum occurs when . We therefore see that the maximum value of is in the direction as we saw from the graph.    Now , which has a minimum value of when . Therefore, the minimum value of is in the direction .    The graph of appears to be similar to the graph of only rotated by . This means the maximum appears to occur in the direction and the minimum in the direction .    Since is orthogonal, we have so that     Since , the maximum of is , which occurs when . This means that , the eigenvector of associated to .  In the same way, the minimum value of is , which occurs when , the eigenvector of associated to .          The maximum appears to occur in the direction of and the minimum appears to occur in the direction of .     so that and .    The maximum value of is in the direction .    The minimum value of is in the direction .    The maximum appears to occur in the direction and the minimum in the direction .    Use the fact that     The maximum of is , which occurs when .  The maximum of is , which occurs when .      "
},
{
  "id": "example-73",
  "level": "2",
  "url": "sec-quadratic-forms.html#example-73",
  "type": "Example",
  "number": "7.2.4",
  "title": "",
  "body": "  Consider the symmetric matrix . Because is symmetric, we know that it can be orthogonally diagonalized. In fact, we have where From this diagonalization, we know that is the largest eigenvalue of with associated eigenvector and that is the smallest eigenvalue with associated eigenvector .  Let's first study the quadratic form because the absence of the cross-term makes it comparatively simple. Remembering that is a unit vector, we have , which means that . Therefore, This tells us that has a maximum value of , which occurs when or in the direction .  In the same way, rewriting allows us to conclude that the minimum value of is , which occurs in the direction .  Let's now return to the matrix whose quadratic form is related to because . In particular, we have In other words, we have where . This is quite useful because it allows us to relate the values of to those of , which we already understand quite well.  Now it turns out that is also a unit vector because Therefore, the maximum value of is the same as , which we know to be and which occurs in the direction . This means that the maximum value of is also and that this occurs in the direction . We now know that the maximum value of is the largest eigenvalue and that this maximum value occurs in the direction of an associated eigenvector.  In the same way, we see that the minimum value of is the smallest eigenvalue and that this minimum occurs in the direction of , an associated eigenvector.   "
},
{
  "id": "prop-quadform-extrema",
  "level": "2",
  "url": "sec-quadratic-forms.html#prop-quadform-extrema",
  "type": "Proposition",
  "number": "7.2.5",
  "title": "",
  "body": "  Suppose that is a symmetric matrix, that we list its eigenvalues in decreasing order , and that is a basis of associated eigenvectors. The maximum value of among all unit vectors is , which occurs in the direction . Similarly, the minimum value of is , which occurs in the direction .   "
},
{
  "id": "example-74",
  "level": "2",
  "url": "sec-quadratic-forms.html#example-74",
  "type": "Example",
  "number": "7.2.6",
  "title": "",
  "body": "  Suppose that is the symmetric matrix , which may be orthogonally diagonalized as where We see that the maximum value of is 12, which occurs in the direction , and the minimum value is -6, which occurs in the direction .   "
},
{
  "id": "example-75",
  "level": "2",
  "url": "sec-quadratic-forms.html#example-75",
  "type": "Example",
  "number": "7.2.7",
  "title": "",
  "body": "  Suppose we have the matrix of demeaned data points that we considered in . The data points are shown in .      The set of demeaned data points from .   Constructing the covariance matrix gives , which has eigenvalues , with associated eigenvector , and , with associated eigenvector .  Remember that the variance in a direction is . Therefore, the variance attains a maximum value of 9 in the direction and a minimum value of 1\/3 in the direction . shows the data projected onto the lines defined by these vectors.       The demeaned data from is shown projected onto the lines of maximal and minimal variance.   Remember that variance is additive, as stated in , which tells us that the total variance is .   "
},
{
  "id": "theorem-4",
  "level": "2",
  "url": "sec-quadratic-forms.html#theorem-4",
  "type": "Theorem",
  "number": "7.2.10",
  "title": "Principle Axes Theorem.",
  "body": " Principle Axes Theorem   If is a symmetric matrix with eigenvalues , then the quadratic form can be written, after an orthogonal change of coordinates , as    "
},
{
  "id": "definition-38",
  "level": "2",
  "url": "sec-quadratic-forms.html#definition-38",
  "type": "Definition",
  "number": "7.2.11",
  "title": "",
  "body": "  A symmetric matrix is called positive definite if its associated quadratic form satisfies for any nonzero vector . If for all nonzero vectors , we say that is positive semidefinite .  Likewise, we say that is negative definite if for all nonzero vectors .  Finally, is called indefinite if for some and for others.   "
},
{
  "id": "activity-95",
  "level": "2",
  "url": "sec-quadratic-forms.html#activity-95",
  "type": "Activity",
  "number": "7.2.4",
  "title": "",
  "body": "  This activity explores the relationship between the eigenvalues of a symmetric matrix and its definiteness.   Consider the diagonal matrix and write its quadratic form in terms of the components of . How does this help you decide whether is positive definite or not?    Now consider and write its quadratic form in terms of and . What can you say about the definiteness of ?    If is a diagonal matrix, what condition on the diagonal entries guarantee that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?      Suppose that is a symmetric matrix with eigenvalues 4 and 2 so that where . If , then we have . Explain why this tells us that is positive definite.    Suppose that is a symmetric matrix with eigenvalues 4 and 0. What can you say about the definiteness of in this case?    What condition on the eigenvalues of a symmetric matrix guarantees that is   positive definite?   positive semidefinite?   negative definite?   negative semidefinite?   indefinite?             . Both addends are nonnegative, and one of them is positive if is nonzero. This means that when is nonzero and so is positive definite.     , which is always nonnegative. However, so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .       Since we know that when is nonzero, we know that when is nonzero. Therefore, is positive definite.    It will be positive semidefinite.    They will be the same as before.           so is positive definite.     so is positive semidefinite.       They are all positive.    They are all nonnegative.    They are all negative.    They are all nonpositive.    They are some positive eigenvalues and some negative ones .        is positive definite.     is positive semidefinite.    They will be the same as before.      "
},
{
  "id": "prop-definite-matrices",
  "level": "2",
  "url": "sec-quadratic-forms.html#prop-definite-matrices",
  "type": "Proposition",
  "number": "7.2.12",
  "title": "",
  "body": "  A symmetric matrix is positive definite if all its eigenvalues are positive. It is positive semidefinite if all its eigenvalues are nonnegative.  Likewise, a symmetric matrix is indefinite if some eigenvalues are positive and some are negative.   "
},
{
  "id": "activity-96",
  "level": "2",
  "url": "sec-quadratic-forms.html#activity-96",
  "type": "Activity",
  "number": "7.2.5",
  "title": "",
  "body": "  Let's explore how our understanding of quadratic forms helps us determine the behavior of a function near a critical point.   Consider the function . Find the partial derivatives and and use these expressions to determine the critical points of .    Evaluate the second partial derivatives , , and .    Let's first consider the critical point . Use the quadratic approximation as written above to find an expression approximating near the critical point.    Using the vector , rewrite your approximation as for some matrix . What is the matrix in this case?    Find the eigenvalues of . What can you conclude about the definiteness of ?    Recall that is a local minimum for if for nearby points . Explain why our understanding of the eigenvalues of shows that is a local minimum for .           We have     We have     This gives     The matrix is .    The eigenvalues are and , both of which are positive, which means that is positive definite.    We have if is nonzero so for points near to .          We have     We have     This gives      .     is positive definite.     for points near to .      "
},
{
  "id": "proposition-54",
  "level": "2",
  "url": "sec-quadratic-forms.html#proposition-54",
  "type": "Proposition",
  "number": "7.2.13",
  "title": "Second Derivative Test.",
  "body": " Second Derivative Test   The nature of a critical point of a multivariable function is determined by the Hessian of the function at the critical point. If    has all positive eigenvalues, the critical point is a local minimum.     has all negative eigenvalues, the critical point is a local maximum.     has both positive and negative eigenvalues, the critical point is neither a local maximum nor minimum.      "
},
{
  "id": "exercise-259",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-259",
  "type": "Exercise",
  "number": "7.2.4.1",
  "title": "",
  "body": " Suppose that .   Find an orthogonal diagonalization of .    Evaluate the quadratic form .    Find the unit vector for which is as large as possible. What is the value of in this direction?    Find the unit vector for which is as small as possible. What is the value of in this direction?          where          The maximum value is in the direction .    The minimum value is in the direction .          where          The maximum value is in the direction .    The minimum value is in the direction .     "
},
{
  "id": "exercise-260",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-260",
  "type": "Exercise",
  "number": "7.2.4.2",
  "title": "",
  "body": " Consider the quadratic form    Find a matrix such that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.              The maximum value is in the direction . The minimum value is in the direction .              The maximum value is in the direction . The minimum value is in the direction .     "
},
{
  "id": "exercise-261",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-261",
  "type": "Exercise",
  "number": "7.2.4.3",
  "title": "",
  "body": " Suppose that is a demeaned data matrix:    Find the covariance matrix .    What is the variance of the data projected onto the line defined by .    What is the total variance?    In which direction is the variance greatest and what is the variance in this direction?                    .    In the direction where .                   The total variance is .    The variance is greatest in the direction where .     "
},
{
  "id": "exercise-262",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-262",
  "type": "Exercise",
  "number": "7.2.4.4",
  "title": "",
  "body": " Consider the matrix .   Find and such that .    Find the maximum and minimum values of among all unit vectors .    Describe the direction in which the minimum value occurs. What can you say about the direction in which the maximum occurs?              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .              The maximum is 7 and the minimum is -2.    The minimum occurs in the direction of . The maximum occurs along a circle defined by where and and are the first two columns of .     "
},
{
  "id": "exercise-263",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-263",
  "type": "Exercise",
  "number": "7.2.4.5",
  "title": "",
  "body": " Consider the matrix .   Find the matrix so that .    Find the maximum and minimum values of among all unit vectors and describe the directions in which they occur.    What does the minimum value of tell you about the matrix ?          .    The maximum value is in the direction and the minimum value is in the direction .    The columns of are linearly dependent.          so .    The maximum value is in the direction and the minimum value is in the direction .    The minimum value of says there is a vector such that , which means that . So there is a nonzero vector that is a solution to the homogeneous equations, which implies that the columns of are linearly dependent.     "
},
{
  "id": "exercise-264",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-264",
  "type": "Exercise",
  "number": "7.2.4.6",
  "title": "",
  "body": " Consider the quadratic form    What can you say about the definiteness of the matrix that defines the quadratic form?    Find a matrix so that the change of coordinates transforms the quadratic form into one that has no cross terms. Write the quadratic form in terms of .    What are the maximum and minimum values for among all unit vectors ?          is positive definite.     and     The maximum value is and the minimum value is .         The matrix has eigenvalues , , and so is positive definite.     . If , then     The maximum value is and the minimum value is .     "
},
{
  "id": "exercise-265",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-265",
  "type": "Exercise",
  "number": "7.2.4.7",
  "title": "",
  "body": " Explain why the following statements are true.   Given any matrix , the matrix is a symmetric, positive semidefinite matrix.    If both and are symmetric, positive definite matrices, then is a symmetric, positive definite matrix.    If is a symmetric, invertible, positive definite matrix, then is also.          .     .    The eigenvalues of are the reciprocals of the eigenvalues of .         The matrix is symmetric because . If is an eigenvector having unit length, then and .    If is a nonzero vector, then .    In this case, we know that the eigenvalues of are all positive. If is an eigenvalue of , then is an eigenvalue of so is also positive definite.     "
},
{
  "id": "exercise-266",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-266",
  "type": "Exercise",
  "number": "7.2.4.8",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.   If is an indefinite matrix, we can't know whether it is positive definite or not.    If the smallest eigenvalue of is 3, then is positive definite.    If is the covariance matrix associated with a dataset, then is positive semidefinite.    If is a symmetric matrix and the maximum and minimum values of occur at and , then is diagonal.    If is negative definite and is an orthogonal matrix with , then is negative definite.         False    True    True    True    True         False. We know that it is neither positive nor negative definite.    True. This implies that for all unit vectors . Then for any other vector .    True. If is an eigenvalue with associated unit eigenvector , then     True. The matrix is the identity so .    True. In this case, and are similar so they have the same eigenvalues, all of which are negative.     "
},
{
  "id": "exercise-267",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-267",
  "type": "Exercise",
  "number": "7.2.4.9",
  "title": "",
  "body": " Determine the critical points for each of the following functions. At each critical point, determine the Hessian , describe the definiteness of , and determine whether the critical point is a local maximum or minimum.    .     .          is neither a local maximum nor minimum.     is neither a local maximum nor minimum. Both and are local minima.            Also, , , and so that the Hessian matrix is . The eigenvalues are and so this matrix is indefinite and the critical point is neither a local maximum nor minimum.     so and . Therefore, so , , or .  Also, , , and so that the Hessian matrix is .  At , the Hessian is indefinite so this critical point is neither a local maximum nor minimum. At both and , it is positive definite so the critical points are local minima.     "
},
{
  "id": "exercise-268",
  "level": "2",
  "url": "sec-quadratic-forms.html#exercise-268",
  "type": "Exercise",
  "number": "7.2.4.10",
  "title": "",
  "body": " Consider the function .   Show that has a critical point at and construct the Hessian at that point.    Find the eigenvalues of . Is this a definite matrix of some kind?    What does this imply about whether is a local maximum or minimum?         The Hessian matrix at that point is      is positive definite.    The critical point is a local minimum.          and all three equations are satisfied at the point .  The Hessian matrix at that point is     The eigenvalues are , , and so is positive definite    This implies that the critical point is a local minimum.     "
},
{
  "id": "sec-pca",
  "level": "1",
  "url": "sec-pca.html",
  "type": "Section",
  "number": "7.3",
  "title": "Principal Component Analysis",
  "body": " Principal Component Analysis   We are sometimes presented with a dataset having many data points that live in a high dimensional space. For instance, we looked at a dataset describing body fat index (BFI) in where each data point is six-dimensional. Developing an intuitive understanding of the data is hampered by the fact that it cannot be visualized.  This section explores a technique called principal component analysis , which enables us to reduce the dimension of a dataset so that it may be visualized or studied in a way so that interesting features more readily stand out. Our previous work with variance and the orthogonal diagonalization of symmetric matrices provides the key ideas.    We will begin by recalling our earlier discussion of variance. Suppose we have a dataset that leads to the covariance matrix    Suppose that is a unit eigenvector of with eigenvalue . What is the variance in the direction?    Find an orthogonal diagonalization of .     What is the total variance?    In which direction is the variance greatest and what is the variance in this direction? If we project the data onto this line, how much variance is lost?    In which direction is the variance smallest and how is this direction related to the direction of maximum variance?           .    We can write where     The total variance is the sum of the eigenvalues, .    The variance is greatest in the direction of the eigenvector associated to the largest eigenvalue. This direction is defined by , and the variance is 15 in this direction.    The variance is smallest in the direction defined by .       Here are some ideas we've seen previously that will be particularly useful for us in this section. Remember that the covariance matrix of a dataset is where is the matrix of demeaned data points.   When is a unit vector, the variance of the demeaned data after projecting onto the line defined by is given by the quadratic form .    In particular, if is a unit eigenvector of with associated eigenvalue , then .    Moreover, variance is additive, as we recorded in : if is a subspace having an orthonormal basis , then the variance        Principal Component Analysis  Let's begin by looking at an example that illustrates the central theme of this technique.    Suppose that we work with a dataset having 100 five-dimensional data points. The demeaned data matrix is therefore and leads to the covariance matrix , which is a matrix. Because is symmetric, the Spectral Theorem tells us it is orthogonally diagonalizable so suppose that where    What is , the variance in the direction?    Find the variance of the data projected onto the line defined by . What does this say about the data?    What is the total variance of the data?    Consider the 2-dimensional subspace spanned by and . If we project the data onto this subspace, what fraction of the total variance is represented by the variance of the projected data?    How does this question change if we project onto the 3-dimensional subspace spanned by , , and ?    What does this tell us about the data?                , which tells us there is no variance in the direction. Therefore, when we project onto the line defined by , every data point projects to so every data point is in the orthogonal complement of .     .    The variance of the data projected onto this subspace is , which represents of the variance.    Projecting onto this 3-dimensional subspace retains all of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .                , which tells us every data point is in the orthogonal complement of .          of the variance     of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .       This activity demonstrates how the eigenvalues of the covariance matrix can tell us when data are clustered around, or even wholly contained within, a smaller dimensional subspace. In particular, the original data is 5-dimensional, but we see that it actually lies in a 3-dimensional subspace of . Later in this section, we'll see how to use this observation to work with the data as if it were three-dimensional, an idea known as dimensional reduction .   principal components The eigenvectors of the covariance matrix are called principal components , and we will order them so that their associated eigenvalues decrease. Generally speaking, we hope that the first few principal components retain most of the variance, as the example in the activity demonstrates. In that example, we have the sequence of subspaces    , the 1-dimensional subspace spanned by , which retains of the total variance,     , the 2-dimensional subspace spanned by and , which retains of the variance, and     , the 3-dimensional subspace spanned by , , and , which retains all of the variance.     Notice how we retain more of the total variance as we increase the dimension of the subspace onto which the data are projected. Eventually, projecting the data onto retains all the variance, which tells us the data must lie in , a smaller dimensional subspace of .  In fact, these subspaces are the best possible. We know that the first principal component is the eigenvector of associated to the largest eigenvalue. This means that the variance is as large as possible in the direction. In other words, projecting onto any other line will retain a smaller amount of variance. Similarly, projecting onto any other 2-dimensional subspace besides will retain less variance than projecting onto . The principal components have the wonderful ability to pick out the best possible subspaces to retain as much variance as possible.  Of course, this is a contrived example. Typically, the presence of noise in a dataset means that we do not expect all the points to be wholly contained in a smaller dimensional subspace. In fact, the 2-dimensional subspace retains of the variance. Depending on the situation, we may want to write off the remaining of the variance as noise in exchange for the convenience of working with a smaller dimensional subspace. As we'll see later, we will seek a balance using a number of principal components large enough to retain most of the variance but small enough to be easy to work with.    We will work here with a dataset having 100 3-dimensional demeaned data points. Evaluating the following cell will plot those data points and define the demeaned data matrix A whose shape is . Notice that the data appears to cluster around a plane though it does not seem to be wholly contained within that plane.   Use the matrix A to construct the covariance matrix . Then determine the variance in the direction of ?     Find the eigenvalues of and determine the total variance. Notice that Sage does not necessarily sort the eigenvalues in decreasing order.    Use the right_eigenmatrix() command to find the eigenvectors of . Remembering that the Sage command B.column(1) retrieves the vector represented by the second column of B , define vectors u1 , u2 , and u3 representing the three principal components in order of decreasing eigenvalues. How can you check if these vectors are an orthonormal basis for ?    What fraction of the total variance is retained by projecting the data onto , the subspace spanned by ? What fraction of the total variance is retained by projecting onto , the subspace spanned by and ? What fraction of the total variance do we lose by projecting onto ?    If we project a data point onto , the Projection Formula tells us we obtain Rather than viewing the projected data in , we will record the coordinates of in the basis defined by and ; that is, we will record the coordinates Construct the matrix so that .     Since each column of represents a data point, the matrix represents the coordinates of the projected data points. Evaluating the following cell will plot those projected data points. Notice how this plot enables us to view the data as if it were two-dimensional. Why is this plot wider than it is tall?          After constructing the covariance matrix , we find that .    The total variance is the sum of the eigenvalues of so we obtain .    If we obtain , the matrix of eigenvectors, from Sage, computing evaluates the dot products between the columns. Since , the basis provided by Sage is orthonormal.    Projecting onto , we see that so retains about of the total variance. The subspace retains or of the total variance. If we project onto we lose less than of the variance.         The plot is wider because the variance in the direction, which corresponds to the horizontal coordinate, is greater than the variance in the direction.                    If is the matrix of eigenvectors, evaluate .     retains of the total variance, and retains .         Because the variance in the direction is greater than the variance in the direction.       This example is a more realistic illustration of principal component analysis. The plot of the 3-dimensional data appears to show that the data lies close to a plane, and the principal components will identify this plane. Starting with the matrix of demeaned data , we construct the covariance matrix and study its eigenvalues. Notice that the first two principal components account for more than 98% of the variance, which means we can expect the points to lie close to , the two-dimensional subspace spanned by and .  Since is a subspace of , projecting the data points onto gives a list of 100 points in . In order to visualize them more easily, we instead consider the coordinates of the projections in the basis defined by and . For instance, we know that the projection of a data point is which is a three-dimensional vector. Instead, we can record the coordinates and plot them in the two-dimensional coordinate plane, as illustrated in .       The projection of a data point onto is a three-dimensional vector, which may be represented by the two coordinates describing this vector as a linear combination of and .   If we form the matrix , then we have This means that the columns of represent the coordinates of the projected points, which may now be plotted in the plane.  In this plot, the first coordinate, represented by the horizontal coordinate, represents the projection of a data point onto the line defined by while the second coordinate represents the projection onto the line defined by . Since is the first principal component, the variance in the direction is greater than the variance in the direction. For this reason, the plot will be more spread out in the horizontal direction than in the vertical.    Using Principal Component Analysis  Now that we've explored the ideas behind principal component analysis, we will look at a few examples that illustrate its use.    The next cell will load a dataset describing the average consumption of various food groups for citizens in each of the four nations of the United Kingdom. The units for each entry are grams per person per week. We will view this as a dataset consisting of four points in . As such, it is impossible to visualize and studying the numbers themselves doesn't lead to much insight.  In addition to loading the data, evaluating the cell above created a vector data_mean , which is the mean of the four data points, and A , the matrix of demeaned data.   What is the average consumption of Beverages across the four nations?     Find the covariance matrix and its eigenvalues. Because there are four points in whose mean is zero, there are only three nonzero eigenvalues.    For what percentage of the total variance does the first principal component account?    Find the first principal component and project the four demeaned data points onto the line defined by . Plot those points on       A plot of the demeaned data projected onto the first principal component.     For what percentage of the total variance do the first two principal components account?    Find the coordinates of the demeaned data points projected onto , the two-dimensional subspace of spanned by the first two principal components.   Plot these coordinates in .      The coordinates of the demeaned data points projected onto the first two principal components.     What information do these plots reveal that is not clear from consideration of the original data points?    Study the first principal component and find the first component of , which corresponds to the dietary category Alcoholic Drinks. (To do this, you may wish to use N(u1, digits=2) for a result that's easier to read.) If a data point lies on the far right side of the plot in , what does it mean about that nation's consumption of Alcoholic Drinks?          Beverages is the second category so this would be the second component of the data_mean vector, which is .    The three nonzero eigenvalues are , , and .    The total variance is the sum of the eigenvalues so the first principal component accounts for of the total variance.    The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales           The first two principal components account for of the total variance.    The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations. There are several possible reasons for this, both historical and geographical, that we might explore.    The first component of is negative. Therefore, if a nation is on the right side of this plot, the average consumption of Alcoholic Drinks will be less than the mean. This can be confirmed by looking at the original data.                , , and          The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales                The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations.    The average consumption of Alcoholic Drinks will be less than the mean.       This activity demonstrates how principal component analysis enables us to extract information from a dataset that may not be easily obtained otherwise. As in our previous example, we see that the data points lie quite close to a two-dimensional subspace of . In fact, , the subspace spanned by the first two principal components, accounts for more than 96% of the variance. More importantly, when we project the data onto , it becomes apparent that Northern Ireland is fundamentally different from the other three nations.  With some additional thought, we can determine more specific ways in which Northern Ireland is different. On the -dimensional plot, Northern Ireland lies far to the right compared to the other three nations. Since the data has been demeaned, the origin in this plot corresponds to the average of the four nations. The coordinates of the point representing Northern Ireland are about , meaning that the projected data point differs from the mean by about .  Let's just focus on the contribution from . We see that the ninth component of , the one that describes Fresh Fruit, is about . This means that the ninth component of differs from the mean by about grams per person per week. So roughly speaking, people in Northern Ireland are eating about 300 fewer grams of Fresh Fruit than the average across the four nations. This is borne out by looking at the original data, which show that the consumption of Fresh Fruit in Northern Ireland is significantly less than the other nations. Examing the other components of shows other ways in which Northern Ireland differs from the other three nations.    In this activity, we'll look at a well-known dataset that describes 150 irises representing three species of iris: Iris setosa, Iris versicolor, and Iris virginica. For each flower, the length and width of its sepal and the length and width of its petal, all in centimeters, are recorded.      One of the three species, iris versicolor, represented in the dataset showing three shorter petals and three longer sepals. (Source: Wikipedia , License: GNU Free Documetation License )   Evaluating the following cell will load the dataset, which consists of 150 points in . In addition, we have a vector data_mean , a four-dimensional vector holding the mean of the data points, and A , the demeaned data matrix. Since the data is four-dimensional, we are not able to visualize it. Of course, we could forget about two of the measurements and plot the 150 points represented by their, say, sepal length and sepal width.    What is the mean sepal width?    Find the covariance matrix and its eigenvalues.     Find the fraction of variance for which the first two principal components account.    Construct the first two principal components and along with the matrix whose columns are and .     As we have seen, the columns of the matrix hold the coordinates of the demeaned data points after projecting onto , the subspace spanned by the first two principal components. Evaluating the following cell shows a plot of these coordinates. Suppose we have a flower whose coordinates in this plane are . To what species does this iris most likely belong? Find an estimate of the sepal length, sepal width, petal length, and petal width for this flower.    Suppose you have an iris, but you only know that its sepal length is 5.65 cm and its sepal width is 2.75 cm. Knowing only these two measurements, determine the coordinates in the plane where this iris lies. To what species does this iris most likely belong? Now estimate the petal length and petal width of this iris.     Suppose you find another iris whose sepal width is 3.2 cm and whose petal width is 2.2 cm. Find the coordinates of this iris and determine the species to which it most likely belongs. Also, estimate the sepal length and the petal length.           The second component of data_mean , which is the one corresponding to sepal width, is .    The eigenvalues are , , , and .    The first two principal components account for of the variance.    If is the matrix whose columns are an orthonormal basis of eigenvectors, then is formed from the first two columns of .    This would most likely belong to Iris setosa. To find its measurements, we evaluate where is the vector of means. This is the same as , which gives the vector of measurements .    Subtracting the mean sepal length and sepal width, we have . Then the first two components of . This gives . This looks like an Iris versicolor. As in the previous part, we can now find the petal length to be and the petal width to be .    Using the same approach as the last part, we find , which gives a sepal length of and a petal length of . Most likely, this flower belongs to Iris virginica.           .     , , , .         The columns of are for the first two principal components.    Iris setosa and the vector of measurements is .    The petal length is and the petal width is .    The sepal length is and the petal length is .         Summary  This section has explored principal component analysis as a technique to reduce the dimension of a dataset. From the demeaned data matrix , we form the covariance matrix , where is the number of data points.   The eigenvectors , of are called the principal components. We arrange them so that their corresponding eigenvalues are in decreasing order.    If is the subspace spanned by the first principal components, then the variance of the demeaned data projected onto is the sum of the first eigenvalues of . No other -dimensional subspace retains more variance when the data is projected onto it.    If is the matrix whose columns are the first principal components, then the columns of hold the coordinates, expressed in the basis , of the data once projected onto .    Our goal is to use a number of principal components that is large enough to retain most of the variance in the dataset but small enough to be manageable.        Suppose that and that we have two datasets, one whose covariance matrix is and one whose covariance matrix is . For each dataset, find   the total variance.    the fraction of variance represented by the first principal component.    a verbal description of how the demeaned data points appear when plotted in the plane.          for the first dataset and for the second     and     The points in the second set are clustered very close to a line.         For the first dataset, the total variance is while the second dataset has a total variance of .    In the first dataset, the first principal component represents of the variance compared to in the second dataset.    The points in the first dataset are scattered almost uniformly around the origin. In the second dataset, they are clustered very close to the line defined by the vector .       Suppose that a dataset has mean and that its associated covariance matrix is .    What fraction of the variance is represented by the first two principal components?    If is one of the data points, find the coordinates when the demeaned point is projected into the plane defined by the first two principal components.    If a projected data point has coordinates , find an estimate for the original data point.               .     .         The eigenvalues are , , and so the fraction of variance represented by the first two principal components is     Suppose is this data point and the vector of means. If is the matrix whose columns are the first two principal components, then we find .    We find .       Evaluating the following cell loads a demeaned data matrix A .    Find the principal components and and the variance in the direction of each principal component.    What is the total variance?    What can you conclude about this dataset?         The variances are and .         All of the data lies on a line         We find that and . The variances are and .    The total variance is .    All of the data lies on the line defined by .       Determine whether the following statements are true or false and explain your thinking.   If the eigenvalues of the covariance matrix are , , and , then is the variance of the demeaned data points when projected on the third principal component .    Principal component analysis always allows us to construct a smaller dimensional representation of a dataset without losing any information.    If the eigenvalues of the covariance matrix are 56, 32, and 0, then the demeaned data points all lie on a line in .         True    False    False         True. The variance of the projected data points is .    False. If we project onto a subspace that retains less than all the variance, then we are losing information about the data in the directions orthogonal to the subspace.    False. They will lie on the plane spanned by the first two principal components.       In , we looked at a dataset consisting of four measurements of 150 irises. These measurements are sepal length, sepal width, petal length, and petal width.   Find the first principal component and describe the meaning of its four components. Which component is most significant? What can you say about the relative importance of the four measurements?    When the dataset is plotted in the plane defined by and , the specimens from the species iris-setosa lie on the left side of the plot. What does this tell us about how iris-setosa differs from the other two species in the four measurements?    In general, which species is closest to the average iris ?         The petal length is the most signficant and the sepal width is the least.    Iris setosa has a below average petal length and appears overall smaller.    Iris versicolor         The first principal component is . If we move in the direction, each component describes how we move away from the mean of that measurement. The most important component is the third one, which corresponds to petal length, while the second component, corresponding to sepal width, is least signficant. Of the four measurements, there is the most variance in petal length.    To reach an Iris setosa sample, we multiply by a negative number. This says that the petal length, the most signficant component of will be below the mean for Iris setosa. The sepal length and petal width will also be below the mean. Basically, the Iris setosa would appear to be smaller overall.    Iris versicolor would appear to be closest to average since the points are closest to the origin in the - plane.       This problem explores a dataset describing 333 penguins. There are three species, Adelie, Chinstrap, and Gentoo, as illustrated on the left of , as well as both male and female penguins in the dataset.       Artwork by @allison_horst    Evaluating the next cell will load and display the data. The meaning of the culmen length and width is contained in the illustration on the right of . This dataset is a bit different from others that we've looked at because the scale of the measurements is significantly different. For instance, the measurements for the body mass are roughly 100 times as large as those for the culmen length. For this reason, we will standardize the data by first demeaning it, as usual, and then rescaling each measurement by the reciprocal of its standard deviation. The result is stored in the matrix A .    Find the covariance matrix and its eigenvalues.    What fraction of the total variance is explained by the first two principal components?    Construct the matrix whose columns are the coordinates of the demeaned data points projected onto the first two principal components. The following cell will create the plot.     Examine the components of the first two principal component vectors. How does the body mass of Gentoo penguins compare to that of the other two species?    What seems to be generally true about the culmen measurements for a Chinstrap penguin compared to a Adelie?    You can plot just the males or females using the following cell. What seems to be generally true about the body mass measurements for a male Gentoo compared to a female Gentoo?          , , , .          where the columns of are the first two principal components.    Gentoo penguins have a larger body mass.    These measurements seem larger for the Chinstrap penguins.    The male Gentoo have a higher body mass.         The eigenvalues are , , , and .    The first two principal components retain of the total variance.     where the columns of are the first two principal components.    Gentoo penguins have a larger body mass because the fourth component, which corresponds to body mass, of is negative and the Gentoo penguins are represented by where is negative.    These measurements seem larger for the Chinstrap penguins because the first two components of are positive.    The male Gentoo lie further to the left, which means they have a higher body mass.       "
},
{
  "id": "exploration-28",
  "level": "2",
  "url": "sec-pca.html#exploration-28",
  "type": "Preview Activity",
  "number": "7.3.1",
  "title": "",
  "body": "  We will begin by recalling our earlier discussion of variance. Suppose we have a dataset that leads to the covariance matrix    Suppose that is a unit eigenvector of with eigenvalue . What is the variance in the direction?    Find an orthogonal diagonalization of .     What is the total variance?    In which direction is the variance greatest and what is the variance in this direction? If we project the data onto this line, how much variance is lost?    In which direction is the variance smallest and how is this direction related to the direction of maximum variance?           .    We can write where     The total variance is the sum of the eigenvalues, .    The variance is greatest in the direction of the eigenvector associated to the largest eigenvalue. This direction is defined by , and the variance is 15 in this direction.    The variance is smallest in the direction defined by .      "
},
{
  "id": "activity-97",
  "level": "2",
  "url": "sec-pca.html#activity-97",
  "type": "Activity",
  "number": "7.3.2",
  "title": "",
  "body": "  Suppose that we work with a dataset having 100 five-dimensional data points. The demeaned data matrix is therefore and leads to the covariance matrix , which is a matrix. Because is symmetric, the Spectral Theorem tells us it is orthogonally diagonalizable so suppose that where    What is , the variance in the direction?    Find the variance of the data projected onto the line defined by . What does this say about the data?    What is the total variance of the data?    Consider the 2-dimensional subspace spanned by and . If we project the data onto this subspace, what fraction of the total variance is represented by the variance of the projected data?    How does this question change if we project onto the 3-dimensional subspace spanned by , , and ?    What does this tell us about the data?                , which tells us there is no variance in the direction. Therefore, when we project onto the line defined by , every data point projects to so every data point is in the orthogonal complement of .     .    The variance of the data projected onto this subspace is , which represents of the variance.    Projecting onto this 3-dimensional subspace retains all of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .                , which tells us every data point is in the orthogonal complement of .          of the variance     of the variance.    All of the data lies in the -dimensional subspace spanned by , , and .      "
},
{
  "id": "activity-98",
  "level": "2",
  "url": "sec-pca.html#activity-98",
  "type": "Activity",
  "number": "7.3.3",
  "title": "",
  "body": "  We will work here with a dataset having 100 3-dimensional demeaned data points. Evaluating the following cell will plot those data points and define the demeaned data matrix A whose shape is . Notice that the data appears to cluster around a plane though it does not seem to be wholly contained within that plane.   Use the matrix A to construct the covariance matrix . Then determine the variance in the direction of ?     Find the eigenvalues of and determine the total variance. Notice that Sage does not necessarily sort the eigenvalues in decreasing order.    Use the right_eigenmatrix() command to find the eigenvectors of . Remembering that the Sage command B.column(1) retrieves the vector represented by the second column of B , define vectors u1 , u2 , and u3 representing the three principal components in order of decreasing eigenvalues. How can you check if these vectors are an orthonormal basis for ?    What fraction of the total variance is retained by projecting the data onto , the subspace spanned by ? What fraction of the total variance is retained by projecting onto , the subspace spanned by and ? What fraction of the total variance do we lose by projecting onto ?    If we project a data point onto , the Projection Formula tells us we obtain Rather than viewing the projected data in , we will record the coordinates of in the basis defined by and ; that is, we will record the coordinates Construct the matrix so that .     Since each column of represents a data point, the matrix represents the coordinates of the projected data points. Evaluating the following cell will plot those projected data points. Notice how this plot enables us to view the data as if it were two-dimensional. Why is this plot wider than it is tall?          After constructing the covariance matrix , we find that .    The total variance is the sum of the eigenvalues of so we obtain .    If we obtain , the matrix of eigenvectors, from Sage, computing evaluates the dot products between the columns. Since , the basis provided by Sage is orthonormal.    Projecting onto , we see that so retains about of the total variance. The subspace retains or of the total variance. If we project onto we lose less than of the variance.         The plot is wider because the variance in the direction, which corresponds to the horizontal coordinate, is greater than the variance in the direction.                    If is the matrix of eigenvectors, evaluate .     retains of the total variance, and retains .         Because the variance in the direction is greater than the variance in the direction.      "
},
{
  "id": "fig-pca-coords",
  "level": "2",
  "url": "sec-pca.html#fig-pca-coords",
  "type": "Figure",
  "number": "7.3.1",
  "title": "",
  "body": "     The projection of a data point onto is a three-dimensional vector, which may be represented by the two coordinates describing this vector as a linear combination of and .  "
},
{
  "id": "activity-99",
  "level": "2",
  "url": "sec-pca.html#activity-99",
  "type": "Activity",
  "number": "7.3.4",
  "title": "",
  "body": "  The next cell will load a dataset describing the average consumption of various food groups for citizens in each of the four nations of the United Kingdom. The units for each entry are grams per person per week. We will view this as a dataset consisting of four points in . As such, it is impossible to visualize and studying the numbers themselves doesn't lead to much insight.  In addition to loading the data, evaluating the cell above created a vector data_mean , which is the mean of the four data points, and A , the matrix of demeaned data.   What is the average consumption of Beverages across the four nations?     Find the covariance matrix and its eigenvalues. Because there are four points in whose mean is zero, there are only three nonzero eigenvalues.    For what percentage of the total variance does the first principal component account?    Find the first principal component and project the four demeaned data points onto the line defined by . Plot those points on       A plot of the demeaned data projected onto the first principal component.     For what percentage of the total variance do the first two principal components account?    Find the coordinates of the demeaned data points projected onto , the two-dimensional subspace of spanned by the first two principal components.   Plot these coordinates in .      The coordinates of the demeaned data points projected onto the first two principal components.     What information do these plots reveal that is not clear from consideration of the original data points?    Study the first principal component and find the first component of , which corresponds to the dietary category Alcoholic Drinks. (To do this, you may wish to use N(u1, digits=2) for a result that's easier to read.) If a data point lies on the far right side of the plot in , what does it mean about that nation's consumption of Alcoholic Drinks?          Beverages is the second category so this would be the second component of the data_mean vector, which is .    The three nonzero eigenvalues are , , and .    The total variance is the sum of the eigenvalues so the first principal component accounts for of the total variance.    The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales           The first two principal components account for of the total variance.    The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations. There are several possible reasons for this, both historical and geographical, that we might explore.    The first component of is negative. Therefore, if a nation is on the right side of this plot, the average consumption of Alcoholic Drinks will be less than the mean. This can be confirmed by looking at the original data.                , , and          The coordinates are    Nation  Coordinate    England       Northern Ireland       Scotland       Wales                The coordinates are    Nation  Coordinates    England       Northern Ireland       Scotland       England           Northern Ireland appears to be significantly different from the other three nations.    The average consumption of Alcoholic Drinks will be less than the mean.      "
},
{
  "id": "activity-pca-iris",
  "level": "2",
  "url": "sec-pca.html#activity-pca-iris",
  "type": "Activity",
  "number": "7.3.5",
  "title": "",
  "body": "  In this activity, we'll look at a well-known dataset that describes 150 irises representing three species of iris: Iris setosa, Iris versicolor, and Iris virginica. For each flower, the length and width of its sepal and the length and width of its petal, all in centimeters, are recorded.      One of the three species, iris versicolor, represented in the dataset showing three shorter petals and three longer sepals. (Source: Wikipedia , License: GNU Free Documetation License )   Evaluating the following cell will load the dataset, which consists of 150 points in . In addition, we have a vector data_mean , a four-dimensional vector holding the mean of the data points, and A , the demeaned data matrix. Since the data is four-dimensional, we are not able to visualize it. Of course, we could forget about two of the measurements and plot the 150 points represented by their, say, sepal length and sepal width.    What is the mean sepal width?    Find the covariance matrix and its eigenvalues.     Find the fraction of variance for which the first two principal components account.    Construct the first two principal components and along with the matrix whose columns are and .     As we have seen, the columns of the matrix hold the coordinates of the demeaned data points after projecting onto , the subspace spanned by the first two principal components. Evaluating the following cell shows a plot of these coordinates. Suppose we have a flower whose coordinates in this plane are . To what species does this iris most likely belong? Find an estimate of the sepal length, sepal width, petal length, and petal width for this flower.    Suppose you have an iris, but you only know that its sepal length is 5.65 cm and its sepal width is 2.75 cm. Knowing only these two measurements, determine the coordinates in the plane where this iris lies. To what species does this iris most likely belong? Now estimate the petal length and petal width of this iris.     Suppose you find another iris whose sepal width is 3.2 cm and whose petal width is 2.2 cm. Find the coordinates of this iris and determine the species to which it most likely belongs. Also, estimate the sepal length and the petal length.           The second component of data_mean , which is the one corresponding to sepal width, is .    The eigenvalues are , , , and .    The first two principal components account for of the variance.    If is the matrix whose columns are an orthonormal basis of eigenvectors, then is formed from the first two columns of .    This would most likely belong to Iris setosa. To find its measurements, we evaluate where is the vector of means. This is the same as , which gives the vector of measurements .    Subtracting the mean sepal length and sepal width, we have . Then the first two components of . This gives . This looks like an Iris versicolor. As in the previous part, we can now find the petal length to be and the petal width to be .    Using the same approach as the last part, we find , which gives a sepal length of and a petal length of . Most likely, this flower belongs to Iris virginica.           .     , , , .         The columns of are for the first two principal components.    Iris setosa and the vector of measurements is .    The petal length is and the petal width is .    The sepal length is and the petal length is .      "
},
{
  "id": "exercise-269",
  "level": "2",
  "url": "sec-pca.html#exercise-269",
  "type": "Exercise",
  "number": "7.3.4.1",
  "title": "",
  "body": " Suppose that and that we have two datasets, one whose covariance matrix is and one whose covariance matrix is . For each dataset, find   the total variance.    the fraction of variance represented by the first principal component.    a verbal description of how the demeaned data points appear when plotted in the plane.          for the first dataset and for the second     and     The points in the second set are clustered very close to a line.         For the first dataset, the total variance is while the second dataset has a total variance of .    In the first dataset, the first principal component represents of the variance compared to in the second dataset.    The points in the first dataset are scattered almost uniformly around the origin. In the second dataset, they are clustered very close to the line defined by the vector .     "
},
{
  "id": "exercise-270",
  "level": "2",
  "url": "sec-pca.html#exercise-270",
  "type": "Exercise",
  "number": "7.3.4.2",
  "title": "",
  "body": " Suppose that a dataset has mean and that its associated covariance matrix is .    What fraction of the variance is represented by the first two principal components?    If is one of the data points, find the coordinates when the demeaned point is projected into the plane defined by the first two principal components.    If a projected data point has coordinates , find an estimate for the original data point.               .     .         The eigenvalues are , , and so the fraction of variance represented by the first two principal components is     Suppose is this data point and the vector of means. If is the matrix whose columns are the first two principal components, then we find .    We find .     "
},
{
  "id": "exercise-271",
  "level": "2",
  "url": "sec-pca.html#exercise-271",
  "type": "Exercise",
  "number": "7.3.4.3",
  "title": "",
  "body": " Evaluating the following cell loads a demeaned data matrix A .    Find the principal components and and the variance in the direction of each principal component.    What is the total variance?    What can you conclude about this dataset?         The variances are and .         All of the data lies on a line         We find that and . The variances are and .    The total variance is .    All of the data lies on the line defined by .     "
},
{
  "id": "exercise-272",
  "level": "2",
  "url": "sec-pca.html#exercise-272",
  "type": "Exercise",
  "number": "7.3.4.4",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your thinking.   If the eigenvalues of the covariance matrix are , , and , then is the variance of the demeaned data points when projected on the third principal component .    Principal component analysis always allows us to construct a smaller dimensional representation of a dataset without losing any information.    If the eigenvalues of the covariance matrix are 56, 32, and 0, then the demeaned data points all lie on a line in .         True    False    False         True. The variance of the projected data points is .    False. If we project onto a subspace that retains less than all the variance, then we are losing information about the data in the directions orthogonal to the subspace.    False. They will lie on the plane spanned by the first two principal components.     "
},
{
  "id": "exercise-273",
  "level": "2",
  "url": "sec-pca.html#exercise-273",
  "type": "Exercise",
  "number": "7.3.4.5",
  "title": "",
  "body": " In , we looked at a dataset consisting of four measurements of 150 irises. These measurements are sepal length, sepal width, petal length, and petal width.   Find the first principal component and describe the meaning of its four components. Which component is most significant? What can you say about the relative importance of the four measurements?    When the dataset is plotted in the plane defined by and , the specimens from the species iris-setosa lie on the left side of the plot. What does this tell us about how iris-setosa differs from the other two species in the four measurements?    In general, which species is closest to the average iris ?         The petal length is the most signficant and the sepal width is the least.    Iris setosa has a below average petal length and appears overall smaller.    Iris versicolor         The first principal component is . If we move in the direction, each component describes how we move away from the mean of that measurement. The most important component is the third one, which corresponds to petal length, while the second component, corresponding to sepal width, is least signficant. Of the four measurements, there is the most variance in petal length.    To reach an Iris setosa sample, we multiply by a negative number. This says that the petal length, the most signficant component of will be below the mean for Iris setosa. The sepal length and petal width will also be below the mean. Basically, the Iris setosa would appear to be smaller overall.    Iris versicolor would appear to be closest to average since the points are closest to the origin in the - plane.     "
},
{
  "id": "exercise-274",
  "level": "2",
  "url": "sec-pca.html#exercise-274",
  "type": "Exercise",
  "number": "7.3.4.6",
  "title": "",
  "body": " This problem explores a dataset describing 333 penguins. There are three species, Adelie, Chinstrap, and Gentoo, as illustrated on the left of , as well as both male and female penguins in the dataset.       Artwork by @allison_horst    Evaluating the next cell will load and display the data. The meaning of the culmen length and width is contained in the illustration on the right of . This dataset is a bit different from others that we've looked at because the scale of the measurements is significantly different. For instance, the measurements for the body mass are roughly 100 times as large as those for the culmen length. For this reason, we will standardize the data by first demeaning it, as usual, and then rescaling each measurement by the reciprocal of its standard deviation. The result is stored in the matrix A .    Find the covariance matrix and its eigenvalues.    What fraction of the total variance is explained by the first two principal components?    Construct the matrix whose columns are the coordinates of the demeaned data points projected onto the first two principal components. The following cell will create the plot.     Examine the components of the first two principal component vectors. How does the body mass of Gentoo penguins compare to that of the other two species?    What seems to be generally true about the culmen measurements for a Chinstrap penguin compared to a Adelie?    You can plot just the males or females using the following cell. What seems to be generally true about the body mass measurements for a male Gentoo compared to a female Gentoo?          , , , .          where the columns of are the first two principal components.    Gentoo penguins have a larger body mass.    These measurements seem larger for the Chinstrap penguins.    The male Gentoo have a higher body mass.         The eigenvalues are , , , and .    The first two principal components retain of the total variance.     where the columns of are the first two principal components.    Gentoo penguins have a larger body mass because the fourth component, which corresponds to body mass, of is negative and the Gentoo penguins are represented by where is negative.    These measurements seem larger for the Chinstrap penguins because the first two components of are positive.    The male Gentoo lie further to the left, which means they have a higher body mass.     "
},
{
  "id": "sec-svd-intro",
  "level": "1",
  "url": "sec-svd-intro.html",
  "type": "Section",
  "number": "7.4",
  "title": "Singular Value Decompositions",
  "body": " Singular Value Decompositions   The Spectral Theorem has motivated the past few sections. In particular, we applied the fact that symmetric matrices can be orthogonally diagonalized to simplify quadratic forms, which enabled us to use principal component analysis to reduce the dimension of a dataset.  But what can we do with matrices that are not symmetric or even square? For instance, the following matrices are not diagonalizable, much less orthogonally so: In this section, we will develop a description of matrices called the singular value decomposition that is, in many ways, analogous to an orthogonal diagonalization. For example, we have seen that any symmetric matrix can be written in the form where is an orthogonal matrix and is diagonal. A singular value decomposition will have the form where and are orthogonal and is diagonal. Most notably, we will see that every matrix has a singular value decomposition whether it's symmetric or not.    Let's review orthogonal diagonalizations and quadratic forms as our understanding of singular value decompositions will rely on them.   Suppose that is any matrix. Explain why the matrix is symmetric.    Suppose that . Find the matrix and write out the quadratic form as a function of and .    What is the maximum value of and in which direction does it occur?     What is the minimum value of and in which direction does it occur?    What is the geometric relationship between the directions in which the maximum and minimum values occur?           .     leads to the quadratic form .    The maximum value of equals the largest eigenvalue of , which is . This maximum value occurs in the direction of the associated eigenvector .    The minimum value of equals the smallest eigenvalue of , which is . This minimum value occurs in the direction of the associated eigenvector .    These two directions are orthogonal to each other.         Finding singular value decompositions  We will begin by explaining what a singular value decomposition is and how we can find one for a given matrix .  Recall how the orthogonal diagonalization of a symmetric matrix is formed: if is symmetric, we write where the diagonal entries of are the eigenvalues of and the columns of are the associated eigenvectors. Moreover, the eigenvalues are related to the maximum and minimum values of the associated quadratic form among all unit vectors.  A general matrix, particularly a matrix that is not square, may not have eigenvalues and eigenvectors, but we can discover analogous features, called singular values and singular vectors , by studying a function somewhat similar to a quadratic form. More specifically, any matrix defines a function which measures the length of . For example, the diagonal matrix gives the function . The presence of the square root means that this function is not a quadratic form. We can, however, define the singular values and vectors by looking for the maximum and minimum of this function among all unit vectors .  While is not itself a quadratic form, it becomes one if we square it:  Gram matrix We call , the Gram matrix associated to and note that This is important in the next activity, which introduces singular values and singular vectors.    The following interactive figure will help us explore singular values and vectors geometrically before we begin a more algebraic approach.   Singular values, right singular vectors and left singular vectors    Select the matrix . As we vary the vector , we see the vector on the right in gray while the height of the blue bar to the right tells us .     The first singular value  is the maximum value of and an associated right singular vector  is a unit vector describing a direction in which this maximum occurs.  Use the diagram to find the first singular value and an associated right singular vector .    The second singular value is the minimum value of and an associated right singular vector is a unit vector describing a direction in which this minimum occurs.  Use the diagram to find the second singular value and an associated right singular vector .    Here's how we can find the right singular values and vectors without using the diagram. Remember that where is the Gram matrix associated to . Since is symmetric, it is orthogonally diagonalizable. Find and an orthogonal diagonalization of it. What is the maximum value of the quadratic form among all unit vectors and in which direction does it occur? What is the minimum value of and in which direction does it occur?    Because , the first singular value will be the square root of the maximum value of and the square root of the minimum. Verify that the singular values that you found from the diagram are the square roots of the maximum and minimum values of .    Verify that the right singular vectors and that you found from the diagram are the directions in which the maximum and minimum values occur.    Finally, we introduce the left singular vectors  and by requiring that and . Find the two left singular vectors.     Form the matrices and explain why .    Finally, explain why and verify that this relationship holds for this specific example.          The maximum value of is , which occurs at .    The minimum value of is , which occurs at .     where The maximum value of is therefore , which occurs in the direction . The minimum value of is , which occurs in the direction .    We see that and .    We also see that , the first right singular vector, agrees with the direction in which has its maximum value. The corresponding fact is true for .    We find that and .     .    Since is an orthogonal matrix, we have .           ,      ,     The maximum value of is , which occurs in the direction . The minimum value of is , which occurs in the direction .     and      agrees with the direction in which has its maximum value. The corresponding fact is true for .     and      .    Since is an orthogonal matrix, we have .       As this activity shows, the singular values of are the maximum and minimum values of among all unit vectors and the right singular vectors and are the directions in which they occur. The key to finding the singular values and vectors is to utilize the Gram matrix and its associated quadratic form . We will illustrate with some more examples.    We will find a singular value decomposition of the matrix . Notice that this matrix is not symmetric so it cannot be orthogonally diagonalized.  We begin by constructing the Gram matrix . Since is symmetric, it can be orthogonally diagonalized with   We now know that the maximum value of the quadratic form is 8, which occurs in the direction . Since , this tells us that the maximum value of , the first singular value, is and that this occurs in the direction of the first right singular vector .  In the same way, we also know that the second singular value with associated right singular vector .  The first left singular vector is defined by . Because , we have . Notice that is a unit vector because .  In the same way, the second left singular vector is defined by , which gives us .  We then construct   We now have because Because the right singular vectors, the columns of , are eigenvectors of the symmetric matrix , they form an orthonormal basis, which means that is orthogonal. Therefore, we have . This gives the singular value decomposition     To summarize, we find a singular value decomposition of a matrix in the following way:   Construct the Gram matrix and find an orthogonal diagonalization to obtain eigenvalues and an orthonormal basis of eigenvectors.    The singular values of are the squares roots of eigenvalues of ; that is, . By convention, the singular values are listed in decreasing order: . The right singular vectors are the associated eigenvectors of .    The left singular vectors are found by . Because , we know that will be a unit vector.  In fact, the left singular vectors will also form an orthonormal basis. To see this, suppose that the associated singular values are nonzero. We then have: since the right singular vectors are orthogonal.       Let's find a singular value decomposition for the symmetric matrix . The associated Gram matrix is which has an orthogonal diagonalization with This gives singular values and vectors and the singular value decomposition where   This example is special because is symmetric. With a little thought, it's possible to relate this singular value decomposition to an orthogonal diagonalization of using the fact that .      In this activity, we will construct the singular value decomposition of . Notice that this matrix is not square so there are no eigenvalues and eigenvectors associated to it.   Construct the Gram matrix and find an orthogonal diagonalization of it.     Identify the singular values of and the right singular vectors , , and . What is the dimension of these vectors? How many nonzero singular values are there?    Find the left singular vectors and using the fact that . What is the dimension of these vectors? What happens if you try to find a third left singular vector in this way?    As before, form the orthogonal matrices and from the left and right singular vectors. What are the shapes of and ? How do these shapes relate to the number of rows and columns of ?    Now form so that it has the same shape as : and verify that .     How can you use this singular value decomposition of to easily find a singular value decomposition of ?          Constructing the Gram matrix of gives the matrix which can be orthogonally diagaonalized with     This tells us that , , and . The three right singular vectors are the columns of . Since these vectors are 3-dimensional, it follows that the matrix will be .    We have showing that Notice that so it is not possible to find a vector in this way.  The left singular vectors are 2-dimensional so will be a matrix.    We have The matrix is since there are two rows in and is since there are three columns in     With , we see that .    If , then .           can be orthogonally diagaonalized with      , , and . The three right singular vectors are the columns of .     and     We have     With , we see that .     .         We will find a singular value decomposition of the matrix .  Finding an orthogonal diagonalization of gives which gives singular values , , and . The right singular vectors appear as the columns of so that .  We now find Notice that it's not possible to find a third left singular vector since . We therefore form the matrices which gives the singular value decomposition .  Notice that is a orthogonal matrix because has two rows, and is a orthogonal matrix because has three columns.    As we'll see in the next section, some additional work may be needed to construct the left singular vectors if more of the singular values are zero, but we won't worry about that now. For the time being, let's record our work in the following theorem.   The singular value decomposition  An matrix may be written as where is an orthogonal matrix, is an orthogonal matrix, and is an matrix whose entries are zero except for the singular values of which appear in decreasing order on the diagonal.   Notice that a singular value decomposition of gives us a singular value decomposition of . More specifically, if , then     If , then . In other words, and share the same singular values, and the left singular vectors of are the right singular vectors of and vice-versa.    As we said earlier, a singular value decomposition should be thought of a generalization of an orthogonal diagonalization. For instance, the Spectral Theorem tells us that a symmetric matrix can be written as . Many matrices, however, are not symmetric and so they are not orthogonally diagonalizable. However, every matrix has a singular value decomposition . The price of this generalization is that we usually have two sets of singular vectors that form the orthogonal matrices and whereas a symmetric matrix has a single set of eignevectors that form the orthogonal matrix .    The structure of singular value decompositions  Now that we have an understanding of what a singular value decomposition is and how to construct it, let's explore the ways in which a singular value decomposition reveals the underlying structure of the matrix. As we'll see, the matrices and in a singular value decomposition provide convenient bases for some important subspaces, such as the column and null spaces of the matrix. This observation will provide the key to some of our uses of these decompositions in the next section.    Let's suppose that a matrix has a singular value decomposition where    What is the shape of ; that is, how many rows and columns does have?    Suppose we write a three-dimensional vector as a linear combination of right singular vectors: We would like to find an expression for .  To begin, .  Now .  And finally, .  To summarize, we have .  What condition on , , and must be satisfied if is a solution to the equation ? Is there a unique solution or infinitely many?    Remembering that and are linearly independent, what condition on , , and must be satisfied if ?    How do the right singular vectors provide a basis for , the subspace of solutions to the equation ?    Remember that is in if the equation is consistent, which means that for some coefficients and . How do the left singular vectors provide an orthonormal basis for ?    Remember that is the dimension of the column space. What is and how do the number of nonzero singular values determine ?          The shape of is the same as so is ; that is, has rows and columns.    We have . This means that , , and could be anything. Since there is no condition on , there are infinitely many solutions.    We have , which says that , , and could be anything.    Any vector in satisfies and must have the form . Therefore, forms a basis for .    The vector is in only if is a linear combination of and . Therefore, and form a basis for .     , which is the number of nonzero singular values.           is      , , and there is no condition on      , , and there is no condition on      forms a basis for .     and form a basis for .     , which is the number of nonzero singular values.       This activity shows how a singular value decomposition of a matrix encodes important information about its null and column spaces. More specifically, the left and right singular vectors provide orthonormal bases for and . This is one of the reasons that singular value decompositions are so useful.    Suppose we have a singular value decomposition where . This means that has four rows and five columns just as does.  As in the activity, if , we have   If is in , then must have the form which says that is a linear combination of , , and . These three vectors therefore form a basis for . In fact, since they are columns in the orthogonal matrix , they form an orthonormal basis for .  Remembering that , we see that , which results from the three nonzero singular values. In general, the rank of a matrix equals the number of nonzero singular values, and form an orthonormal basis for .  Moreover, if satisfies , then which implies that , , and . Therefore, so and form an orthonormal basis for .  More generally, if is an matrix and if , the last right singular vectors form an orthonormal basis for .    Generally speaking, if the rank of an matrix is , then there are nonzero singular values and has the form The first columns of form an orthonormal basis for : and the last columns of form an orthonormal basis for :   Remember that says that and its transpose share the same singular values. Since the rank of a matrix equals its number of nonzero singular values, this means that , a fact that we cited back in .    For any matrix ,     If we have a singular value decomposition of an matrix , also tells us that the left singular vectors of are the right singular vectors of . Therefore, is the matrix whose columns are the right singular vectors of . This means that the last vectors form an orthonormal basis for . Therefore, the columns of provide orthonormal bases for and : This reflects the familiar fact that is the orthogonal complement of .   row space In the same way, is the matrix whose columns are the left singular vectors of , which means that the first vectors form an orthonormal basis for . Because the columns of are the rows of , this subspace is sometimes called the row space of and denoted . While we have yet to have an occasion to use , there are times when it is important to have an orthonormal basis for it, and a singular value decomposition provides just that. To summarize, the columns of provide orthonormal bases for and :   Considered altogether, the subspaces , , , and are called the four fundamental subspaces associated to . In addition to telling us the rank of a matrix, a singular value decomposition gives us orthonormal bases for all four fundamental subspaces.    Suppose is an matrix having a singular value decomposition . Then    is the number of nonzero singular values.    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .       When we previously outlined a procedure for finding a singular decomposition of an matrix , we found the left singular vectors using the expression . This produces left singular vectors , where . If , however, we still need to find the left singular vectors . tells us how to do that: because those vectors form an orthonormal basis for , we can find them by solving to obtain a basis for and applying the Gram-Schmidt algorithm.  We won't worry about this issue too much, however, as we will frequently use software to find singular value decompositions for us.    Reduced singular value decompositions  As we'll see in the next section, there are times when it is helpful to express a singular value decomposition in a slightly different form.    Suppose we have a singular value decomposition where    What is the shape of ? What is ?    Identify bases for and .    Explain why     Explain why     If , explain why where the columns of are an orthonormal basis for , is a square, diagonal, invertible matrix, and the columns of form an orthonormal basis for .           is a matrix and because there are two nonzero singular values.     and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where            is a matrix and      and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where        We call this a reduced singular value decomposition .   Reduced singular value decomposition   If is an matrix having rank , then where    is an matrix whose columns form an orthonormal basis for ,     is an diagonal, invertible matrix, and     is an matrix whose columns form an orthonormal basis for .         In , we found the singular value decomposition Since there are two nonzero singular values, so that the reduced singular value decomposition is       Summary  This section has explored singular value decompositions, how to find them, and how they organize important information about a matrix.   A singular value decomposition of a matrix is a factorization where . The matrix has the same shape as , and its only nonzero entries are the singular values of , which appear in decreasing order on the diagonal. The matrices and are orthogonal and contain the left and right singular vectors, respectively, as their columns.    To find a singular value decomposition of a matrix, we construct the Gram matrix , which is symmetric. The singular values of are the square roots of the eigenvalues of , and the right singular vectors are the associated eigenvectors of . The left singular vectors are determined from the relationship .    A singular value decomposition reveals fundamental information about a matrix. For instance, the number of nonzero singular values is the rank of the matrix. The first left singular vectors form an orthonormal basis for with the remaining left singular vectors forming an orthonormal basis of . The first right singular vectors form an orthonormal basis for while the remaining right singular vectors form an orthonormal basis of .    If is a rank matrix, we can write a reduced singular value decomposition as where the columns of form an orthonormal basis for , the columns of form an orthonormal basis for , and is an diagonal, invertible matrix.        Consider the matrix .    Find the Gram matrix and use it to find the singular values and right singular vectors of .    Find the left singular vectors.    Form the matrices , , and and verify that .    What is and what does this say about ?    Determine an orthonormal basis for .          , which has    and . The right singular vectors are the columns of .     and .          and .               , which has   This says that and . The right singular vectors are the columns of .     and .          because there are two nonzero singular values. Therefore, is 2-dimensional and so .            Find singular value decompositions for the following matrices:    .     .                                                                   Consider the matrix .   Find a singular value decomposition of and verify that it is also an orthogonal diagonalization of .    If is a symmetric, positive semidefinite matrix, explain why a singular value decomposition of is an orthogonal diagonalization of .              The singular values , the eigenvalues of and the right singular vectors are eigenvectors of .            Since , this is an orthogonal diagonalization.    In this case, . If are the eigenvalues of and the associated eigenvectors, then is an eigenvalue of with associated eigenvector . Therefore, and the right singular vectors are .  The left singular vectors are so the left singular vectors are the same as the right singular vectors.       Suppose that the matrix has the singular value decomposition    What are the dimensions of ?    What is ?    Find orthonormal bases for , , , and .    Find the orthogonal projection of onto .                   A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .     .          is          A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .    Form and find .       Consider the matrix .    Construct the Gram matrix and use it to find the singular values and right singular vectors , , and of . What are the matrices and in a singular value decomposition?    What is ?    Find as many left singular vectors as you can using the relationship .    Find an orthonormal basis for and use it to construct the matrix so that .    State an orthonormal basis for and an orthonormal basis for .          and          and         The third column of forms a basis for while the first two columns of form a basis for .          and          and     so that     The third column of forms a basis for while the first two columns of form a basis for .       Consider the matrix and notice that where is the matrix in .    Use your result from to find a singular value decomposition of .    What is ? Determine a basis for and .    Suppose that . Use the bases you found in the previous part of this exericse to write , where is in and is in .    Find the least-squares approximate solution to the equation .         From the previous problem, we have                .         From the previous problem, we have      . A basis for consists of the first two columns of and a basis for consists of the third column of .    If is the matrix consisting of the first two columns of , then . Then .    We solve the equation to find .       Suppose that is a square matrix with singular value decomposition .   If is invertible, find a singular value decomposition of .    What condition on the singular values must hold for to be invertible?    How are the singular values of and the singular values of related to one another?    How are the right and left singular vectors of related to the right and left singular vectors of ?              All the singular values are nonzero.    They are reciprocals of one another.    The left singular vectors of are the right singular vectors of and vice versa.              We need so all the singular values are nonzero. This means that the square matrix is invertible.    If are the singular values for , then are the singular values for .    The left singular vectors of are the right singular vectors of and vice versa.          If is an orthogonal matrix, remember that . Explain why .    If is a singular value decomposition of a square matrix , explain why is the product of the singular values of .    What does this say about the singular values of if is invertible?                   All the singular values are nonzero.          so .     , which equals the product of the singular values.    If is invertible, then , which says that all the singular values are nonzero.       If is a matrix and its Gram matrix, remember that    For a general matrix , explain why the eigenvalues of are nonnegative.    Given a symmetric matrix having an eigenvalue , explain why is an eigenvalue of .    If is symmetric, explain why the singular values of equal the absolute value of its eigenvalues: .         If is an eigenvector of with eigenvalue , then     If is symmetric, then .    If is an eigenvalue of , then is an eigenvalue of .         If is an eigenvector of with eigenvalue , then so .    If is symmetric, then . Then if , it follows that .    If is an eigenvalue of , then is an eigenvalue of . Then .       Determine whether the following statements are true or false and explain your reasoning.   If is a singular value decomposition of , then is an orthogonal diagonalization of its Gram matrix.    If is a singular value decomposition of a rank 2 matrix , then and form an orthonormal basis for the column space .    If is a symmetric matrix, then its set of singular values is the same as its set of eigenvalues.    If is a matrix and , then the columns of are linearly independent.    The Gram matrix is always orthogonally diagonalizable.         True    False    False    True    True         True. Remembering that is orthogonal, we have . Since is diagonal and is orthogonal, this is an orthogonal diagonalization.    False. They form a basis for .    False. The singular values are always nonnegative whereas the eigenvalues can take on any sign.    True. This would say that , which implies that the columns are linearly independent.    True. The Gram matrix is always symmetric and hence orthogonally diagonalizable.       Suppose that is a singular value decomposition of the matrix . If are the nonzero singular values, the general form of the matrix is    If you know that the columns of are linearly independent, what more can you say about the form of ?    If you know that the columns of span , what more can you say about the form of ?    If you know that the columns of are linearly independent and span , what more can you say about the form of ?         Every column of has a nonzero singular value.    Every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.         If the columns are linearly independent, then , which means that the rank equals the number of columns. Therefore, every column of has a nonzero singular value.    In the columns span , then , which says that the rank equals the number of rows. Therefore, every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.       "
},
{
  "id": "exploration-29",
  "level": "2",
  "url": "sec-svd-intro.html#exploration-29",
  "type": "Preview Activity",
  "number": "7.4.1",
  "title": "",
  "body": "  Let's review orthogonal diagonalizations and quadratic forms as our understanding of singular value decompositions will rely on them.   Suppose that is any matrix. Explain why the matrix is symmetric.    Suppose that . Find the matrix and write out the quadratic form as a function of and .    What is the maximum value of and in which direction does it occur?     What is the minimum value of and in which direction does it occur?    What is the geometric relationship between the directions in which the maximum and minimum values occur?           .     leads to the quadratic form .    The maximum value of equals the largest eigenvalue of , which is . This maximum value occurs in the direction of the associated eigenvector .    The minimum value of equals the smallest eigenvalue of , which is . This minimum value occurs in the direction of the associated eigenvector .    These two directions are orthogonal to each other.      "
},
{
  "id": "activity-101",
  "level": "2",
  "url": "sec-svd-intro.html#activity-101",
  "type": "Activity",
  "number": "7.4.2",
  "title": "",
  "body": "  The following interactive figure will help us explore singular values and vectors geometrically before we begin a more algebraic approach.   Singular values, right singular vectors and left singular vectors    Select the matrix . As we vary the vector , we see the vector on the right in gray while the height of the blue bar to the right tells us .     The first singular value  is the maximum value of and an associated right singular vector  is a unit vector describing a direction in which this maximum occurs.  Use the diagram to find the first singular value and an associated right singular vector .    The second singular value is the minimum value of and an associated right singular vector is a unit vector describing a direction in which this minimum occurs.  Use the diagram to find the second singular value and an associated right singular vector .    Here's how we can find the right singular values and vectors without using the diagram. Remember that where is the Gram matrix associated to . Since is symmetric, it is orthogonally diagonalizable. Find and an orthogonal diagonalization of it. What is the maximum value of the quadratic form among all unit vectors and in which direction does it occur? What is the minimum value of and in which direction does it occur?    Because , the first singular value will be the square root of the maximum value of and the square root of the minimum. Verify that the singular values that you found from the diagram are the square roots of the maximum and minimum values of .    Verify that the right singular vectors and that you found from the diagram are the directions in which the maximum and minimum values occur.    Finally, we introduce the left singular vectors  and by requiring that and . Find the two left singular vectors.     Form the matrices and explain why .    Finally, explain why and verify that this relationship holds for this specific example.          The maximum value of is , which occurs at .    The minimum value of is , which occurs at .     where The maximum value of is therefore , which occurs in the direction . The minimum value of is , which occurs in the direction .    We see that and .    We also see that , the first right singular vector, agrees with the direction in which has its maximum value. The corresponding fact is true for .    We find that and .     .    Since is an orthogonal matrix, we have .           ,      ,     The maximum value of is , which occurs in the direction . The minimum value of is , which occurs in the direction .     and      agrees with the direction in which has its maximum value. The corresponding fact is true for .     and      .    Since is an orthogonal matrix, we have .      "
},
{
  "id": "example-76",
  "level": "2",
  "url": "sec-svd-intro.html#example-76",
  "type": "Example",
  "number": "7.4.2",
  "title": "",
  "body": "  We will find a singular value decomposition of the matrix . Notice that this matrix is not symmetric so it cannot be orthogonally diagonalized.  We begin by constructing the Gram matrix . Since is symmetric, it can be orthogonally diagonalized with   We now know that the maximum value of the quadratic form is 8, which occurs in the direction . Since , this tells us that the maximum value of , the first singular value, is and that this occurs in the direction of the first right singular vector .  In the same way, we also know that the second singular value with associated right singular vector .  The first left singular vector is defined by . Because , we have . Notice that is a unit vector because .  In the same way, the second left singular vector is defined by , which gives us .  We then construct   We now have because Because the right singular vectors, the columns of , are eigenvectors of the symmetric matrix , they form an orthonormal basis, which means that is orthogonal. Therefore, we have . This gives the singular value decomposition    "
},
{
  "id": "example-77",
  "level": "2",
  "url": "sec-svd-intro.html#example-77",
  "type": "Example",
  "number": "7.4.3",
  "title": "",
  "body": "  Let's find a singular value decomposition for the symmetric matrix . The associated Gram matrix is which has an orthogonal diagonalization with This gives singular values and vectors and the singular value decomposition where   This example is special because is symmetric. With a little thought, it's possible to relate this singular value decomposition to an orthogonal diagonalization of using the fact that .   "
},
{
  "id": "activity-102",
  "level": "2",
  "url": "sec-svd-intro.html#activity-102",
  "type": "Activity",
  "number": "7.4.3",
  "title": "",
  "body": "  In this activity, we will construct the singular value decomposition of . Notice that this matrix is not square so there are no eigenvalues and eigenvectors associated to it.   Construct the Gram matrix and find an orthogonal diagonalization of it.     Identify the singular values of and the right singular vectors , , and . What is the dimension of these vectors? How many nonzero singular values are there?    Find the left singular vectors and using the fact that . What is the dimension of these vectors? What happens if you try to find a third left singular vector in this way?    As before, form the orthogonal matrices and from the left and right singular vectors. What are the shapes of and ? How do these shapes relate to the number of rows and columns of ?    Now form so that it has the same shape as : and verify that .     How can you use this singular value decomposition of to easily find a singular value decomposition of ?          Constructing the Gram matrix of gives the matrix which can be orthogonally diagaonalized with     This tells us that , , and . The three right singular vectors are the columns of . Since these vectors are 3-dimensional, it follows that the matrix will be .    We have showing that Notice that so it is not possible to find a vector in this way.  The left singular vectors are 2-dimensional so will be a matrix.    We have The matrix is since there are two rows in and is since there are three columns in     With , we see that .    If , then .           can be orthogonally diagaonalized with      , , and . The three right singular vectors are the columns of .     and     We have     With , we see that .     .      "
},
{
  "id": "example-svd-nonsquare",
  "level": "2",
  "url": "sec-svd-intro.html#example-svd-nonsquare",
  "type": "Example",
  "number": "7.4.4",
  "title": "",
  "body": "  We will find a singular value decomposition of the matrix .  Finding an orthogonal diagonalization of gives which gives singular values , , and . The right singular vectors appear as the columns of so that .  We now find Notice that it's not possible to find a third left singular vector since . We therefore form the matrices which gives the singular value decomposition .  Notice that is a orthogonal matrix because has two rows, and is a orthogonal matrix because has three columns.   "
},
{
  "id": "theorem-svd",
  "level": "2",
  "url": "sec-svd-intro.html#theorem-svd",
  "type": "Theorem",
  "number": "7.4.5",
  "title": "The singular value decomposition.",
  "body": " The singular value decomposition  An matrix may be written as where is an orthogonal matrix, is an orthogonal matrix, and is an matrix whose entries are zero except for the singular values of which appear in decreasing order on the diagonal.  "
},
{
  "id": "prop-svd-transpose",
  "level": "2",
  "url": "sec-svd-intro.html#prop-svd-transpose",
  "type": "Proposition",
  "number": "7.4.6",
  "title": "",
  "body": "  If , then . In other words, and share the same singular values, and the left singular vectors of are the right singular vectors of and vice-versa.   "
},
{
  "id": "activity-103",
  "level": "2",
  "url": "sec-svd-intro.html#activity-103",
  "type": "Activity",
  "number": "7.4.4",
  "title": "",
  "body": "  Let's suppose that a matrix has a singular value decomposition where    What is the shape of ; that is, how many rows and columns does have?    Suppose we write a three-dimensional vector as a linear combination of right singular vectors: We would like to find an expression for .  To begin, .  Now .  And finally, .  To summarize, we have .  What condition on , , and must be satisfied if is a solution to the equation ? Is there a unique solution or infinitely many?    Remembering that and are linearly independent, what condition on , , and must be satisfied if ?    How do the right singular vectors provide a basis for , the subspace of solutions to the equation ?    Remember that is in if the equation is consistent, which means that for some coefficients and . How do the left singular vectors provide an orthonormal basis for ?    Remember that is the dimension of the column space. What is and how do the number of nonzero singular values determine ?          The shape of is the same as so is ; that is, has rows and columns.    We have . This means that , , and could be anything. Since there is no condition on , there are infinitely many solutions.    We have , which says that , , and could be anything.    Any vector in satisfies and must have the form . Therefore, forms a basis for .    The vector is in only if is a linear combination of and . Therefore, and form a basis for .     , which is the number of nonzero singular values.           is      , , and there is no condition on      , , and there is no condition on      forms a basis for .     and form a basis for .     , which is the number of nonzero singular values.      "
},
{
  "id": "example-79",
  "level": "2",
  "url": "sec-svd-intro.html#example-79",
  "type": "Example",
  "number": "7.4.7",
  "title": "",
  "body": "  Suppose we have a singular value decomposition where . This means that has four rows and five columns just as does.  As in the activity, if , we have   If is in , then must have the form which says that is a linear combination of , , and . These three vectors therefore form a basis for . In fact, since they are columns in the orthogonal matrix , they form an orthonormal basis for .  Remembering that , we see that , which results from the three nonzero singular values. In general, the rank of a matrix equals the number of nonzero singular values, and form an orthonormal basis for .  Moreover, if satisfies , then which implies that , , and . Therefore, so and form an orthonormal basis for .  More generally, if is an matrix and if , the last right singular vectors form an orthonormal basis for .   "
},
{
  "id": "prop-rank-transpose",
  "level": "2",
  "url": "sec-svd-intro.html#prop-rank-transpose",
  "type": "Proposition",
  "number": "7.4.8",
  "title": "",
  "body": "  For any matrix ,    "
},
{
  "id": "thm-four-subspaces",
  "level": "2",
  "url": "sec-svd-intro.html#thm-four-subspaces",
  "type": "Theorem",
  "number": "7.4.9",
  "title": "",
  "body": "  Suppose is an matrix having a singular value decomposition . Then    is the number of nonzero singular values.    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .    The columns form an orthonormal basis for .      "
},
{
  "id": "activity-104",
  "level": "2",
  "url": "sec-svd-intro.html#activity-104",
  "type": "Activity",
  "number": "7.4.5",
  "title": "",
  "body": "  Suppose we have a singular value decomposition where    What is the shape of ? What is ?    Identify bases for and .    Explain why     Explain why     If , explain why where the columns of are an orthonormal basis for , is a square, diagonal, invertible matrix, and the columns of form an orthonormal basis for .           is a matrix and because there are two nonzero singular values.     and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where            is a matrix and      and form an orthonormal basis for , and and form an orthonormal basis for .    Notice that     Notice that     Put together the previous parts to see that where       "
},
{
  "id": "prop-reduced-svd",
  "level": "2",
  "url": "sec-svd-intro.html#prop-reduced-svd",
  "type": "Proposition",
  "number": "7.4.10",
  "title": "Reduced singular value decomposition.",
  "body": " Reduced singular value decomposition   If is an matrix having rank , then where    is an matrix whose columns form an orthonormal basis for ,     is an diagonal, invertible matrix, and     is an matrix whose columns form an orthonormal basis for .      "
},
{
  "id": "example-80",
  "level": "2",
  "url": "sec-svd-intro.html#example-80",
  "type": "Example",
  "number": "7.4.11",
  "title": "",
  "body": "  In , we found the singular value decomposition Since there are two nonzero singular values, so that the reduced singular value decomposition is    "
},
{
  "id": "ex-7-4-1",
  "level": "2",
  "url": "sec-svd-intro.html#ex-7-4-1",
  "type": "Exercise",
  "number": "7.4.5.1",
  "title": "",
  "body": " Consider the matrix .    Find the Gram matrix and use it to find the singular values and right singular vectors of .    Find the left singular vectors.    Form the matrices , , and and verify that .    What is and what does this say about ?    Determine an orthonormal basis for .          , which has    and . The right singular vectors are the columns of .     and .          and .               , which has   This says that and . The right singular vectors are the columns of .     and .          because there are two nonzero singular values. Therefore, is 2-dimensional and so .          "
},
{
  "id": "exercise-276",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-276",
  "type": "Exercise",
  "number": "7.4.5.2",
  "title": "",
  "body": " Find singular value decompositions for the following matrices:    .     .                                                                 "
},
{
  "id": "exercise-277",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-277",
  "type": "Exercise",
  "number": "7.4.5.3",
  "title": "",
  "body": " Consider the matrix .   Find a singular value decomposition of and verify that it is also an orthogonal diagonalization of .    If is a symmetric, positive semidefinite matrix, explain why a singular value decomposition of is an orthogonal diagonalization of .              The singular values , the eigenvalues of and the right singular vectors are eigenvectors of .            Since , this is an orthogonal diagonalization.    In this case, . If are the eigenvalues of and the associated eigenvectors, then is an eigenvalue of with associated eigenvector . Therefore, and the right singular vectors are .  The left singular vectors are so the left singular vectors are the same as the right singular vectors.     "
},
{
  "id": "exercise-278",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-278",
  "type": "Exercise",
  "number": "7.4.5.4",
  "title": "",
  "body": " Suppose that the matrix has the singular value decomposition    What are the dimensions of ?    What is ?    Find orthonormal bases for , , , and .    Find the orthogonal projection of onto .                   A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .     .          is          A basis for is formed by the first two columns of . A basis for is formed by the last two columns of . A basis for is formed by the first two columns of . A basis for is formed by the last column of .    Form and find .     "
},
{
  "id": "exercise-279",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-279",
  "type": "Exercise",
  "number": "7.4.5.5",
  "title": "",
  "body": " Consider the matrix .    Construct the Gram matrix and use it to find the singular values and right singular vectors , , and of . What are the matrices and in a singular value decomposition?    What is ?    Find as many left singular vectors as you can using the relationship .    Find an orthonormal basis for and use it to construct the matrix so that .    State an orthonormal basis for and an orthonormal basis for .          and          and         The third column of forms a basis for while the first two columns of form a basis for .          and          and     so that     The third column of forms a basis for while the first two columns of form a basis for .     "
},
{
  "id": "exercise-280",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-280",
  "type": "Exercise",
  "number": "7.4.5.6",
  "title": "",
  "body": " Consider the matrix and notice that where is the matrix in .    Use your result from to find a singular value decomposition of .    What is ? Determine a basis for and .    Suppose that . Use the bases you found in the previous part of this exericse to write , where is in and is in .    Find the least-squares approximate solution to the equation .         From the previous problem, we have                .         From the previous problem, we have      . A basis for consists of the first two columns of and a basis for consists of the third column of .    If is the matrix consisting of the first two columns of , then . Then .    We solve the equation to find .     "
},
{
  "id": "exercise-281",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-281",
  "type": "Exercise",
  "number": "7.4.5.7",
  "title": "",
  "body": " Suppose that is a square matrix with singular value decomposition .   If is invertible, find a singular value decomposition of .    What condition on the singular values must hold for to be invertible?    How are the singular values of and the singular values of related to one another?    How are the right and left singular vectors of related to the right and left singular vectors of ?              All the singular values are nonzero.    They are reciprocals of one another.    The left singular vectors of are the right singular vectors of and vice versa.              We need so all the singular values are nonzero. This means that the square matrix is invertible.    If are the singular values for , then are the singular values for .    The left singular vectors of are the right singular vectors of and vice versa.     "
},
{
  "id": "exercise-282",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-282",
  "type": "Exercise",
  "number": "7.4.5.8",
  "title": "",
  "body": "    If is an orthogonal matrix, remember that . Explain why .    If is a singular value decomposition of a square matrix , explain why is the product of the singular values of .    What does this say about the singular values of if is invertible?                   All the singular values are nonzero.          so .     , which equals the product of the singular values.    If is invertible, then , which says that all the singular values are nonzero.     "
},
{
  "id": "exercise-283",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-283",
  "type": "Exercise",
  "number": "7.4.5.9",
  "title": "",
  "body": " If is a matrix and its Gram matrix, remember that    For a general matrix , explain why the eigenvalues of are nonnegative.    Given a symmetric matrix having an eigenvalue , explain why is an eigenvalue of .    If is symmetric, explain why the singular values of equal the absolute value of its eigenvalues: .         If is an eigenvector of with eigenvalue , then     If is symmetric, then .    If is an eigenvalue of , then is an eigenvalue of .         If is an eigenvector of with eigenvalue , then so .    If is symmetric, then . Then if , it follows that .    If is an eigenvalue of , then is an eigenvalue of . Then .     "
},
{
  "id": "exercise-284",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-284",
  "type": "Exercise",
  "number": "7.4.5.10",
  "title": "",
  "body": " Determine whether the following statements are true or false and explain your reasoning.   If is a singular value decomposition of , then is an orthogonal diagonalization of its Gram matrix.    If is a singular value decomposition of a rank 2 matrix , then and form an orthonormal basis for the column space .    If is a symmetric matrix, then its set of singular values is the same as its set of eigenvalues.    If is a matrix and , then the columns of are linearly independent.    The Gram matrix is always orthogonally diagonalizable.         True    False    False    True    True         True. Remembering that is orthogonal, we have . Since is diagonal and is orthogonal, this is an orthogonal diagonalization.    False. They form a basis for .    False. The singular values are always nonnegative whereas the eigenvalues can take on any sign.    True. This would say that , which implies that the columns are linearly independent.    True. The Gram matrix is always symmetric and hence orthogonally diagonalizable.     "
},
{
  "id": "exercise-285",
  "level": "2",
  "url": "sec-svd-intro.html#exercise-285",
  "type": "Exercise",
  "number": "7.4.5.11",
  "title": "",
  "body": " Suppose that is a singular value decomposition of the matrix . If are the nonzero singular values, the general form of the matrix is    If you know that the columns of are linearly independent, what more can you say about the form of ?    If you know that the columns of span , what more can you say about the form of ?    If you know that the columns of are linearly independent and span , what more can you say about the form of ?         Every column of has a nonzero singular value.    Every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.         If the columns are linearly independent, then , which means that the rank equals the number of columns. Therefore, every column of has a nonzero singular value.    In the columns span , then , which says that the rank equals the number of rows. Therefore, every row of has a nonzero singular value.    Then is a square matrix and every singular value is nonzero.     "
},
{
  "id": "sec-svd-uses",
  "level": "1",
  "url": "sec-svd-uses.html",
  "type": "Section",
  "number": "7.5",
  "title": "Using Singular Value Decompositions",
  "body": " Using Singular Value Decompositions   We've now seen what singular value decompositions are, how to construct them, and how they provide important information about a matrix such as orthonormal bases for the four fundamental subspaces. This puts us in a good position to begin using singular value decompositions to solve a wide variety of problems.  Given the fact that singular value decompositions so immediately convey fundamental data about a matrix, it seems natural that some of our previous work can be reinterpreted in terms of singular value decompositions. Therefore, we'll take some time in this section to revisit some familiar issues, such as least-squares problems and principal component analysis, while also looking at some new applications.    Suppose that where vectors form the columns of , and vectors form the columns of .   What are the shapes of the matrices , , and ?    What is the rank of ?    Describe how to find an orthonormal basis for .    Describe how to find an orthonormal basis for .    If the columns of form an orthonormal basis for , what is ?    How would you form a matrix that projects vectors orthogonally onto ?           is , is , and is .     since there are three nonzero singular values.    The first three columns, , , and , of form an orthonormal basis for .    The last column of is a basis for .     since each entry is a dot product of two vectors in an orthonormal set.     projects vectors orthogonally onto .         Least-squares problems  Least-squares problems, which we explored in , arise when we are confronted with an inconsistent linear system . Since there is no solution to the system, we instead find the vector minimizing the distance between and . That is, we find the vector , the least-squares approximate solution, by solving where is the orthogonal projection of onto the column space of .  If we have a singular value decomposition , then the number of nonzero singular values tells us the rank of , and the first columns of form an orthonormal basis for . This basis may be used to project vectors onto and hence to solve least-squares problems.  Before exploring this connection further, we will introduce Sage as a tool for automating the construction of singular value decompositions. One new feature is that we need to declare our matrix to consist of floating point entries. We do this by including RDF inside the matrix definition, as illustrated in the following cell.     Consider the equation where    Find a singular value decomposition for using the Sage cell below. What are singular values of ?     What is , the rank of ? How can we identify an orthonormal basis for ?    Form the reduced singular value decomposition by constructing: the matrix , consisting of the first columns of ; the matrix , consisting of the first columns of ; and , a square diagonal matrix. Verify that .  You may find it convenient to remember that if B is a matrix defined in Sage, then B.matrix_from_columns( list ) and B.matrix_from_rows( list ) can be used to extract columns or rows from B . For instance, B.matrix_from_rows([0,1,2]) provides a matrix formed from the first three rows of B .     How does the reduced singular value decomposition provide a matrix whose columns are an orthonormal basis for ?    Explain why a least-squares approximate solution satisfies     What is the product and why does it have this form?    Explain why is the least-squares approximate solution, and use this expression to find .           The singular values are and .    There are two nonzero singular values so . The first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .    Since , we obtain the equation .     since this product computes the dot products of the columns.    We have the equation , which gives            and .     and the first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .                      This activity demonstrates the power of a singular value decomposition to find a least-squares approximate solution for an equation . Because it immediately provides an orthonormal basis for , something that we've had to construct using the Gram-Schmidt process in the past, we can easily project onto , which results in a simple expression for .    If is a reduced singular value decomposition of , then a least-squares approximate solution to is given by     If the columns of are linearly independent, then the equation has only one solution so there is a unique least-squares approximate solution . Otherwise, the expression in produces the solution to having the shortest length.   Moore-Penrose psuedoinverse The matrix is known as the Moore-Penrose psuedoinverse of . When is invertible, .    Rank approximations  If we have a singular value decomposition for a matrix , we can form a sequence of matrices that approximate with increasing accuracy. This may feel familiar to calculus students who have seen the way in which a function can be approximated by a linear function, a quadratic function, and so forth with increasing accuracy.  We'll begin with a singular value decomposition of a rank matrix so that . To create the approximating matrix , we keep the first singular values and set the others to zero. For instance, if , we can form matrices and define and . Because has nonzero singular values, we know that . In fact, there is a sense in which is the closest matrix to among all rank matrices.    Let's consider a matrix where Evaluating the following cell will create the matrices U , V , and Sigma . Notice how the diagonal_matrix command provides a convenient way to form the diagonal matrix .    Form the matrix . What is ?     Now form the approximating matrix . What is ?     Find the error in the approximation by finding .    Now find and the error . What is ?     Find and the error . What is ?    What would happen if we were to compute ?    What do you notice about the error as increases?          We find that is the rank matrix: .    Because it has one nonzero singular value, has rank and has the form: .          is the rank matrix and .     is the rank matrix with .     since is a rank matrix.    As increases, the entries in the get closer to . This means that our approximations are improving.           .     .          and .     and .         The entries get closer to .       In this activity, the approximating matrix has rank because its singular value decomposition has nonzero singular values. We then saw how the difference between and the approximations decreases as increases, which means that the sequence forms better approximations as increases.  Another way to represent is with a reduced singular value decomposition so that where Notice that the rank matrix then has the form and that we can similarly write:   Given two vectors and , the matrix is called the outer product of and . (The dot product is sometimes called the inner product .) An outer product will always be a rank matrix so we see above how is obtained by adding together rank matrices, each of which gets us one step closer to the original matrix .    Principal component analysis  In , we explored principal component analysis as a technique to reduce the dimension of a dataset. In particular, we constructed the covariance matrix from a demeaned data matrix and saw that the eigenvalues and eigenvectors of tell us about the variance of the dataset in different directions. We referred to the eigenvectors of as principal components and found that projecting the data onto a subspace defined by the first few principal components frequently gave us a way to visualize the dataset. As we added more principal components, we retained more information about the original dataset. This feels similar to the rank approximations we have just seen so let's explore the connection.  Suppose that we have a dataset with points, that represents the demeaned data matrix, that is a singular value decomposition, and that the singular values are are denoted as . It follows that the covariance matrix Notice that is a diagonal matrix whose diagonal entries are . Therefore, it follows that is an orthogonal diagonalization of showing that   the principal components of the dataset, which are the eigenvectors of , are given by the columns of . In other words, the left singular vectors of are the principal components of the dataset.    the variance in the direction of a principal component is the associated eigenvalue of and therefore        Let's revisit the iris dataset that we studied in . Remember that there are four measurements given for each of 150 irises and that each iris belongs to one of three species.  Evaluating the following cell will load the dataset and define the demeaned data matrix whose shape is .    Find the singular values of using the command A.singular_values() and use them to determine the variance in the direction of each of the four principal components. What is the fraction of variance retained by the first two principal components?     We will now write the matrix so that . Suppose that a demeaned data point, say, the 100th column of , is written as a linear combination of principal components: Explain why , the vector of coordinates of in the basis of principal components, appears as 100th column of .    Suppose that we now project this demeaned data point orthogonally onto the subspace spanned by the first two principal components and . What are the coordinates of the projected point in this basis and how can we find them in the matrix ?    Alternatively, consider the approximation of the demeaned data matrix . Explain why the 100th column of represents the projection of onto the two-dimensional subspace spanned by the first two principal components, and . Then explain why the coefficients in that projection, , form the two-dimensional vector that is the 100th column of .    Now we've seen that the columns of form the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by and . In the cell below, find a singular value decomposition of and use it to form the matrix Gamma2 . When you evaluate this cell, you will see a plot of the projected demeaned data plots, similar to the one we created in .           The singular values are , , and . Since , the variances are , , , and . The fraction of the total variance represented by the first two principal components is .    If is the column of , then is the column of .    The projected data point is since is orthogonal to the subspace spanned by the first two principal components. Therefore, the coordinates of the projected data point are the first two components of the corresponding column of . The coordinates of all the projected data points are given by the first two rows of .    Once again, suppose that is a column of and that . Then is the corresponding column of .    We can construct or just pull out the first two rows of .          The fraction of the total variance represented by the first two principal components is .    If , then is the corresponding column of .    We just need the first two components of the corresponding column of .     is the corresponding column of .    We can construct or just pull out the first two rows of .       In our first encounter with principal component analysis, we began with a demeaned data matrix , formed the covariance matrix , and used the eigenvalues and eigenvectors of to project the demeaned data onto a smaller dimensional subspace. In this section, we have seen that a singular value decomposition of provides a more direct route: the left singular vectors of form the principal components and the approximating matrix represents the data points projected onto the subspace spanned by the first principal components. The coordinates of a projected demeaned data point are given by the columns of .    Image compressing and denoising  In addition to principal component analysis, the approximations of a matrix obtained from a singular value decomposition can be used in image processing. Remember that we studied the JPEG compression algorithm, whose foundation is the change of basis defined by the Discrete Cosine Transform, in . We will now see how a singular value decomposition provides another tool for both compressing images and removing noise in them.    Evaluating the following cell loads some data that we'll use in this activity. To begin, it defines and displays a matrix .    If we interpret 0 as black and 1 as white, this matrix represents an image as shown below. We will explore how the singular value decomposition helps us to compress this image.   By inspecting the image represented by , identify a basis for and determine .    The following cell plots the singular values of . Explain how this plot verifies that the rank is what you found in the previous part.     There is a command approximate(A, k) that creates the approximation . Use the cell below to define and look at the images represented by the first few approximations. What is the smallest value of for which ?     Now we can see how the singular value decomposition allows us to compress images. Since this is a matrix, we need numbers to represent the image. However, we can also reconstruct the image using a small number of singular values and vectors: What are the dimensions of the singular vectors and ? Between the singular vectors and singular values, how many numbers do we need to reconstruct for the smallest for which ? This is the compressed size of the image.    The compression ratio is the ratio of the uncompressed size to the compressed size. What compression ratio does this represent?       Next we'll explore an example based on a photograph.   Consider the following image consisting of an array of pixels stored in the matrix .   Plot the singular values of .     Use the cell below to study the approximations for . Notice how the approximating image more closely approximates the original image as increases.  What is the compression ratio when ? What is the compression ratio when ? Notice how a higher compression ratio leads to a lower quality reconstruction of the image.       A second, related application of the singular value decomposition to image processing is called denoising . For example, consider the image represented by the matrix below. This image is similar to the image of the letter \"O\" we first studied in this activity, but there are splotchy regions in the background that result, perhaps, from scanning the image. We think of the splotchy regions as noise, and our goal is to improve the quality of the image by reducing the noise.   Plot the singular values below. How are the singular values of this matrix similar to those represented by the clean image that we considered earlier and how are they different?     There is a natural point where the singular values dramatically decrease so it makes sense to think of the noise as being formed by the small singular values. To denoise the image, we will therefore replace by its approximation , where is the point at which the singular values drop off. This has the effect of setting the small singular values to zero and hence eliminating the noise. Choose an appropriate value of below and notice that the new image appears to be somewhat cleaned up as a result of removing the noise.                  because there are three distinct columns represented by the first, third, and sixth columns.    There are three nonzero singular values so as we suspected.    The smallest value is since .    The left singular vectors are -dimensional and the right singular vectors are -dimensional. If we keep three singular values, left singular vectors, and right singular vectors, we have .    The compression ratio is so the data is compressed by a factor of .          The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation since that's the place where the singular values become almost .                                    The compression ratio is           The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation          Several examples illustrating how the singular value decomposition compresses images are available at this page from Tim Baumann.     Analyzing Supreme Court cases  As we've seen, a singular value decomposition concentrates the most important features of a matrix into the first singular values and singular vectors. We will now use this observation to extract meaning from a large dataset giving the voting records of Supreme Court justices. A similar analysis appears in the paper A pattern analysis of the second Rehnquist U.S. Supreme Court by Lawrence Sirovich.  The makeup of the Supreme Court was unusually stable during a period from 1994-2005 when it was led by Chief Justice William Rehnquist. This is sometimes called the second Rehnquist court . The justices during this period were:  William Rehnquist  Antonin Scalia  Clarence Thomas  Anthony Kennedy  Sandra Day O'Connor  John Paul Stevens  David Souter  Ruth Bader Ginsburg  Stephen Breyer    During this time, there were 911 cases in which all nine judges voted. We would like to understand patterns in their voting.    Evaluating the following cell loads and displays a dataset describing the votes of each justice in these 911 cases. More specifically, an entry of +1 means that the justice represented by the row voted with the majority in the case represented by the column. An entry of -1 means that justice was in the minority. This information is also stored in the matrix . The justices are listed, very roughly, in order from more conservative to more progressive.  In this activity, it will be helpful to visualize the entries in various matrices and vectors. The next cell displays the first 50 columns of the matrix with white representing an entry of +1, red representing -1, and black representing 0.    Plot the singular values of below. Describe the significance of this plot, including the relative contributions from the singular values as increases.     Form the singular value decomposition and the matrix of coefficients so that .     We will now study a particular case, the second case which appears as the column of indexed by 1 . There is a command display_column(A, k) that provides a visual display of the column of a matrix . Describe the justices' votes in the second case.     Also, display the first left singular vector , the column of indexed by , and the column of holding the coefficients that express the second case as a linear combination of left singular vectors. What does this tell us about how the second case is constructed as a linear combination of left singular vectors? What is the significance of the first left singular vector ?    Let's now study the case, which is represented by the column of indexed by 47 . Describe the voting pattern in this case.     Display the second left singular vector and the vector of coefficients that express the case as a linear combination of left singular vectors. Describe how this case is constructed as a linear combination of singular vectors. What is the significance of the second left singular vector ?    The data in describes the number of cases decided by each possible vote count.  Number of cases by vote count    Vote count  # of cases    9-0  405    8-1  89    7-2  111    6-3  118    5-4  188    How do the singular vectors and reflect this data? Would you characterize the court as leaning toward the conservatives or progressives? Use these singular vectors to explain your response.    Cases decided by a 5-4 vote are often the most impactful as they represent a sharp divide among the justices and, often, society at large. For that reason, we will now focus on the 5-4 decisions. Evaluating the next cell forms the matrix consisting of 5-4 decisions. Form the singular value decomposition of along with the matrix of coefficients so that and display the first left singular vector . Study how the case, indexed by 6 , is constructed as a linear combination of left singular vectors. What does this singular vector tell us about the make up of the court and whether it leans towards the conservatives or progressives?    Display the second left singular vector and study how the case, indexed by 5 , is constructed as a linear combination of left singular vectors. What does tell us about the relative importance of the justices' voting records?    By a swing vote , we mean a justice who is less inclined to vote with a particular bloc of justices but instead swings from one bloc to another with the potential to sway close decisions. What do the singular vectors and tell us about the presence of voting blocs on the court and the presence of a swing vote? Which justice represents the swing vote?          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.    The unanimous decision is essentially represented as so represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.    This 5-4 decision is essentially represented as , the second most important left singular vector.    We see that the most decisions are unanimous, which is why represents unanimous decisions. The second most frequently occurring decisions is a 5-4 decision, which is why represents a 5-4 decision that leans to the conservative justices.    The first singular vector represents a case where the five conservative justices are voting together. From this we conclude that the court leans toward the conservatives.    The second left singular vector essentially records the vote of Sandra Day O'Connor and shows how her vote has the power to swing a 5-4 decision from a conservative majority to a progressive majority.    Sandra Day O'Connor would be the swing vote.          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.     represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.     represents a 5-4 decision.    The most frequently occurring decisions are unanimous and the second most frequently occurring are 5-4.     represents a case where the five conservative justices are voting together.    The second left singular vector essentially records the vote of Sandra Day O'Connor.    Sandra Day O'Connor         Summary  This section has demonstrated some uses of the singular value decomposition. Because the singular values appear in decreasing order, the decomposition has the effect of concentrating the most important features of the matrix into the first singular values and singular vectors.   Because the first left singular vectors form an orthonormal basis for , a singular value decomposition provides a convenient way to project vectors onto and therefore to solve least-squares problems.    A singular value decomposition of a rank matrix leads to a series of approximations of where In each case, is the rank matrix that is closest to .    If is a demeaned data matrix, the left singular vectors give the principal components of , and the variance in the direction of a principal component can be simply expressed in terms of the corresponding singular value.    The singular value decomposition has many applications. In this section, we looked at how the decomposition is used in image processing through the techniques of compression and denoising.    Because the first few left singular vectors contain the most important features of a matrix, we can use a singular value decomposition to extract meaning from a large dataset as we did when analyzing the voting patterns of the second Rehnquist court.        Suppose that     Find the singular values of . What is ?    Find the sequence of matrices , , , and where is the rank approximation of .          .     , ,\\   ,          The singular values are , , , and so we have .     , ,   ,        Suppose we would like to find the best quadratic function fitting the points     Set up a linear system describing the coefficients .    Find the singular value decomposition of .    Use the singular value decomposition to find the least-squares approximate solution .         The linear system is      is a matrix, is , and is               The linear system is      is a matrix, is , and is     The rank of is 3 so we'll form the reduced singular value decomposition . Then        Remember that the outer product of two vectors and is the matrix .   Suppose that and . Evaluate the outer product . To get a clearer sense of how this works, perform this operation without using technology.  How is each of the columns of related to ?    Suppose and are general vectors. What is and what is a basis for its column space ?    Suppose that is a unit vector. What is the effect of multiplying a vector by the matrix ?              The rank is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .              Since each column of is a scalar multiple of , the rank of the matrix is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .       Evaluating the following cell loads in a dataset recording some features of 1057 houses. Notice how the lot size varies over a relatively small range compared to the other features. For this reason, in addition to demeaning the data, we'll scale each feature by dividing by its standard deviation so that the range of values is similar for each feature. The matrix holds the result.    Find the singular values of and use them to determine the variance in the direction of the principal components.     For what fraction of the variance do the first two principal components account?    Find a singular value decomposition of and construct the matrix whose entries are the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by the first two principal components. You can plot the projected data points using list_plot(B.columns()) .     Study the entries in the first two principal components and . Would a more expensive house lie on the left, right, top, or bottom of the plot you constructed?    In what ways does a house that lies on the far left of the plot you constructed differ from an average house? In what ways does a house that lies near the top of the plot you constructed differ from an average house?         The variances are , , , and .         The matrix is formed from the first two rows of the product .    Left    Houses on the left have larger living area, lot sizes, and prices and are newer. Houses near the top have larger lot sizes and ages.         The singular values are , , , and . Therefore, the variances in the direction of the principal components are given by and are , , , and .    The first two principal components retain of the total variance.    The matrix is formed from the first two rows of the product .    The fourth components, the ones that correspond to house prices, of and are and respectively. This shows us that doesn't contribute much to the house price. Moving in the negative direction causes the house price to go up so the more expensive homes are on the left.    The first principal component is . If we move to the left, this vector is multiplied by a negative number so the living area, lot size, and price go up while the age of the house decreases.  The second principal component is . If we move up, this is multiplied by a positive number so the lot size and the age of the house increase.       Let's revisit the voting records of justices on the second Rehnquist court. Evaluating the following cell will load the voting records of the justices in the 188 cases decided by a 5-4 vote and store them in the matrix .    The cell above also defined the 188-dimensional vector whose entries are all 1. What does the product represent? Use the following cell to evaluate this product.     How does the product tell us which justice voted in the majority most frequently? What does this say about the presence of a swing vote on the court?    How does this product tell us whether we should characterize this court as leaning conservative or progressive?    How does this product tell us about the presence of a second swing vote on the court?    Study the left singular vector and describe how it reinforces the fact that there was a second swing vote. Who was this second swing vote?         For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    The justice that most often votes with the majority could be a swing vote.    The first five justices usually vote in the majority.    A second justice voted with the majority 84 times more than with the minority.    Anthony Kennedy          is a 9-dimensional vector. For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    Out of 188 cases, the fifth justice Sandra Day O'Connor voted with the majority 94 more times than with the minority. Since a swing vote will often vote in the majority, this gives additional evidence that Sandra Day O'Connor is a swing vote.    The first five justices more often vote with the majority and the last four more often with the minority. This means the court leans toward the first five justices, who are the more conservative ones.    The fourth justice Anthony Kennedy voted with the majority 84 times more than with the minority. He is a potential swing vote.    The third singular vector effectively records his vote and not the votes of the other justices. This lends support to the claim that Anthony Kennedy is a second, somewhat less influential, swing vote.       The following cell loads a dataset that describes the percentages with which justices on the second Rehnquist court agreed with one another. For instance, the entry in the first row and second column is 72.78, which means that Justices Rehnquist and Scalia agreed with each other in 72.78% of the cases.    Examine the matrix . What special structure does this matrix have and why should we expect it to have this structure?    Plot the singular values of below. For what value of would the approximation be a reasonable approximation of ?     Find a singular value decomposition and examine the matrices and using, for instance, n(U, 3) . What do you notice about the relationship between and and why should we expect this relationship to hold?     The command approximate(A, k) will form the approximating matrix . Study the matrix using the display_matrix command. Which justice or justices seem to be most agreeable, that is, most likely to agree with other justices? Which justice is least agreeable?     Examine the difference and describe how this tells us about the presence of voting blocs and swing votes on the court.          The matrix is symmetric.     should be a good approximation to .         John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others.         The matrix is symmetric because the number of times Justice A votes with Justice B is the same as the number of times Justice B votes with Justice A.    The first two singular values appear to be most important so should be a good approximation to .     since is a positive definite, symmetric matrix.    The sixth justice, John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others so they form swing votes.       Suppose that is a reduced singular value decomposition of the matrix . The matrix is called the Moore-Penrose inverse of .   Explain why is an matrix.    If is an invertible, square matrix, explain why .    Explain why , the orthogonal projection of onto .    Explain why , the orthogonal projection of onto .         Find the number of columns of and the number of rows of .    If is invertible, then is an matrix whose rank is .                   The number of columns of is the number of columns of , which is , and the number of rows is the number of rows of , which is .    If is invertible, then is an matrix whose rank is . The reduced singular value decomposition is therefore so that .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .       In , we saw how some linear algebraic computations are sensitive to round off error made by a computer. A singular value decomposition can help us understand when this situation can occur.  For instance, consider the matrices The entries in these matrices are quite close to one another, but is invertible while is not. It seems like is almost singular. In fact, we can measure how close a matrix is to being singular by forming the condition number , , the ratio of the largest to smallest singular value. If were singular, the condition number would be undefined because the singular value . Therefore, we will think of matrices with large condition numbers as being close to singular.   Define the matrix and find a singular value decomposition. What is the condition number of ?     Define the left singular vectors and . Compare the results when    .     .   Notice how a small change in the vector leads to a small change in .    Now compare the results when    .     .   Notice now how a small change in leads to a large change in .    Previously, we saw that, if we write in terms of left singular vectors , then we have If we write , explain why is sensitive to small changes in .   Generally speaking, a square matrix with a large condition number will demonstrate this type of behavior so that the computation of is likely to be affected by round off error. We call such a matrix ill-conditioned .      The condition number is                                               The singular values of are and , which means the condition number is .                                    We have so . Since is so much smaller than , even small changes in can lead to relatively large changes in .       "
},
{
  "id": "exploration-30",
  "level": "2",
  "url": "sec-svd-uses.html#exploration-30",
  "type": "Preview Activity",
  "number": "7.5.1",
  "title": "",
  "body": "  Suppose that where vectors form the columns of , and vectors form the columns of .   What are the shapes of the matrices , , and ?    What is the rank of ?    Describe how to find an orthonormal basis for .    Describe how to find an orthonormal basis for .    If the columns of form an orthonormal basis for , what is ?    How would you form a matrix that projects vectors orthogonally onto ?           is , is , and is .     since there are three nonzero singular values.    The first three columns, , , and , of form an orthonormal basis for .    The last column of is a basis for .     since each entry is a dot product of two vectors in an orthonormal set.     projects vectors orthogonally onto .      "
},
{
  "id": "activity-105",
  "level": "2",
  "url": "sec-svd-uses.html#activity-105",
  "type": "Activity",
  "number": "7.5.2",
  "title": "",
  "body": "  Consider the equation where    Find a singular value decomposition for using the Sage cell below. What are singular values of ?     What is , the rank of ? How can we identify an orthonormal basis for ?    Form the reduced singular value decomposition by constructing: the matrix , consisting of the first columns of ; the matrix , consisting of the first columns of ; and , a square diagonal matrix. Verify that .  You may find it convenient to remember that if B is a matrix defined in Sage, then B.matrix_from_columns( list ) and B.matrix_from_rows( list ) can be used to extract columns or rows from B . For instance, B.matrix_from_rows([0,1,2]) provides a matrix formed from the first three rows of B .     How does the reduced singular value decomposition provide a matrix whose columns are an orthonormal basis for ?    Explain why a least-squares approximate solution satisfies     What is the product and why does it have this form?    Explain why is the least-squares approximate solution, and use this expression to find .           The singular values are and .    There are two nonzero singular values so . The first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .    Since , we obtain the equation .     since this product computes the dot products of the columns.    We have the equation , which gives            and .     and the first two columns of form an orthonormal basis for .    We have     The columns of form an orthonormal basis for .                     "
},
{
  "id": "prop-svd-ols",
  "level": "2",
  "url": "sec-svd-uses.html#prop-svd-ols",
  "type": "Proposition",
  "number": "7.5.1",
  "title": "",
  "body": "  If is a reduced singular value decomposition of , then a least-squares approximate solution to is given by    "
},
{
  "id": "activity-106",
  "level": "2",
  "url": "sec-svd-uses.html#activity-106",
  "type": "Activity",
  "number": "7.5.3",
  "title": "",
  "body": "  Let's consider a matrix where Evaluating the following cell will create the matrices U , V , and Sigma . Notice how the diagonal_matrix command provides a convenient way to form the diagonal matrix .    Form the matrix . What is ?     Now form the approximating matrix . What is ?     Find the error in the approximation by finding .    Now find and the error . What is ?     Find and the error . What is ?    What would happen if we were to compute ?    What do you notice about the error as increases?          We find that is the rank matrix: .    Because it has one nonzero singular value, has rank and has the form: .          is the rank matrix and .     is the rank matrix with .     since is a rank matrix.    As increases, the entries in the get closer to . This means that our approximations are improving.           .     .          and .     and .         The entries get closer to .      "
},
{
  "id": "activity-107",
  "level": "2",
  "url": "sec-svd-uses.html#activity-107",
  "type": "Activity",
  "number": "7.5.4",
  "title": "",
  "body": "  Let's revisit the iris dataset that we studied in . Remember that there are four measurements given for each of 150 irises and that each iris belongs to one of three species.  Evaluating the following cell will load the dataset and define the demeaned data matrix whose shape is .    Find the singular values of using the command A.singular_values() and use them to determine the variance in the direction of each of the four principal components. What is the fraction of variance retained by the first two principal components?     We will now write the matrix so that . Suppose that a demeaned data point, say, the 100th column of , is written as a linear combination of principal components: Explain why , the vector of coordinates of in the basis of principal components, appears as 100th column of .    Suppose that we now project this demeaned data point orthogonally onto the subspace spanned by the first two principal components and . What are the coordinates of the projected point in this basis and how can we find them in the matrix ?    Alternatively, consider the approximation of the demeaned data matrix . Explain why the 100th column of represents the projection of onto the two-dimensional subspace spanned by the first two principal components, and . Then explain why the coefficients in that projection, , form the two-dimensional vector that is the 100th column of .    Now we've seen that the columns of form the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by and . In the cell below, find a singular value decomposition of and use it to form the matrix Gamma2 . When you evaluate this cell, you will see a plot of the projected demeaned data plots, similar to the one we created in .           The singular values are , , and . Since , the variances are , , , and . The fraction of the total variance represented by the first two principal components is .    If is the column of , then is the column of .    The projected data point is since is orthogonal to the subspace spanned by the first two principal components. Therefore, the coordinates of the projected data point are the first two components of the corresponding column of . The coordinates of all the projected data points are given by the first two rows of .    Once again, suppose that is a column of and that . Then is the corresponding column of .    We can construct or just pull out the first two rows of .          The fraction of the total variance represented by the first two principal components is .    If , then is the corresponding column of .    We just need the first two components of the corresponding column of .     is the corresponding column of .    We can construct or just pull out the first two rows of .      "
},
{
  "id": "activity-108",
  "level": "2",
  "url": "sec-svd-uses.html#activity-108",
  "type": "Activity",
  "number": "7.5.5",
  "title": "",
  "body": "  Evaluating the following cell loads some data that we'll use in this activity. To begin, it defines and displays a matrix .    If we interpret 0 as black and 1 as white, this matrix represents an image as shown below. We will explore how the singular value decomposition helps us to compress this image.   By inspecting the image represented by , identify a basis for and determine .    The following cell plots the singular values of . Explain how this plot verifies that the rank is what you found in the previous part.     There is a command approximate(A, k) that creates the approximation . Use the cell below to define and look at the images represented by the first few approximations. What is the smallest value of for which ?     Now we can see how the singular value decomposition allows us to compress images. Since this is a matrix, we need numbers to represent the image. However, we can also reconstruct the image using a small number of singular values and vectors: What are the dimensions of the singular vectors and ? Between the singular vectors and singular values, how many numbers do we need to reconstruct for the smallest for which ? This is the compressed size of the image.    The compression ratio is the ratio of the uncompressed size to the compressed size. What compression ratio does this represent?       Next we'll explore an example based on a photograph.   Consider the following image consisting of an array of pixels stored in the matrix .   Plot the singular values of .     Use the cell below to study the approximations for . Notice how the approximating image more closely approximates the original image as increases.  What is the compression ratio when ? What is the compression ratio when ? Notice how a higher compression ratio leads to a lower quality reconstruction of the image.       A second, related application of the singular value decomposition to image processing is called denoising . For example, consider the image represented by the matrix below. This image is similar to the image of the letter \"O\" we first studied in this activity, but there are splotchy regions in the background that result, perhaps, from scanning the image. We think of the splotchy regions as noise, and our goal is to improve the quality of the image by reducing the noise.   Plot the singular values below. How are the singular values of this matrix similar to those represented by the clean image that we considered earlier and how are they different?     There is a natural point where the singular values dramatically decrease so it makes sense to think of the noise as being formed by the small singular values. To denoise the image, we will therefore replace by its approximation , where is the point at which the singular values drop off. This has the effect of setting the small singular values to zero and hence eliminating the noise. Choose an appropriate value of below and notice that the new image appears to be somewhat cleaned up as a result of removing the noise.                  because there are three distinct columns represented by the first, third, and sixth columns.    There are three nonzero singular values so as we suspected.    The smallest value is since .    The left singular vectors are -dimensional and the right singular vectors are -dimensional. If we keep three singular values, left singular vectors, and right singular vectors, we have .    The compression ratio is so the data is compressed by a factor of .          The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation since that's the place where the singular values become almost .                                    The compression ratio is           The singular values fall off steeply but never reach .    When , the compression ratio is about . When , the compression ratio is about .          The singular values are similar, but they never reach .     seems like a natural approximation         "
},
{
  "id": "activity-109",
  "level": "2",
  "url": "sec-svd-uses.html#activity-109",
  "type": "Activity",
  "number": "7.5.6",
  "title": "",
  "body": "  Evaluating the following cell loads and displays a dataset describing the votes of each justice in these 911 cases. More specifically, an entry of +1 means that the justice represented by the row voted with the majority in the case represented by the column. An entry of -1 means that justice was in the minority. This information is also stored in the matrix . The justices are listed, very roughly, in order from more conservative to more progressive.  In this activity, it will be helpful to visualize the entries in various matrices and vectors. The next cell displays the first 50 columns of the matrix with white representing an entry of +1, red representing -1, and black representing 0.    Plot the singular values of below. Describe the significance of this plot, including the relative contributions from the singular values as increases.     Form the singular value decomposition and the matrix of coefficients so that .     We will now study a particular case, the second case which appears as the column of indexed by 1 . There is a command display_column(A, k) that provides a visual display of the column of a matrix . Describe the justices' votes in the second case.     Also, display the first left singular vector , the column of indexed by , and the column of holding the coefficients that express the second case as a linear combination of left singular vectors. What does this tell us about how the second case is constructed as a linear combination of left singular vectors? What is the significance of the first left singular vector ?    Let's now study the case, which is represented by the column of indexed by 47 . Describe the voting pattern in this case.     Display the second left singular vector and the vector of coefficients that express the case as a linear combination of left singular vectors. Describe how this case is constructed as a linear combination of singular vectors. What is the significance of the second left singular vector ?    The data in describes the number of cases decided by each possible vote count.  Number of cases by vote count    Vote count  # of cases    9-0  405    8-1  89    7-2  111    6-3  118    5-4  188    How do the singular vectors and reflect this data? Would you characterize the court as leaning toward the conservatives or progressives? Use these singular vectors to explain your response.    Cases decided by a 5-4 vote are often the most impactful as they represent a sharp divide among the justices and, often, society at large. For that reason, we will now focus on the 5-4 decisions. Evaluating the next cell forms the matrix consisting of 5-4 decisions. Form the singular value decomposition of along with the matrix of coefficients so that and display the first left singular vector . Study how the case, indexed by 6 , is constructed as a linear combination of left singular vectors. What does this singular vector tell us about the make up of the court and whether it leans towards the conservatives or progressives?    Display the second left singular vector and study how the case, indexed by 5 , is constructed as a linear combination of left singular vectors. What does tell us about the relative importance of the justices' voting records?    By a swing vote , we mean a justice who is less inclined to vote with a particular bloc of justices but instead swings from one bloc to another with the potential to sway close decisions. What do the singular vectors and tell us about the presence of voting blocs on the court and the presence of a swing vote? Which justice represents the swing vote?          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.    The unanimous decision is essentially represented as so represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.    This 5-4 decision is essentially represented as , the second most important left singular vector.    We see that the most decisions are unanimous, which is why represents unanimous decisions. The second most frequently occurring decisions is a 5-4 decision, which is why represents a 5-4 decision that leans to the conservative justices.    The first singular vector represents a case where the five conservative justices are voting together. From this we conclude that the court leans toward the conservatives.    The second left singular vector essentially records the vote of Sandra Day O'Connor and shows how her vote has the power to swing a 5-4 decision from a conservative majority to a progressive majority.    Sandra Day O'Connor would be the swing vote.          The first two singular values contribute most significantly.     is a matrix that expresses the cases as linear combinations of the left singular vectors.    This is a unanimous decision.     represents a unanimous decision.    This is a 5-4 decision with the 5 conservative justices voting in the majority.     represents a 5-4 decision.    The most frequently occurring decisions are unanimous and the second most frequently occurring are 5-4.     represents a case where the five conservative justices are voting together.    The second left singular vector essentially records the vote of Sandra Day O'Connor.    Sandra Day O'Connor      "
},
{
  "id": "exercise-286",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-286",
  "type": "Exercise",
  "number": "7.5.7.1",
  "title": "",
  "body": " Suppose that     Find the singular values of . What is ?    Find the sequence of matrices , , , and where is the rank approximation of .          .     , ,\\   ,          The singular values are , , , and so we have .     , ,   ,      "
},
{
  "id": "exercise-287",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-287",
  "type": "Exercise",
  "number": "7.5.7.2",
  "title": "",
  "body": " Suppose we would like to find the best quadratic function fitting the points     Set up a linear system describing the coefficients .    Find the singular value decomposition of .    Use the singular value decomposition to find the least-squares approximate solution .         The linear system is      is a matrix, is , and is               The linear system is      is a matrix, is , and is     The rank of is 3 so we'll form the reduced singular value decomposition . Then      "
},
{
  "id": "exercise-288",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-288",
  "type": "Exercise",
  "number": "7.5.7.3",
  "title": "",
  "body": " Remember that the outer product of two vectors and is the matrix .   Suppose that and . Evaluate the outer product . To get a clearer sense of how this works, perform this operation without using technology.  How is each of the columns of related to ?    Suppose and are general vectors. What is and what is a basis for its column space ?    Suppose that is a unit vector. What is the effect of multiplying a vector by the matrix ?              The rank is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .              Since each column of is a scalar multiple of , the rank of the matrix is and forms a basis.    This matrix projects vectors orthogonally onto the line defined by .     "
},
{
  "id": "exercise-289",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-289",
  "type": "Exercise",
  "number": "7.5.7.4",
  "title": "",
  "body": " Evaluating the following cell loads in a dataset recording some features of 1057 houses. Notice how the lot size varies over a relatively small range compared to the other features. For this reason, in addition to demeaning the data, we'll scale each feature by dividing by its standard deviation so that the range of values is similar for each feature. The matrix holds the result.    Find the singular values of and use them to determine the variance in the direction of the principal components.     For what fraction of the variance do the first two principal components account?    Find a singular value decomposition of and construct the matrix whose entries are the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by the first two principal components. You can plot the projected data points using list_plot(B.columns()) .     Study the entries in the first two principal components and . Would a more expensive house lie on the left, right, top, or bottom of the plot you constructed?    In what ways does a house that lies on the far left of the plot you constructed differ from an average house? In what ways does a house that lies near the top of the plot you constructed differ from an average house?         The variances are , , , and .         The matrix is formed from the first two rows of the product .    Left    Houses on the left have larger living area, lot sizes, and prices and are newer. Houses near the top have larger lot sizes and ages.         The singular values are , , , and . Therefore, the variances in the direction of the principal components are given by and are , , , and .    The first two principal components retain of the total variance.    The matrix is formed from the first two rows of the product .    The fourth components, the ones that correspond to house prices, of and are and respectively. This shows us that doesn't contribute much to the house price. Moving in the negative direction causes the house price to go up so the more expensive homes are on the left.    The first principal component is . If we move to the left, this vector is multiplied by a negative number so the living area, lot size, and price go up while the age of the house decreases.  The second principal component is . If we move up, this is multiplied by a positive number so the lot size and the age of the house increase.     "
},
{
  "id": "exercise-290",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-290",
  "type": "Exercise",
  "number": "7.5.7.5",
  "title": "",
  "body": " Let's revisit the voting records of justices on the second Rehnquist court. Evaluating the following cell will load the voting records of the justices in the 188 cases decided by a 5-4 vote and store them in the matrix .    The cell above also defined the 188-dimensional vector whose entries are all 1. What does the product represent? Use the following cell to evaluate this product.     How does the product tell us which justice voted in the majority most frequently? What does this say about the presence of a swing vote on the court?    How does this product tell us whether we should characterize this court as leaning conservative or progressive?    How does this product tell us about the presence of a second swing vote on the court?    Study the left singular vector and describe how it reinforces the fact that there was a second swing vote. Who was this second swing vote?         For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    The justice that most often votes with the majority could be a swing vote.    The first five justices usually vote in the majority.    A second justice voted with the majority 84 times more than with the minority.    Anthony Kennedy          is a 9-dimensional vector. For each justice, it tells us the number of times that justice voted with the majority minus the number of times they voted in the minority.    Out of 188 cases, the fifth justice Sandra Day O'Connor voted with the majority 94 more times than with the minority. Since a swing vote will often vote in the majority, this gives additional evidence that Sandra Day O'Connor is a swing vote.    The first five justices more often vote with the majority and the last four more often with the minority. This means the court leans toward the first five justices, who are the more conservative ones.    The fourth justice Anthony Kennedy voted with the majority 84 times more than with the minority. He is a potential swing vote.    The third singular vector effectively records his vote and not the votes of the other justices. This lends support to the claim that Anthony Kennedy is a second, somewhat less influential, swing vote.     "
},
{
  "id": "exercise-291",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-291",
  "type": "Exercise",
  "number": "7.5.7.6",
  "title": "",
  "body": " The following cell loads a dataset that describes the percentages with which justices on the second Rehnquist court agreed with one another. For instance, the entry in the first row and second column is 72.78, which means that Justices Rehnquist and Scalia agreed with each other in 72.78% of the cases.    Examine the matrix . What special structure does this matrix have and why should we expect it to have this structure?    Plot the singular values of below. For what value of would the approximation be a reasonable approximation of ?     Find a singular value decomposition and examine the matrices and using, for instance, n(U, 3) . What do you notice about the relationship between and and why should we expect this relationship to hold?     The command approximate(A, k) will form the approximating matrix . Study the matrix using the display_matrix command. Which justice or justices seem to be most agreeable, that is, most likely to agree with other justices? Which justice is least agreeable?     Examine the difference and describe how this tells us about the presence of voting blocs and swing votes on the court.          The matrix is symmetric.     should be a good approximation to .         John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others.         The matrix is symmetric because the number of times Justice A votes with Justice B is the same as the number of times Justice B votes with Justice A.    The first two singular values appear to be most important so should be a good approximation to .     since is a positive definite, symmetric matrix.    The sixth justice, John Paul Stevens, is the least agreeable.    The fourth and fifth justices, Anthony Kennedy and Sandra Day O'Connor are less inclined to agree with the others so they form swing votes.     "
},
{
  "id": "exercise-292",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-292",
  "type": "Exercise",
  "number": "7.5.7.7",
  "title": "",
  "body": " Suppose that is a reduced singular value decomposition of the matrix . The matrix is called the Moore-Penrose inverse of .   Explain why is an matrix.    If is an invertible, square matrix, explain why .    Explain why , the orthogonal projection of onto .    Explain why , the orthogonal projection of onto .         Find the number of columns of and the number of rows of .    If is invertible, then is an matrix whose rank is .                   The number of columns of is the number of columns of , which is , and the number of rows is the number of rows of , which is .    If is invertible, then is an matrix whose rank is . The reduced singular value decomposition is therefore so that .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .     , which projects vectors orthogonal onto since the columns of are an orthonormal basis for .     "
},
{
  "id": "exercise-293",
  "level": "2",
  "url": "sec-svd-uses.html#exercise-293",
  "type": "Exercise",
  "number": "7.5.7.8",
  "title": "",
  "body": " In , we saw how some linear algebraic computations are sensitive to round off error made by a computer. A singular value decomposition can help us understand when this situation can occur.  For instance, consider the matrices The entries in these matrices are quite close to one another, but is invertible while is not. It seems like is almost singular. In fact, we can measure how close a matrix is to being singular by forming the condition number , , the ratio of the largest to smallest singular value. If were singular, the condition number would be undefined because the singular value . Therefore, we will think of matrices with large condition numbers as being close to singular.   Define the matrix and find a singular value decomposition. What is the condition number of ?     Define the left singular vectors and . Compare the results when    .     .   Notice how a small change in the vector leads to a small change in .    Now compare the results when    .     .   Notice now how a small change in leads to a large change in .    Previously, we saw that, if we write in terms of left singular vectors , then we have If we write , explain why is sensitive to small changes in .   Generally speaking, a square matrix with a large condition number will demonstrate this type of behavior so that the computation of is likely to be affected by round off error. We call such a matrix ill-conditioned .      The condition number is                                               The singular values of are and , which means the condition number is .                                    We have so . Since is so much smaller than , even small changes in can lead to relatively large changes in .     "
},
{
  "id": "app-sage-reference",
  "level": "1",
  "url": "app-sage-reference.html",
  "type": "Appendix",
  "number": "A",
  "title": "Sage Reference",
  "body": " Sage Reference  We have introduced a number of Sage commands throughout the text, and the most important ones are summarized here in a single place.    Accessing Sage  In addition to the Sage cellls included throughout the book, there are a number of ways to access Sage.  There is a freely available Sage cell at .  You can save your Sage work by creating an account at and working in a Sage worksheet.  There is a page of Sage cells at . The results obtained from evaluating one cell are available in other cells on that page. However, you will lose any work once the page is reloaded.    Creating matrices  There are a couple of ways to create matrices. For instance, the matrix can be created in either of the two following ways.    matrix(3, 4, [-2, 3, 0, 4, 1,-2, 1,-3, 0, 2, 3, 0])      matrix([ [-2, 3, 0, 4], [ 1,-2, 1,-3], [ 0, 2, 3, 0] ])      Be aware that Sage can treat mathematically equivalent matrices in different ways depending on how they are entered. For instance, the matrix matrix([ [1, 2], [2, 1] ]) has integer entries while matrix([ [1.0, 2.0], [2.0, 1.0] ]) has floating point entries.  If you would like the entries to be considered as floating point numbers, you can include RDF in the definition of the matrix. matrix(RDF, [ [1, 2], [2, 1] ])     Special matrices  The identity matrix can be created with identity_matrix(4) A diagonal matrix can be created from a list of its diagonal entries. For instance, diagonal_matrix([3,-4,2])     Reduced row echelon form  The reduced row echelon form of a matrix can be obtained using the rref() function. For instance, A = matrix([ [1,2], [2,1] ]) A.rref()     Vectors  A vector is defined by listing its components. v = vector([3,-1,2])     Addition  The + operator performs vector and matrix addition. v = vector([2,1]) w = vector([-3,2]) print(v+w)  A = matrix([[2,-3],[1,2]]) B = matrix([[-4,1],[3,-1]]) print(A+B)     Multiplication  The * operator performs scalar multiplication of vectors and matrices. v = vector([2,1]) print(3*v) A = matrix([[2,1],[-3,2]]) print(3*A)   Similarly, the * is used for matrix-vector and matrix-matrix multiplication. A = matrix([[2,-3],[1,2]]) v = vector([2,1]) print(A*v) B = matrix([[-4,1],[3,-1]]) print(A*B)     Operations on vectors     The length of a vector v is found using v.norm() .    The dot product of two vectors v and w is v*w .       Operations on matrices    The transpose of a matrix A is obtained using either A.transpose() or A.T .    The inverse of a matrix A is obtained using either A.inverse() or A^-1 .    The determinant of A is A.det() .    A basis for the null space is found with A.right_kernel() .    Pull out a column of A using, for instance, A.column(0) , which returns the vector that is the first column of A .    The command A.matrix_from_columns([0,1,2]) returns the matrix formed by the first three columns of A .       Eigenvectors and eigenvalues     The eigenvalues of a matrix A can be found with A.eigenvalues() . The number of times that an eigenvalue appears in the list equals its multiplicity.    The eigenvectors of a matrix having rational entries can be found with A.eigenvectors_right() .    If can be diagonalized as , then D, P = A.right_eigenmatrix() provides the matrices D and P .    The characteristic polynomial of A is A.charpoly('x') and its factored form A.fcp('x') .       Matrix factorizations     The factorization of a matrix P, L, U = A.LU() gives matrices so that .    A singular value decomposition is obtained with U, Sigma, V = A.SVD() It's important to note that the matrix must be defined using RDF . For instance, A = matrix(RDF, 3,2,[1,0,-1,1,1,1]) .    The factorization of A is A.QR() provided that A is defined using RDF .        "
},
{
  "id": "index-1",
  "level": "1",
  "url": "index-1.html",
  "type": "Index",
  "number": "",
  "title": "Index",
  "body": " Index   "
},
{
  "id": "colophon-2",
  "level": "1",
  "url": "colophon-2.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": " This book was authored in PreTeXt .  "
}
]

var ptx_lunr_idx = lunr(function () {
  this.ref('id')
  this.field('title')
  this.field('body')

  ptx_lunr_docs.forEach(function (doc) {
    this.add(doc)
  }, this)
})
